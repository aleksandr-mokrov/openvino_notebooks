{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Convert and Optimize YOLOv8 with OpenVINO™\n",
    "\n",
    "The YOLOv8 algorithm developed by Ultralytics is a cutting-edge, state-of-the-art (SOTA) model that is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection, image segmentation, and image classification tasks.\n",
    "\n",
    "YOLO stands for “You Only Look Once”, it is a popular family of real-time object detection algorithms.\n",
    "The original YOLO object detector was first released in 2016. Since then, different versions and variants of YOLO have been proposed, each providing a significant increase in performance and efficiency.\n",
    "YOLOv8 builds upon the success of previous YOLO versions and introduces new features and improvements to further boost performance and flexibility.\n",
    "More details about its realization can be found in the original model [repository](https://github.com/ultralytics/ultralytics).\n",
    "\n",
    "Real-time object detection and instance segmentation are often used as key components in computer vision systems.\n",
    "Applications that use real-time object detection models include video analytics, robotics, autonomous vehicles, multi-object tracking and object counting, medical image analysis, and many others.\n",
    "\n",
    "\n",
    "This tutorial demonstrates step-by-step instructions on how to run and optimize PyTorch YOLOv8 with OpenVINO. We consider the steps required for object detection and instance segmentation scenarios.\n",
    "\n",
    "The tutorial consists of the following steps:\n",
    "\n",
    "- Prepare the PyTorch model.\n",
    "- Download and prepare a dataset.\n",
    "- Convert the PyTorch model to OpenVINO IR.\n",
    "- Quantize with accuracy control."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"2\"></a>\n",
    "#### Prerequisites [&#8657;](#0)\n",
    "\n",
    "Install necessary packages."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install --pre openvino\n",
    "!pip install git+https://github.com/openvinotoolkit/nncf.git@develop\n",
    "!pip install -q \"ultralytics==8.0.43\" onnx"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"1\"></a>\n",
    "## Get Pytorch model [&#8657;](#0)\n",
    "\n",
    "Generally, PyTorch models represent an instance of the [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class, initialized by a state dictionary with model weights.\n",
    "We will use the YOLOv8 nano model (also known as `yolov8n`) pre-trained on a COCO dataset, which is available in this [repo](https://github.com/ultralytics/ultralytics). Similar steps are also applicable to other YOLOv8 models.\n",
    "Typical steps to obtain a pre-trained model:\n",
    "\n",
    "1. Create an instance of a model class.\n",
    "2. Load a checkpoint state dict, which contains the pre-trained model weights.\n",
    "\n",
    "In this case, the creators of the model provide an API that enables converting the YOLOv8 model to ONNX and then to OpenVINO IR. Therefore, we do not need to do these steps manually."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.yolo.cfg import get_cfg\n",
    "from ultralytics.yolo.data.utils import check_det_dataset\n",
    "from ultralytics.yolo.engine.validator import BaseValidator as Validator\n",
    "from ultralytics.yolo.utils import DATASETS_DIR\n",
    "from ultralytics.yolo.utils import DEFAULT_CFG\n",
    "from ultralytics.yolo.utils import ops\n",
    "from ultralytics.yolo.utils.metrics import ConfusionMatrix\n",
    "\n",
    "ROOT = os.path.abspath('')\n",
    "\n",
    "MODEL_NAME = \"yolov8n-seg\"\n",
    "\n",
    "model = YOLO(f\"{ROOT}/{MODEL_NAME}.pt\")\n",
    "args = get_cfg(cfg=DEFAULT_CFG)\n",
    "args.data = \"coco128-seg.yaml\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define validator and dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "validator = model.ValidatorClass(args)\n",
    "validator.data = check_det_dataset(args.data)\n",
    "dataset = validator.data[\"val\"]\n",
    "\n",
    "data_loader = validator.get_dataloader(f\"{DATASETS_DIR}/coco128-seg\", 1)\n",
    "\n",
    "validator = model.ValidatorClass(args)\n",
    "\n",
    "validator.is_coco = True\n",
    "validator.class_map = ops.coco80_to_coco91_class()\n",
    "validator.names = model.model.names\n",
    "validator.metrics.names = validator.names\n",
    "validator.nc = model.model.model[-1].nc\n",
    "validator.nm = 32\n",
    "validator.process = ops.process_mask\n",
    "validator.plot_masks = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import openvino\n",
    "\n",
    "\n",
    "model_path = Path(f\"{ROOT}/{MODEL_NAME}_openvino_model/{MODEL_NAME}.xml\")\n",
    "if not model_path.exists():\n",
    "    model.export(format=\"openvino\", dynamic=True, half=False)\n",
    "\n",
    "model = openvino.Core().read_model(model_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define quantization_dataset and validation_fn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import nncf\n",
    "from nncf.quantization.advanced_parameters import AdvancedAccuracyRestorerParameters\n",
    "\n",
    "\n",
    "def transform_fn(data_item: Dict):\n",
    "    input_tensor = validator.preprocess(data_item)[\"img\"].numpy()\n",
    "    return input_tensor\n",
    "\n",
    "def validation_ac(\n",
    "    compiled_model: openvino.CompiledModel,\n",
    "    validation_loader: torch.utils.data.DataLoader,\n",
    "    validator: Validator,\n",
    "    num_samples: int = None,\n",
    ") -> float:\n",
    "    validator.seen = 0\n",
    "    validator.jdict = []\n",
    "    validator.stats = []\n",
    "    validator.batch_i = 1\n",
    "    validator.confusion_matrix = ConfusionMatrix(nc=validator.nc)\n",
    "    num_outputs = len(compiled_model.outputs)\n",
    "\n",
    "    counter = 0\n",
    "    for batch_i, batch in enumerate(validation_loader):\n",
    "        if num_samples is not None and batch_i == num_samples:\n",
    "            break\n",
    "        batch = validator.preprocess(batch)\n",
    "        results = compiled_model(batch[\"img\"])\n",
    "        if num_outputs == 1:\n",
    "            preds = torch.from_numpy(results[compiled_model.output(0)])\n",
    "        else:\n",
    "            preds = [\n",
    "                torch.from_numpy(results[compiled_model.output(0)]),\n",
    "                torch.from_numpy(results[compiled_model.output(1)]),\n",
    "            ]\n",
    "        preds = validator.postprocess(preds)\n",
    "        validator.update_metrics(preds, batch)\n",
    "        counter += 1\n",
    "    stats = validator.get_stats()\n",
    "    if num_outputs == 1:\n",
    "        stats_metrics = stats[\"metrics/mAP50-95(B)\"]\n",
    "    else:\n",
    "        stats_metrics = stats[\"metrics/mAP50-95(M)\"]\n",
    "    print(f\"Validate: dataset length = {counter}, metric value = {stats_metrics:.3f}\")\n",
    "    return stats_metrics\n",
    "\n",
    "quantization_dataset = nncf.Dataset(data_loader, transform_fn)\n",
    "\n",
    "validation_fn = partial(validation_ac, validator=validator)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run quantization with accuracy control."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "quantized_model = nncf.quantize_with_accuracy_control(\n",
    "    model,\n",
    "    quantization_dataset,\n",
    "    quantization_dataset,\n",
    "    validation_fn=validation_fn,\n",
    "    max_drop=0.01,\n",
    "    preset=nncf.QuantizationPreset.MIXED,\n",
    "    advanced_accuracy_restorer_parameters=AdvancedAccuracyRestorerParameters(\n",
    "        ranking_subset_size=25\n",
    "    ),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
