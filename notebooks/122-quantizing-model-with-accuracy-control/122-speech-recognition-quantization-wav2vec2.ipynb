{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Quantize Speech Recognition Models with accuracy control using NNCF PTQ API\n",
    "This tutorial demonstrates how to apply `INT8` quantization with accuracy control to the speech recognition model, known as [Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2), using the NNCF (Neural Network Compression Framework) 8-bit quantization with accuracy control in post-training mode (without the fine-tuning pipeline). This notebook uses a fine-tuned [Wav2Vec2-Base-960h](https://huggingface.co/facebook/wav2vec2-base-960h) [PyTorch](https://pytorch.org/) model trained on the [LibriSpeech ASR corpus](https://www.openslr.org/12). The tutorial is designed to be extendable to custom models and datasets. It consists of the following steps:\n",
    "\n",
    "- Download and prepare the Wav2Vec2 model and LibriSpeech dataset.\n",
    "- Define data loading and accuracy validation functionality.\n",
    "- Model quantization with accuracy control.\n",
    "- Compare Accuracy of original PyTorch model, OpenVINO FP16 and INT8 models.\n",
    "- Compare performance of the original and quantized models.\n",
    "\n",
    "The advanced quantization flow allows to apply 8-bit quantization to the model with control of accuracy metric. This is achieved by keeping the most impactful operations within the model in the original precision. The flow is based on the [Basic 8-bit quantization](https://docs.openvino.ai/2023.0/basic_quantization_flow.html) and has the following differences:\n",
    "\n",
    "- Besides the calibration dataset, a validation dataset is required to compute the accuracy metric. Both datasets can refer to the same data in the simplest case.\n",
    "- Validation function, used to compute accuracy metric is required. It can be a function that is already available in the source framework or a custom function.\n",
    "- Since accuracy validation is run several times during the quantization process, quantization with accuracy control can take more time than the Basic 8-bit quantization flow.\n",
    "- The resulted model can provide smaller performance improvement than the Basic 8-bit quantization flow because some of the operations are kept in the original precision.\n",
    "\n",
    "> **NOTE**: Currently, 8-bit quantization with accuracy control is available only for models in OpenVINO representation.\n",
    "\n",
    "The steps for the quantization with accuracy control are described below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"0\"></a>\n",
    "### Table of content:\n",
    "- [Imports](#1)\n",
    "- [Settings](#2)\n",
    "- [Prepare the Model](#3)\n",
    "- [Prepare LibriSpeech Dataset](#4)\n",
    "- [Define DataLoader](#5)\n",
    "- [Prepare calibration and validation datasets](#6)\n",
    "- [Prepare validation function](#7)\n",
    "- [Run quantization with accuracy control](#8)\n",
    "- [Model Usage Example](#9)\n",
    "- [Compare Performance of the Original and Quantized Models](#10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# !pip install -q \"openvino-dev>=2023.1.0\" \"nncf>=2.6.0\"\n",
    "!pip install openvino==2023.1.0.dev20230728\n",
    "!pip install git+https://github.com/openvinotoolkit/nncf.git@develop\n",
    "!pip install -q soundfile librosa transformers torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"1\"></a>\n",
    "## Imports [&#8657;](#0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import torch\n",
    "\n",
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from notebook_utils import download_file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"2\"></a>\n",
    "## Settings [&#8657;](#0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Set the data and model directories, model source URL and model filename.\n",
    "MODEL_DIR = Path(\"model\")\n",
    "DATA_DIR = Path(\"../data/datasets/librispeech\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "DATA_DIR.mkdir(exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"3\"></a>\n",
    "## Prepare the Model [&#8657;](#0)\n",
    "Perform the following:\n",
    "- Download and unpack a pre-trained Wav2Vec2 model.\n",
    "- Run model conversion API to convert the model to the OpenVINO Intermediate Representation (OpenVINO IR)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "download_file(\"https://huggingface.co/facebook/wav2vec2-base-960h/resolve/main/pytorch_model.bin\", directory=Path(MODEL_DIR) / 'pytorch', show_progress=True)\n",
    "download_file(\"https://huggingface.co/facebook/wav2vec2-base-960h/resolve/main/config.json\", directory=Path(MODEL_DIR) / 'pytorch', show_progress=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import all dependencies to load the original PyTorch model and convert it to the OpenVINO Intermediate Representation (OpenVINO IR).."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "MAX_SEQ_LENGTH = 30480\n",
    "\n",
    "\n",
    "torch_model = Wav2Vec2ForCTC.from_pretrained(Path(MODEL_DIR) / 'pytorch')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import openvino\n",
    "\n",
    "\n",
    "default_input = torch.zeros([1, MAX_SEQ_LENGTH], dtype=torch.float)\n",
    "ov_model = openvino.convert_model(torch_model, example_input=default_input)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"4\"></a>\n",
    "## Prepare LibriSpeech Dataset [&#8657;](#0)\n",
    "\n",
    "Use the code below to download and unpack the archives with 'dev-clean' and 'test-clean' subsets of LibriSpeech Dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "download_file(\"http://openslr.elda.org/resources/12/dev-clean.tar.gz\", directory=DATA_DIR, show_progress=True)\n",
    "download_file(\"http://openslr.elda.org/resources/12/test-clean.tar.gz\", directory=DATA_DIR, show_progress=True)\n",
    "\n",
    "if not os.path.exists(f'{DATA_DIR}/LibriSpeech/dev-clean'):\n",
    "    with tarfile.open(f\"{DATA_DIR}/dev-clean.tar.gz\") as tar:\n",
    "        tar.extractall(path=DATA_DIR)\n",
    "if not os.path.exists(f'{DATA_DIR}/LibriSpeech/test-clean'):\n",
    "    with tarfile.open(f\"{DATA_DIR}/test-clean.tar.gz\") as tar:\n",
    "        tar.extractall(path=DATA_DIR)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"5\"></a>\n",
    "## Define DataLoader [&#8657;](#0)\n",
    "Wav2Vec2 model accepts a raw waveform of the speech signal as input and produces vocabulary class estimations as output. Since the dataset contains\n",
    "audio files in FLAC format, use the `soundfile` package to convert them to waveform.\n",
    "\n",
    "> **NOTE**: Consider increasing `samples_limit` to get more precise results. A suggested value is `300` or more, as it will take longer time to process."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import soundfile\n",
    "\n",
    "\n",
    "class LibriSpeechDataLoader:\n",
    "\n",
    "    @staticmethod\n",
    "    def read_flac(file_name):\n",
    "        speech, samplerate = soundfile.read(file_name)\n",
    "        assert samplerate == 16000, \"read_flac: only 16kHz supported!\"\n",
    "        return speech\n",
    "\n",
    "    # Required methods\n",
    "    def __init__(self, config, samples_limit=300):\n",
    "        \"\"\"Constructor\n",
    "        :param config: data loader specific config\n",
    "        \"\"\"\n",
    "        self.samples_limit = samples_limit\n",
    "        self._data_dir = config[\"data_source\"]\n",
    "        self._ds = []\n",
    "        self._prepare_dataset()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns size of the dataset\"\"\"\n",
    "        return len(self._ds)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns annotation, data and metadata at the specified index.\n",
    "        Possible formats:\n",
    "        (index, annotation), data\n",
    "        (index, annotation), data, metadata\n",
    "        \"\"\"\n",
    "        label = self._ds[index][0]\n",
    "        inputs = {'inputs': np.expand_dims(self._ds[index][1], axis=0)}\n",
    "        return label, inputs\n",
    "\n",
    "    # Methods specific to the current implementation\n",
    "    def _prepare_dataset(self):\n",
    "        pattern = re.compile(r'([0-9\\-]+)\\s+(.+)')\n",
    "        data_folder = Path(self._data_dir)\n",
    "        txts = list(data_folder.glob('**/*.txt'))\n",
    "        counter = 0\n",
    "        for txt in txts:\n",
    "            content = txt.open().readlines()\n",
    "            for line in content:\n",
    "                res = pattern.search(line)\n",
    "                if not res:\n",
    "                    continue\n",
    "                name = res.group(1)\n",
    "                transcript = res.group(2)\n",
    "                fname = txt.parent / name\n",
    "                fname = fname.with_suffix('.flac')\n",
    "                identifier = str(fname.relative_to(data_folder))\n",
    "                self._ds.append(((counter, transcript.upper()), LibriSpeechDataLoader.read_flac(os.path.join(self._data_dir, identifier))))\n",
    "                counter += 1\n",
    "                if counter >= self.samples_limit:\n",
    "                    # Limit exceeded\n",
    "                    return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"6\"></a>\n",
    "## Prepare calibration and validation datasets [&#8657;](#0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nncf\n",
    "\n",
    "def transform_fn(data_item):\n",
    "    \"\"\"\n",
    "    Extract the model's input from the data item.\n",
    "    The data item here is the data item that is returned from the data source per iteration.\n",
    "    This function should be passed when the data item cannot be used as model's input.\n",
    "    \"\"\"\n",
    "    _, inputs = data_item\n",
    "\n",
    "    return inputs[\"inputs\"]\n",
    "\n",
    "\n",
    "dataset_config = {\"data_source\": os.path.join(DATA_DIR, \"LibriSpeech/dev-clean\")}\n",
    "data_loader = LibriSpeechDataLoader(dataset_config, samples_limit=300)\n",
    "calibration_dataset = nncf.Dataset(data_loader, transform_fn)\n",
    "dataset_config = {\"data_source\": os.path.join(DATA_DIR, \"LibriSpeech/test-clean\")}\n",
    "test_data_loader = LibriSpeechDataLoader(dataset_config, samples_limit=300)\n",
    "validation_dataset = nncf.Dataset(test_data_loader, transform_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"7\"></a>\n",
    "## Prepare validation function [&#8657;](#0)\n",
    "Define function that decodes predicted probabilities to text, using tokenizer decode_logits."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "\n",
    "def decode_logits(logits):\n",
    "    decoding_vocab = dict(enumerate(MetricWER.alphabet))\n",
    "    token_ids = np.squeeze(np.argmax(logits, -1))\n",
    "    tokens = [decoding_vocab[idx] for idx in token_ids]\n",
    "    tokens = [token_group[0] for token_group in groupby(tokens)]\n",
    "    tokens = [t for t in tokens if t != MetricWER.pad_token]\n",
    "    res_string = ''.join([t if t != MetricWER.words_delimiter else ' ' for t in tokens]).strip()\n",
    "    res_string = ' '.join(res_string.split(' '))\n",
    "    res_string = res_string.lower()\n",
    "\n",
    "    return res_string"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define `MetricWER` class to calculate Word Error Rate."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MetricWER:\n",
    "    alphabet = [\n",
    "        \"<pad>\", \"<s>\", \"</s>\", \"<unk>\", \"|\",\n",
    "        \"e\", \"t\", \"a\", \"o\", \"n\", \"i\", \"h\", \"s\", \"r\", \"d\", \"l\", \"u\",\n",
    "        \"m\", \"w\", \"c\", \"f\", \"g\", \"y\", \"p\", \"b\", \"v\", \"k\", \"'\", \"x\", \"j\", \"q\", \"z\"]\n",
    "    words_delimiter = '|'\n",
    "    pad_token = '<pad>'\n",
    "\n",
    "    # Required methods\n",
    "    def __init__(self):\n",
    "        self._name = \"WER\"\n",
    "        self._sum_score = 0\n",
    "        self._sum_words = 0\n",
    "        self._cur_score = 0\n",
    "        self._decoding_vocab = dict(enumerate(self.alphabet))\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        \"\"\"Returns accuracy metric value for the last model output.\"\"\"\n",
    "        return {self._name: self._cur_score}\n",
    "\n",
    "    @property\n",
    "    def avg_value(self):\n",
    "        \"\"\"Returns accuracy metric value for all model outputs.\"\"\"\n",
    "        return {self._name: self._sum_score / self._sum_words if self._sum_words != 0 else 0}\n",
    "\n",
    "    def update(self, output, target):\n",
    "        \"\"\"\n",
    "        Updates prediction matches.\n",
    "\n",
    "        :param output: model output\n",
    "        :param target: annotations\n",
    "        \"\"\"\n",
    "        decoded = [decode_logits(i) for i in output]\n",
    "        target = [i.lower() for i in target]\n",
    "        assert len(output) == len(target), \"sizes of output and target mismatch!\"\n",
    "        for i in range(len(output)):\n",
    "            self._get_metric_per_sample(decoded[i], target[i])\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets collected matches\n",
    "        \"\"\"\n",
    "        self._sum_score = 0\n",
    "        self._sum_words = 0\n",
    "\n",
    "    def get_attributes(self):\n",
    "        \"\"\"\n",
    "        Returns a dictionary of metric attributes {metric_name: {attribute_name: value}}.\n",
    "        Required attributes: 'direction': 'higher-better' or 'higher-worse'\n",
    "                             'type': metric type\n",
    "        \"\"\"\n",
    "        return {self._name: {\"direction\": \"higher-worse\", \"type\": \"WER\"}}\n",
    "\n",
    "    # Methods specific to the current implementation\n",
    "    def _get_metric_per_sample(self, annotation, prediction):\n",
    "        cur_score = self._editdistance_eval(annotation.split(), prediction.split())\n",
    "        cur_words = len(annotation.split())\n",
    "\n",
    "        self._sum_score += cur_score\n",
    "        self._sum_words += cur_words\n",
    "        self._cur_score = cur_score / cur_words\n",
    "\n",
    "        result = cur_score / cur_words if cur_words != 0 else 0\n",
    "        return result\n",
    "\n",
    "    def _editdistance_eval(self, source, target):\n",
    "        n, m = len(source), len(target)\n",
    "\n",
    "        distance = np.zeros((n + 1, m + 1), dtype=int)\n",
    "        distance[:, 0] = np.arange(0, n + 1)\n",
    "        distance[0, :] = np.arange(0, m + 1)\n",
    "\n",
    "        for i in range(1, n + 1):\n",
    "            for j in range(1, m + 1):\n",
    "                cost = 0 if source[i - 1] == target[j - 1] else 1\n",
    "\n",
    "                distance[i][j] = min(distance[i - 1][j] + 1,\n",
    "                                     distance[i][j - 1] + 1,\n",
    "                                     distance[i - 1][j - 1] + cost)\n",
    "        return distance[n][m]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the validation function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def validation_fn(model, dataset):\n",
    "    \"\"\"\n",
    "    Calculate and returns a metric for the model.\n",
    "    \"\"\"\n",
    "    wer = MetricWER()\n",
    "    for sample in dataset:\n",
    "        # run infer function on sample\n",
    "        output = model(np.array(sample[1]['inputs']))[model.output(0)]\n",
    "\n",
    "        # update metric on sample result\n",
    "        target = [sample[0][1]]\n",
    "        wer.update(output, target)\n",
    "\n",
    "    return 1 - wer.avg_value[\"WER\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"8\"></a>\n",
    "## Run quantization with accuracy control [&#8657;](#0)\n",
    "You should provide the calibration dataset and the validation dataset. It can be the same dataset. \n",
    "  - parameter `max_drop` defines the accuracy drop threshold. The quantization process stops when the degradation of accuracy metric on the validation dataset is less than the `max_drop`. The default value is 0.01. NNCF will stop the quantization and report an error if the `max_drop` value can’t be reached.\n",
    "  - `drop_type` defines how the accuracy drop will be calculated: ABSOLUTE (used by default) or RELATIVE.\n",
    "  - `ranking_subset_size` - size of a subset that is used to rank layers by their contribution to the accuracy drop. Default value is 300, and the more samples it has the better ranking, potentially. Here we use the value 25 to speed up the execution. \n",
    "\n",
    "> **NOTE**: Execution can take tens of minutes and requires up to 10 GB of free memory"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nncf.quantization.advanced_parameters import AdvancedAccuracyRestorerParameters\n",
    "from nncf.parameters import ModelType\n",
    "\n",
    "quantized_model = nncf.quantize_with_accuracy_control(\n",
    "    ov_model,\n",
    "    calibration_dataset=calibration_dataset,\n",
    "    validation_dataset=validation_dataset,\n",
    "    validation_fn=validation_fn,\n",
    "    max_drop=0.01,\n",
    "    drop_type=nncf.DropType.ABSOLUTE,\n",
    "    model_type=ModelType.TRANSFORMER,\n",
    "    advanced_accuracy_restorer_parameters=AdvancedAccuracyRestorerParameters(\n",
    "        ranking_subset_size=25\n",
    "    ),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"9\"></a>\n",
    "## Model Usage Example [&#8657;](#0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "\n",
    "audio = LibriSpeechDataLoader.read_flac(f'{DATA_DIR}/LibriSpeech/test-clean/121/127105/121-127105-0017.flac')\n",
    "\n",
    "ipd.Audio(audio, rate=16000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "core = openvino.Core()\n",
    "\n",
    "compiled_model = core.compile_model(model=quantized_model, device_name='CPU')\n",
    "\n",
    "input_data = np.expand_dims(audio, axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, make a prediction."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = compiled_model([input_data])[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"10\"></a>\n",
    "## Compare Performance of the Original and Quantized Models [&#8657;](#0)\n",
    "\n",
    "  - Define dataloader for test dataset.\n",
    "  - Define functions to get inference for PyTorch and OpenVINO models.\n",
    "  - Define functions to compute Word Error Rate."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "dataset_config = {\"data_source\": os.path.join(DATA_DIR, \"LibriSpeech/test-clean\")}\n",
    "test_data_loader = LibriSpeechDataLoader(dataset_config, samples_limit=300)\n",
    "\n",
    "\n",
    "# inference function for pytorch\n",
    "def torch_infer(model, sample):\n",
    "    output = model(torch.Tensor(sample[1]['inputs'])).logits\n",
    "    output = output.detach().cpu().numpy()\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# inference function for openvino\n",
    "def ov_infer(model, sample):\n",
    "    output = model.output(0)\n",
    "    output = model(np.array(sample[1]['inputs']))[output]\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def compute_wer(dataset, model, infer_fn):\n",
    "    wer = MetricWER()\n",
    "    for sample in tqdm(dataset):\n",
    "        # run infer function on sample\n",
    "        output = infer_fn(model, sample)\n",
    "        # update metric on sample result\n",
    "        wer.update(output, [sample[0][1]])\n",
    "\n",
    "    return wer.avg_value"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, compute WER for the original PyTorch model and quantized model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pt_result = compute_wer(test_data_loader, torch_model, torch_infer)\n",
    "quantized_result = compute_wer(test_data_loader, compiled_model, ov_infer)\n",
    "\n",
    "print(f'[PyTorch]   Word Error Rate: {pt_result[\"WER\"]:.4f}')\n",
    "print(f'[Quantized OpenVino]  Word Error Rate: {quantized_result[\"WER\"]:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
