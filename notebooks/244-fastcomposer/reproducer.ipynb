{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q transformers==4.25.1 accelerate datasets evaluate diffusers==0.16.1 xformers triton scipy clip gradio"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Install fastcomposer from github. The package wasn't published."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone https://github.com/mit-han-lab/fastcomposer.git\n",
    "%cd fastcomposer\n",
    "!python setup.py install\n",
    "%cd .."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download pre-trained model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!mkdir -p model/fastcomposer\n",
    "%cd model/fastcomposer\n",
    "!wget https://huggingface.co/mit-han-lab/fastcomposer/resolve/main/pytorch_model.bin\n",
    "%cd ../.."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define required arguments."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CAPTION=\"a man <|image|> and a man <|image|> are reading book together\"\n",
    "DEMO_NAME=\"newton_einstein\"\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass()\n",
    "class Args:\n",
    "    adam_beta1=0.9\n",
    "    adam_beta2=0.999\n",
    "    adam_epsilon=1e-08\n",
    "    adam_weight_decay=0.01\n",
    "    allow_tf32=False\n",
    "    balance_num_objects=False\n",
    "    cache_dir=None\n",
    "    caption_column='caption'\n",
    "    center_crop=False\n",
    "    checkpointing_steps=500\n",
    "    dataloader_num_workers=0\n",
    "    dataset_config_name=None\n",
    "    dataset_format='huggingface'\n",
    "    dataset_name=None\n",
    "    dataset_name1=None\n",
    "    dataset_name2=None\n",
    "    dataset_name3=None\n",
    "    dataset_type='original'\n",
    "    dataset_type1='original'\n",
    "    dataset_type2='original'\n",
    "    dataset_type3='original'\n",
    "    disable_flashattention=False\n",
    "    enable_xformers_memory_efficient_attention=False\n",
    "    end_idx=50\n",
    "    evaluation_batch_size=4\n",
    "    finetuned_model_path='model/fastcomposer/pytorch_model.bin'\n",
    "    freeze_unet=False\n",
    "    generate_height=512\n",
    "    generate_width=512\n",
    "    gradient_accumulation_steps=1\n",
    "    gradient_checkpointing=False\n",
    "    guidance_scale=5\n",
    "    hub_model_id=None\n",
    "    hub_token=None\n",
    "    image_column='image'\n",
    "    image_encoder_name_or_path='openai/clip-vit-large-patch14'\n",
    "    image_encoder_trainable_layers=0\n",
    "    image_encoder_type='clip'\n",
    "    image_encoder_use_lora=False\n",
    "    inference_split='eval'\n",
    "    inference_steps=50\n",
    "    keep_interval=None\n",
    "    keep_only_last_checkpoint=False\n",
    "    learning_rate=0.0001\n",
    "    load_merged_lora_model=False\n",
    "    load_model=None\n",
    "    local_rank=-1\n",
    "    localization_layers=5\n",
    "    logging_dir='logs'\n",
    "    lora_image_encoder_alpha=16\n",
    "    lora_image_encoder_bias='none'\n",
    "    lora_image_encoder_dropout=0.1\n",
    "    lora_image_encoder_r=16\n",
    "    lora_text_encoder_alpha=16\n",
    "    lora_text_encoder_bias='none'\n",
    "    lora_text_encoder_dropout=0.1\n",
    "    lora_text_encoder_r=16\n",
    "    lr_scheduler='constant'\n",
    "    lr_warmup_steps=500\n",
    "    mask_loss=False\n",
    "    mask_loss_prob=0.5\n",
    "    max_grad_norm=1.0\n",
    "    max_num_objects=4\n",
    "    max_train_samples=None\n",
    "    max_train_steps=None\n",
    "    min_num_objects=None\n",
    "    mixed_precision='fp16'\n",
    "    model_type='dreamer'\n",
    "    no_object_augmentation=False\n",
    "    non_ema_revision=None\n",
    "    num_batches=1\n",
    "    num_datasets=1\n",
    "    num_image_tokens=1\n",
    "    num_images_per_prompt=1\n",
    "    num_rows=1\n",
    "    num_train_epochs=100\n",
    "    object_appear_prob=1\n",
    "    object_background_processor='random'\n",
    "    object_localization=False\n",
    "    object_localization_loss='balanced_l1'\n",
    "    object_localization_normalize=False\n",
    "    object_localization_skip_special_tokens=False\n",
    "    object_localization_threshold=1.0\n",
    "    object_localization_weight=0.01\n",
    "    object_resolution=256\n",
    "    object_types=None\n",
    "    output_dir='log/fine_generator'\n",
    "    pretrained_model_name_or_path='runwayml/stable-diffusion-v1-5'\n",
    "    push_to_hub=False\n",
    "    random_flip=False\n",
    "    report_to=None\n",
    "    resume_from_checkpoint=None\n",
    "    retrieval_identity_path=None\n",
    "    retrieval_identity_path1=None\n",
    "    retrieval_identity_path2=None\n",
    "    retrieval_identity_path3=None\n",
    "    revision=None\n",
    "    scale_lr=False\n",
    "    seed=None\n",
    "    start_idx=0\n",
    "    start_merge_step=0\n",
    "    test_caption=None\n",
    "    test_reference_folder=None\n",
    "    test_resolution=512\n",
    "    text_encoder_use_lora=False\n",
    "    text_only_prob=0\n",
    "    text_prompt_only=False\n",
    "    train_batch_size=16\n",
    "    train_data_dir=None\n",
    "    train_image_encoder=False\n",
    "    train_resolution=256\n",
    "    train_text_encoder=False\n",
    "    uncondition_prob=0\n",
    "    unet_lora_alpha=1.0\n",
    "    unet_lr_scale=1.0\n",
    "    unet_use_lora=False\n",
    "    use_8bit_adam=False\n",
    "    use_ema=False\n",
    "    use_multiple_conditioning=False\n",
    "    use_multiple_datasets=False\n",
    "    val_dataset_name=None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialise model. After notebook restarting you should remove one extra `fastcomposer` from import path."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from fastcomposer.fastcomposer.model import FastComposerModel\n",
    "\n",
    "\n",
    "model = FastComposerModel.from_pretrained(Args)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert image encoder that cause error \"No conversion rule found for operations: aten::_upsample_bilinear2d_aa, aten::sub_\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from openvino.runtime import PartialShape\n",
    "from openvino.tools import mo\n",
    "from openvino.tools.mo.convert import InputCutInfo\n",
    "\n",
    "\n",
    "image_encoder_ir = mo.convert_model(\n",
    "    model.image_encoder,\n",
    "    input=[InputCutInfo(shape=PartialShape([1,2,3,256,256]), type='f32')],\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
