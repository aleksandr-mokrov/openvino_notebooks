{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Semantic segmentation with LRASPP MobileNet v3 and OpenVINO\n",
    "\n",
    "The LRASPP model is based on the [Searching for MobileNetV3](https://arxiv.org/abs/1905.02244) paper. According to the paper, Searching for MobileNetV3, LR-ASPP or Lite Reduced Atrous Spatial Pyramid Pooling has a lightweight and efficient segmentation decoder architecture. he model is pre-trained on the [MS COCO](https://cocodataset.org/#home) dataset. Instead of training on all 80 classes, the segmentation model has been trained on 20 classes from the [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/) dataset:\n",
    "***background*, *aeroplane*, *bicycle*, *bird*, *boat*, *bottle*, *bus*, *car*, *cat*, *chair*, *cow*, *dining table*, *dog*, *horse*, *motorbike*, *person*, *potted plant*, *sheep*, *sofa*, *train*, *tv monitor***\n",
    "\n",
    "More information about the model is available in the [torchvision documentation](https://pytorch.org/vision/main/models/lraspp.html)\n",
    "\n",
    "### Table of content:\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Download and prepare model](#Download-and-prepare-model)\n",
    "- [Convert the original model to OpenVINO IR Format](#Convert-the-original-model-to-OpenVINO-IR-Format)\n",
    "- [Show Results](#Show-results)\n",
    "    - [Load and preprocess an input image](#Load-and-preprocess-an-input-image)\n",
    "    - [Run an inference on the OpenVINO model](#Run-an-inference-on-the-OpenVINO-model)\n",
    "    - [Run an inference on the PyTorch model to compare results](#Run-an-inference-on-the-PyTorch-model-to-compare-results)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55a1a04d5eaf0bea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prerequisites"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b593e3f8d32cfbb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%pip install -q \"torch==2.0.1\" \"torchvision==0.15.2\" datasets\n",
    "%pip install -q  \"openvino>=2023.1.0\""
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "id": "2515d4f6502681dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a51c56a6743cffc",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import openvino as ov\n",
    "import torch\n",
    "from torchvision.models.segmentation import lraspp_mobilenet_v3_large, LRASPP_MobileNet_V3_Large_Weights\n",
    "\n",
    "# Fetch the notebook utils script from the openvino_notebooks repo\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\n",
    "    url='https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/main/notebooks/utils/notebook_utils.py',\n",
    "    filename='notebook_utils.py'\n",
    ")\n",
    "from notebook_utils import download_file"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download and prepare model\n",
    "Set a name for the model, then define width and height of the image that will be used by the network during inference. According to the input transforms function, the model is pre-trained on images with a height of 520 and width of 780."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ca420fd1cb14d78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4bd4bda02d787d",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 640\n",
    "IMAGE_HEIGHT = 480\n",
    "DIRECTORY_NAME = \"model\"\n",
    "BASE_MODEL_NAME = DIRECTORY_NAME + \"/lraspp_mobilenet_v3_large\"\n",
    "weights_path = Path(BASE_MODEL_NAME + \".pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The torchvision module provides a ready to use set of functions for model class initialization. We will use torchvision.models.segmentation.lraspp_mobilenet_v3_large. You can directly pass pre-trained model weights to the model initialization function using weights enum LRASPP_MobileNet_V3_Large_Weights.COCO_WITH_VOC_LABELS_V1. However, for demonstration purposes, we will create it separately. Download the pre-trained weights and load the model. This may take some time if you have not downloaded the model before."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79835c4d4992ccaf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27f89fc36134e34",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Downloading the LRASPP MobileNetV3 model (if it has not been downloaded already)...\") \n",
    "download_file(LRASPP_MobileNet_V3_Large_Weights.COCO_WITH_VOC_LABELS_V1.url, filename=weights_path.name, directory=weights_path.parent)\n",
    "# create model object\n",
    "model = lraspp_mobilenet_v3_large()\n",
    "# read state dict, use map_location argument to avoid a situation where weights are saved in cuda (which may not be unavailable on the system)\n",
    "state_dict = torch.load(weights_path, map_location='cpu')\n",
    "# load state dict to model\n",
    "model.load_state_dict(state_dict)\n",
    "# switch model from training to inference mode\n",
    "model.eval()\n",
    "print(\"Loaded PyTorch LRASPP MobileNetV3 model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6298d1512d6b745",
   "metadata": {},
   "source": [
    "## Convert the original model to OpenVINO IR Format\n",
    "\n",
    "To convert the ONNX model to OpenVINO IR with `FP16` precision, use model conversion API. The models are saved inside the current directory. For more information on how to convert models, see this [page](https://docs.openvino.ai/2023.0/openvino_docs_model_processing_introduction.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ov_model_xml_path = Path('models/ov_lraspp_model.xml')\n",
    "\n",
    "\n",
    "if not ov_model_xml_path.exists():\n",
    "    ov_model_xml_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    dummy_input = torch.randn(1, 3, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "    # call tracing before converting. It is necessary in current version for segmentation models \n",
    "    traced_model = torch.jit.trace(model, dummy_input, strict=False)\n",
    "    ov_model = ov.convert_model(traced_model, example_input=dummy_input)\n",
    "    ov.save_model(ov_model, ov_model_xml_path)\n",
    "else:\n",
    "    print(f\"IR model {ov_model_xml_path} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Show results\n",
    "Confirm that the segmentation results look as expected by comparing model predictions on the OpenVINO IR and PyTorch models."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46be2fdb568b766"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load and preprocess an input image\n",
    "\n",
    "Images need to be normalized before propagating through the network."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edfd9b6758eb0655"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4fb521d188ebd3",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def normalize(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalize the image to the given mean and standard deviation\n",
    "    for CityScapes models.\n",
    "    \"\"\"\n",
    "    image = image.astype(np.float32)\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    image /= 255.0\n",
    "    image -= mean\n",
    "    image /= std\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31b5f9bbd257c49",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"huggingface/cats-image\")\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "\n",
    "display(image)\n",
    "\n",
    "image = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "resized_image = cv2.resize(image, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "normalized_image = normalize(resized_image)\n",
    "\n",
    "# Convert the resized images to network input shape.\n",
    "normalized_input_image = np.expand_dims(np.transpose(normalized_image, (2, 0, 1)), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Select device from dropdown list for running inference using OpenVINO"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4a126aeffa159e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "core = ov.Core()\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value='AUTO',\n",
    "    description='Device:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "677f739a6cc6f60a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run an inference on the OpenVINO model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e269f98a3f07826"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e593740244684cd",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "compiled_model = core.compile_model(ov_model_xml_path, device_name=device.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93992b7166d4e7a",
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_ir = compiled_model(normalized_input_image)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model predicts probabilities for how well each pixel corresponds to a specific label. To get the label with the highest probability for each pixel, operation argmax should be applied. After that, color coding can be applied to each label for more convenient visualization."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f848b2893dd90725"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c9919ae63cee0f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from notebook_utils import segmentation_map_to_image, viz_result_image, SegmentationMap, Label\n",
    "\n",
    "\n",
    "voc_labels = [\n",
    "    Label(index=0, color=(0, 0, 0), name=\"background\"),\n",
    "    Label(index=1, color=(128, 0, 0), name=\"aeroplane\"),\n",
    "    Label(index=2, color=(0, 128, 0), name=\"bicycle\"),\n",
    "    Label(index=3, color=(128, 128, 0), name=\"bird\"),\n",
    "    Label(index=4, color=(0, 0, 128), name=\"boat\"),\n",
    "    Label(index=5, color=(128, 0, 128), name=\"bottle\"),\n",
    "    Label(index=6, color=(0, 128, 128), name=\"bus\"),\n",
    "    Label(index=7, color=(128, 128, 128), name=\"car\"),\n",
    "    Label(index=8, color=(64, 0, 0), name=\"cat\"),\n",
    "    Label(index=9, color=(192, 0, 0), name=\"chair\"),\n",
    "    Label(index=10, color=(64, 128, 0), name=\"cow\"),\n",
    "    Label(index=11, color=(192, 128, 0), name=\"dining table\"),\n",
    "    Label(index=12, color=(64, 0, 128), name=\"dog\"),\n",
    "    Label(index=13, color=(192, 0, 128), name=\"horse\"),\n",
    "    Label(index=14, color=(64, 128, 128), name=\"motorbike\"),\n",
    "    Label(index=15, color=(192, 128, 128), name=\"person\"),\n",
    "    Label(index=16, color=(0, 64, 0), name=\"potted plant\"),\n",
    "    Label(index=17, color=(128, 64, 0), name=\"sheep\"),\n",
    "    Label(index=18, color=(0, 192, 0), name=\"sofa\"),\n",
    "    Label(index=19, color=(128, 192, 0), name=\"train\"),\n",
    "    Label(index=20, color=(0, 64, 128), name=\"tv monitor\")\n",
    "]\n",
    "VOCLabels = SegmentationMap(voc_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b9409b96eecea1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_mask_ir = np.squeeze(np.argmax(res_ir, axis=1)).astype(np.uint8)\n",
    "viz_result_image(\n",
    "    image,\n",
    "    segmentation_map_to_image(result=result_mask_ir, colormap=VOCLabels.get_colormap()),\n",
    "    resize=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run an inference on the PyTorch model to compare results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "681d6797ff090b25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    result_torch = model(torch.as_tensor(normalized_input_image).float())\n",
    "\n",
    "result_mask_torch = torch.argmax(result_torch['out'], dim=1).squeeze(0).numpy().astype(np.uint8)\n",
    "viz_result_image(\n",
    "    image,\n",
    "    segmentation_map_to_image(result=result_mask_torch, colormap=VOCLabels.get_colormap()),\n",
    "    resize=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d306bff2fea83735"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6594ff0cd90f3b2a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
