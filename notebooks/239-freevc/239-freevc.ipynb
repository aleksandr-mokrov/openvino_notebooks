{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# High-Quality Text-Free One-Shot Voice Conversion with FeeVC and OpenVINOâ„¢\n",
    "[FreeVC](https://github.com/OlaWod/FreeVC) allows alter the voice of a source speaker to a target style, while keeping the linguistic content unchanged, without text annotation.\n",
    "FreeVC suggests only command line interface to use and only with CUDA. In this notebook it shows how to use FreeVC in Python and without CUDA devices. It consists of the following steps:\n",
    "- Download and prepare models.\n",
    "- Inference.\n",
    "- Convert models to OpenVINO Intermediate Representation.\n",
    "- Inference using only OpenVINO's IR models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-requisites\n",
    "1. Clone this repo: git clone https://github.com/OlaWod/FreeVC.git\n",
    "2. Download [WavLM-Large](https://github.com/microsoft/unilm/tree/master/wavlm) and put it under directory 'FreeVC/wavlm/'\n",
    "3. Download the [VCTK](https://datashare.ed.ac.uk/handle/10283/3443) dataset. You can use any of them, but for this example two of them already included and available under directory 'dataset': `vctk-16k/p225/p225_001.wav` and `vctk-16k/p226/p226_002.wav`. To use other examples, you should change `convert.txt`\n",
    "4. Download [pretrained models](https://1drv.ms/u/s!AnvukVnlQ3ZTx1rjrOZ2abCwuBAh?e=UlhRR5) and put it under directory 'checkpoints' (for current example only `freevc.pth` are required)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Install extra requirements"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -q \"librosa>=0.8.1\"\n",
    "!pip install webrtcvad==2.0.10"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T22:20:22.582703Z",
     "end_time": "2023-05-09T22:20:23.185400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check if FreeVC is installed and its path to sys.path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "free_vc_repo = 'FreeVC'\n",
    "if not Path(free_vc_repo).exists():\n",
    "    !git clone https://github.com/OlaWod/FreeVC.git\n",
    "\n",
    "sys.path.append(free_vc_repo)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T22:20:23.201779Z",
     "end_time": "2023-05-09T22:20:23.265363Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports and settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.io.wavfile import write\n",
    "from tqdm import tqdm\n",
    "\n",
    "from openvino.runtime import Core\n",
    "from openvino.tools import mo\n",
    "\n",
    "import utils\n",
    "from models import SynthesizerTrn\n",
    "from speaker_encoder.voice_encoder import SpeakerEncoder\n",
    "from wavlm import WavLM, WavLMConfig"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T22:20:23.265363Z",
     "end_time": "2023-05-09T22:20:32.259987Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Redefine function `get_model` from `utils` to exclude cuda"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_cmodel():\n",
    "    checkpoint = torch.load('wavlm/WavLM-Large.pt')\n",
    "    cfg = WavLMConfig(checkpoint['cfg'])\n",
    "    cmodel = WavLM(cfg)\n",
    "    cmodel.load_state_dict(checkpoint['model'])\n",
    "    cmodel.eval()\n",
    "\n",
    "    return cmodel"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T22:20:32.259987Z",
     "end_time": "2023-05-09T22:20:32.328262Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Models initialization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hps = utils.get_hparams_from_file('configs/freevc.json')\n",
    "os.makedirs('outputs/freevc', exist_ok=True)\n",
    "\n",
    "net_g = SynthesizerTrn(\n",
    "    hps.data.filter_length // 2 + 1,\n",
    "    hps.train.segment_size // hps.data.hop_length,\n",
    "    **hps.model\n",
    ")\n",
    "\n",
    "utils.load_checkpoint('checkpoints/freevc.pth', net_g, optimizer=None, strict=True)\n",
    "cmodel = get_cmodel()\n",
    "smodel = SpeakerEncoder('FreeVC/speaker_encoder/ckpt/pretrained_bak_5805000.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T22:20:32.328262Z",
     "end_time": "2023-05-09T22:20:49.946453Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reading dataset settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "titles, srcs, tgts = [], [], []\n",
    "\n",
    "with open('convert.txt', \"r\") as f:\n",
    "    for rawline in f.readlines():\n",
    "        title, src, tgt = rawline.strip().split(\"|\")\n",
    "        titles.append(title)\n",
    "        srcs.append(src)\n",
    "        tgts.append(tgt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T22:20:49.946453Z",
     "end_time": "2023-05-09T22:20:50.018853Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for line in tqdm(zip(titles, srcs, tgts)):\n",
    "        title, src, tgt = line\n",
    "        # tgt\n",
    "        wav_tgt, _ = librosa.load(tgt, sr=hps.data.sampling_rate)\n",
    "        wav_tgt, _ = librosa.effects.trim(wav_tgt, top_db=20)\n",
    "\n",
    "        g_tgt = smodel.embed_utterance(wav_tgt)\n",
    "        g_tgt = torch.from_numpy(g_tgt).unsqueeze(0)\n",
    "\n",
    "        # src\n",
    "        wav_src, _ = librosa.load(src, sr=hps.data.sampling_rate)\n",
    "        wav_src = torch.from_numpy(wav_src).unsqueeze(0)\n",
    "\n",
    "        c = utils.get_content(cmodel, wav_src)\n",
    "\n",
    "        tgt_audio = net_g.infer(c, g=g_tgt)\n",
    "        tgt_audio = tgt_audio[0][0].data.cpu().float().numpy()\n",
    "\n",
    "        timestamp = time.strftime(\"%m-%d_%H-%M\", time.localtime())\n",
    "        write(os.path.join('outputs/freevc', \"{}.wav\".format(timestamp + \"_\" + title)), hps.data.sampling_rate,\n",
    "              tgt_audio)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T22:20:50.035588Z",
     "end_time": "2023-05-09T22:20:54.914303Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Result audio files should be available in 'outputs/freevc'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use Model Optimizer\n",
    "### Convert cmodel (WavLM).\n",
    "First we convert the model to the ONNX format, then to OpenVINO's IR format"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define forward as extract_features for compatibility\n",
    "cmodel.forward = cmodel.extract_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T22:20:54.914303Z",
     "end_time": "2023-05-09T22:20:54.980658Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert cmodel to ONNX"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "OUTPUT_DIR = Path(\"output\")\n",
    "BASE_MODEL_NAME = \"cmodel\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "onnx_path = Path(OUTPUT_DIR / (BASE_MODEL_NAME + \"_fp32\")).with_suffix(\".onnx\")\n",
    "\n",
    "length = 32000\n",
    "input_shape = (1, length)\n",
    "\n",
    "input_names=['input']\n",
    "output_names = ['output']\n",
    "dummy_input = torch.randn(1, length)\n",
    "dynamic_axes= {\n",
    "    'input':{ 1: 'length'},\n",
    "    'output': {1: 'out_length'}\n",
    "}\n",
    "\n",
    "torch.onnx.export(cmodel, dummy_input, onnx_path, input_names=input_names, output_names=output_names, dynamic_axes=dynamic_axes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T22:20:54.991322Z",
     "end_time": "2023-05-09T22:21:16.443131Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert ONNX model to IR format and compile it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ir_cmodel = mo.convert_model(onnx_path, compress_to_fp16=True)\n",
    "core = Core()\n",
    "compiled_cmodel = core.compile_model(ir_cmodel, 'CPU')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T22:21:16.466990Z",
     "end_time": "2023-05-09T22:22:10.316242Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convert SpeakerEncoder\n",
    "Converting to ONNX format."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "OUTPUT_DIR = Path(\"output\")\n",
    "BASE_MODEL_NAME = \"smodel\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "onnx_path = Path(OUTPUT_DIR / (BASE_MODEL_NAME + \"_fp32\")).with_suffix(\".onnx\")\n",
    "\n",
    "\n",
    "length = 32000\n",
    "input_shape = (1, length, 40)\n",
    "\n",
    "input_names=['input']\n",
    "output_names = ['output']\n",
    "dummy_input = torch.randn(1, length, 40)\n",
    "dynamic_axes= {\n",
    "    'input':{\n",
    "        0: 'branch_size',\n",
    "        1: 'length'},\n",
    "    'output': {1: 'out_length'}\n",
    "}\n",
    "\n",
    "torch.onnx.export(smodel, dummy_input, onnx_path, input_names=input_names, output_names=output_names, dynamic_axes=dynamic_axes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T22:22:10.332071Z",
     "end_time": "2023-05-09T22:22:17.890618Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Converting to OpenVINO's IR format."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ir_smodel = mo.convert_model(onnx_path, compress_to_fp16=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T22:22:17.890618Z",
     "end_time": "2023-05-09T22:22:18.175476Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Converted model hasn't helper methods. So, we should define helper functions for preparing an input for inference. Just take `compute_partial_slices` and `embed_utterance` methods from `speaker_encoder.voice_encoder.SpeakerEncoder` class and make functions based on them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from speaker_encoder.hparams import sampling_rate, mel_window_step, partials_n_frames\n",
    "from speaker_encoder import audio\n",
    "\n",
    "\n",
    "def compute_partial_slices(n_samples: int, rate, min_coverage):\n",
    "    \"\"\"\n",
    "    Computes where to split an utterance waveform and its corresponding mel spectrogram to\n",
    "    obtain partial utterances of <partials_n_frames> each. Both the waveform and the\n",
    "    mel spectrogram slices are returned, so as to make each partial utterance waveform\n",
    "    correspond to its spectrogram.\n",
    "\n",
    "    The returned ranges may be indexing further than the length of the waveform. It is\n",
    "    recommended that you pad the waveform with zeros up to wav_slices[-1].stop.\n",
    "\n",
    "    :param n_samples: the number of samples in the waveform\n",
    "    :param rate: how many partial utterances should occur per second. Partial utterances must\n",
    "    cover the span of the entire utterance, thus the rate should not be lower than the inverse\n",
    "    of the duration of a partial utterance. By default, partial utterances are 1.6s long and\n",
    "    the minimum rate is thus 0.625.\n",
    "    :param min_coverage: when reaching the last partial utterance, it may or may not have\n",
    "    enough frames. If at least <min_pad_coverage> of <partials_n_frames> are present,\n",
    "    then the last partial utterance will be considered by zero-padding the audio. Otherwise,\n",
    "    it will be discarded. If there aren't enough frames for one partial utterance,\n",
    "    this parameter is ignored so that the function always returns at least one slice.\n",
    "    :return: the waveform slices and mel spectrogram slices as lists of array slices. Index\n",
    "    respectively the waveform and the mel spectrogram with these slices to obtain the partial\n",
    "    utterances.\n",
    "    \"\"\"\n",
    "    assert 0 < min_coverage <= 1\n",
    "\n",
    "    # Compute how many frames separate two partial utterances\n",
    "    samples_per_frame = int((sampling_rate * mel_window_step / 1000))\n",
    "    n_frames = int(np.ceil((n_samples + 1) / samples_per_frame))\n",
    "    frame_step = int(np.round((sampling_rate / rate) / samples_per_frame))\n",
    "    assert 0 < frame_step, \"The rate is too high\"\n",
    "    assert frame_step <= partials_n_frames, \"The rate is too low, it should be %f at least\" % \\\n",
    "        (sampling_rate / (samples_per_frame * partials_n_frames))\n",
    "\n",
    "    # Compute the slices\n",
    "    wav_slices, mel_slices = [], []\n",
    "    steps = max(1, n_frames - partials_n_frames + frame_step + 1)\n",
    "    for i in range(0, steps, frame_step):\n",
    "        mel_range = np.array([i, i + partials_n_frames])\n",
    "        wav_range = mel_range * samples_per_frame\n",
    "        mel_slices.append(slice(*mel_range))\n",
    "        wav_slices.append(slice(*wav_range))\n",
    "\n",
    "    # Evaluate whether extra padding is warranted or not\n",
    "    last_wav_range = wav_slices[-1]\n",
    "    coverage = (n_samples - last_wav_range.start) / (last_wav_range.stop - last_wav_range.start)\n",
    "    if coverage < min_coverage and len(mel_slices) > 1:\n",
    "        mel_slices = mel_slices[:-1]\n",
    "        wav_slices = wav_slices[:-1]\n",
    "\n",
    "    return wav_slices, mel_slices\n",
    "\n",
    "\n",
    "def embed_utterance(wav: np.ndarray, smodel, return_partials=False, rate=1.3, min_coverage=0.75):\n",
    "    \"\"\"\n",
    "    Computes an embedding for a single utterance. The utterance is divided in partial\n",
    "    utterances and an embedding is computed for each. The complete utterance embedding is the\n",
    "    L2-normed average embedding of the partial utterances.\n",
    "\n",
    "    TODO: independent batched version of this function\n",
    "\n",
    "    :param wav: a preprocessed utterance waveform as a numpy array of float32\n",
    "    :param return_partials: if True, the partial embeddings will also be returned along with\n",
    "    the wav slices corresponding to each partial utterance.\n",
    "    :param rate: how many partial utterances should occur per second. Partial utterances must\n",
    "    cover the span of the entire utterance, thus the rate should not be lower than the inverse\n",
    "    of the duration of a partial utterance. By default, partial utterances are 1.6s long and\n",
    "    the minimum rate is thus 0.625.\n",
    "    :param min_coverage: when reaching the last partial utterance, it may or may not have\n",
    "    enough frames. If at least <min_pad_coverage> of <partials_n_frames> are present,\n",
    "    then the last partial utterance will be considered by zero-padding the audio. Otherwise,\n",
    "    it will be discarded. If there aren't enough frames for one partial utterance,\n",
    "    this parameter is ignored so that the function always returns at least one slice.\n",
    "    :return: the embedding as a numpy array of float32 of shape (model_embedding_size,). If\n",
    "    <return_partials> is True, the partial utterances as a numpy array of float32 of shape\n",
    "    (n_partials, model_embedding_size) and the wav partials as a list of slices will also be\n",
    "    returned.\n",
    "    \"\"\"\n",
    "    # Compute where to split the utterance into partials and pad the waveform with zeros if\n",
    "    # the partial utterances cover a larger range.\n",
    "    wav_slices, mel_slices = compute_partial_slices(len(wav), rate, min_coverage)\n",
    "    max_wave_length = wav_slices[-1].stop\n",
    "    if max_wave_length >= len(wav):\n",
    "        wav = np.pad(wav, (0, max_wave_length - len(wav)), \"constant\")\n",
    "\n",
    "    # Split the utterance into partials and forward them through the model\n",
    "    mel = audio.wav_to_mel_spectrogram(wav)\n",
    "    mels = np.array([mel[s] for s in mel_slices])\n",
    "    with torch.no_grad():\n",
    "        mels = torch.from_numpy(mels).to(torch.device('cpu'))\n",
    "        output_layer = smodel.output(0)\n",
    "        partial_embeds = smodel(mels)[output_layer]\n",
    "\n",
    "    # Compute the utterance embedding from the partial embeddings\n",
    "    raw_embed = np.mean(partial_embeds, axis=0)\n",
    "    embed = raw_embed / np.linalg.norm(raw_embed, 2)\n",
    "\n",
    "    if return_partials:\n",
    "        return embed, partial_embeds, wav_slices\n",
    "    return embed"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T22:22:18.175476Z",
     "end_time": "2023-05-09T22:22:18.234762Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then compile model and check inference."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "compiled_smodel = core.compile_model(ir_smodel, 'CPU')\n",
    "\n",
    "wav_tgt, _ = librosa.load(tgt, sr=hps.data.sampling_rate)\n",
    "wav_tgt, _ = librosa.effects.trim(wav_tgt, top_db=20)\n",
    "\n",
    "g_tgt = embed_utterance(wav_tgt, compiled_smodel)\n",
    "g_tgt = torch.from_numpy(g_tgt).unsqueeze(0)\n",
    "print(g_tgt.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T22:22:18.234762Z",
     "end_time": "2023-05-09T22:22:18.513768Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convert SynthesizerTrn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also, it is possible to convert a model to IR format without direct conversion to ONNX. Just set parameter `use_legacy_frontend` to `True`, and Model Optimizer will do it for you under the hood."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define forward as infer for compatibility\n",
    "net_g.forward = net_g.infer\n",
    "\n",
    "dummy_input_1 = torch.randn(1, 1024, 81)\n",
    "dummy_input_2 = torch.randn(1, 256)\n",
    "\n",
    "ir_net_g_model = mo.convert_model(\n",
    "    net_g,\n",
    "    example_input=(dummy_input_1, dummy_input_2),\n",
    "    input_shape=[[-1, 1024, -1], [-1, 256]],\n",
    "    compress_to_fp16=True,\n",
    "    progress=True,\n",
    "    use_legacy_frontend=True\n",
    ")\n",
    "compiled_ir_net_g_model = core.compile_model(ir_net_g_model, 'CPU')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T22:22:18.513768Z",
     "end_time": "2023-05-09T22:22:29.131580Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And now we can check inference using only IR models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for line in tqdm(zip(titles, srcs, tgts)):\n",
    "        title, src, tgt = line\n",
    "        # tgt\n",
    "        wav_tgt, _ = librosa.load(tgt, sr=hps.data.sampling_rate)\n",
    "        wav_tgt, _ = librosa.effects.trim(wav_tgt, top_db=20)\n",
    "\n",
    "        g_tgt = embed_utterance(wav_tgt, compiled_smodel)\n",
    "        g_tgt = torch.from_numpy(g_tgt).unsqueeze(0)\n",
    "\n",
    "        # src\n",
    "        wav_src, _ = librosa.load(src, sr=hps.data.sampling_rate)\n",
    "        wav_src = torch.from_numpy(wav_src).unsqueeze(0)\n",
    "        output_layer = compiled_cmodel.output(0)\n",
    "        c = compiled_cmodel(wav_src)[output_layer]\n",
    "        c = c.transpose((0, 2, 1))\n",
    "\n",
    "        output_layer = compiled_ir_net_g_model.output(0)\n",
    "        tgt_audio = compiled_ir_net_g_model((c, g_tgt))[output_layer]\n",
    "        tgt_audio = tgt_audio[0][0]\n",
    "\n",
    "        timestamp = time.strftime(\"%m-%d_%H-%M\", time.localtime())\n",
    "        write(os.path.join('outputs/freevc', \"{}.wav\".format(timestamp + \"_\" + title)), hps.data.sampling_rate,\n",
    "              tgt_audio)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-09T22:22:29.155337Z",
     "end_time": "2023-05-09T22:22:40.526795Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Result audio files should be available in 'outputs/freevc' and you can check them and compare with generated earlier."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
