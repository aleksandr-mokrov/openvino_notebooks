{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-18T13:21:54.410428300Z",
     "start_time": "2023-08-18T13:21:39.517274900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-18 15:21:50.997081: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-18 15:21:51.043936: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-18 15:21:51.253749: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-18 15:21:51.257577: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-18 15:21:52.390549: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import TapasForQuestionAnswering\n",
    "from transformers import TapasTokenizer\n",
    "from transformers import pipeline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "model = TapasForQuestionAnswering.from_pretrained('google/tapas-large-finetuned-wtq')\n",
    "tokenizer = TapasTokenizer.from_pretrained(\"google/tapas-large-finetuned-wtq\")\n",
    "\n",
    "data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\", \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "question = \"how many movies does Leonardo Di Caprio have?\"\n",
    "\n",
    "tqa = pipeline(task=\"table-question-answering\", model=model, tokenizer=tokenizer)\n",
    "print(tqa(table=table, query=question)['cells'][0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T13:25:48.739647Z",
     "start_time": "2023-08-18T13:24:13.740722600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:1601: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  self.indices = torch.as_tensor(indices)\n",
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:1602: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  self.num_segments = torch.as_tensor(num_segments, device=indices.device)\n",
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:1704: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  batch_size = torch.prod(torch.tensor(list(index.batch_shape())))\n",
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:1780: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  [torch.as_tensor([-1], dtype=torch.long), torch.as_tensor(vector_shape, dtype=torch.long)], dim=0\n",
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:1783: TracerWarning: Converting a tensor to a Python list might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  flat_values = values.reshape(flattened_shape.tolist())\n",
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:1785: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  out = torch.zeros(int(flat_index.num_segments), dtype=torch.float, device=flat_values.device)\n",
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:1793: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.as_tensor(index.batch_shape(), dtype=torch.long),\n",
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:1794: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.as_tensor([index.num_segments], dtype=torch.long),\n",
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:1795: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.as_tensor(vector_shape, dtype=torch.long),\n",
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:1800: TracerWarning: Converting a tensor to a Python list might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  output_values = segment_means.clone().view(new_shape.tolist()).to(values.dtype)\n",
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:1731: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  batch_shape = torch.as_tensor(\n",
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:1735: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  num_segments = torch.as_tensor(num_segments)  # create a rank 0 tensor (scalar) containing num_segments (e.g. 64)\n",
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:1746: TracerWarning: Converting a tensor to a Python list might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  new_shape = [int(x) for x in new_tensor.tolist()]\n",
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:1749: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  multiples = torch.cat([batch_shape, torch.as_tensor([1])], dim=0)\n",
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:1750: TracerWarning: Converting a tensor to a Python list might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  indices = indices.repeat(multiples.tolist())\n",
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:309: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.as_tensor(self.config.max_position_embeddings - 1, device=device), position - first_position\n",
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:1261: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  indices=torch.min(row_ids, torch.as_tensor(self.config.max_num_rows - 1, device=row_ids.device)),\n",
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:1266: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  indices=torch.min(column_ids, torch.as_tensor(self.config.max_num_columns - 1, device=column_ids.device)),\n",
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:1958: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  column_logits += CLOSE_ENOUGH_TO_LOG_ZERO * torch.as_tensor(\n",
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:1963: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  column_logits += CLOSE_ENOUGH_TO_LOG_ZERO * torch.as_tensor(\n",
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:1999: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  labels_per_column, _ = reduce_sum(torch.as_tensor(labels, dtype=torch.float32, device=labels.device), col_index)\n",
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:2022: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.as_tensor(labels, dtype=torch.long, device=labels.device), cell_index\n",
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:2029: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  column_mask = torch.as_tensor(\n",
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:2054: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  selected_column_id = torch.as_tensor(\n",
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/transformers/models/tapas/modeling_tapas.py:2059: TracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  selected_column_mask = torch.as_tensor(\n"
     ]
    },
    {
     "ename": "UnsupportedOperatorError",
     "evalue": "Exporting the operator 'aten::scatter_reduce' to ONNX opset version 14 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mUnsupportedOperatorError\u001B[0m                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 13\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Modify the input shape of the dummy_input dictionary\u001B[39;00m\n\u001B[1;32m      6\u001B[0m dummy_input \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m: torch\u001B[38;5;241m.\u001B[39mzeros((batch_size, sequence_length), dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong),\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m: torch\u001B[38;5;241m.\u001B[39mzeros((batch_size, sequence_length), dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong),\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoken_type_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m: torch\u001B[38;5;241m.\u001B[39mzeros((batch_size, sequence_length, \u001B[38;5;241m7\u001B[39m), dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong),\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;66;03m#\"attention_mask_for_classification\": torch.zeros((batch_size, sequence_length), dtype=torch.float),\u001B[39;00m\n\u001B[1;32m     11\u001B[0m }\n\u001B[0;32m---> 13\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43monnx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexport\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdummy_input\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtapas.onnx\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdummy_input\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeys\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moutput\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdynamic_axes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbatch_size\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msequence_length\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mattention_mask\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbatch_size\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msequence_length\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtoken_type_ids\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbatch_size\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msequence_length\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnum_labels\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m#\"attention_mask_for_classification\": {0: \"batch_size\", 1: \"sequence_length\"},\u001B[39;49;00m\n\u001B[1;32m     24\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m#\"output\": {0: \"batch_size\", 1: \"sequence_length\", 2: \"num_labels\"},\u001B[39;49;00m\n\u001B[1;32m     25\u001B[0m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/torch/onnx/utils.py:504\u001B[0m, in \u001B[0;36mexport\u001B[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001B[0m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;129m@_beartype\u001B[39m\u001B[38;5;241m.\u001B[39mbeartype\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mexport\u001B[39m(\n\u001B[1;32m    188\u001B[0m     model: Union[torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mModule, torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mScriptModule, torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mScriptFunction],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    204\u001B[0m     export_modules_as_functions: Union[\u001B[38;5;28mbool\u001B[39m, Collection[Type[torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mModule]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    205\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    206\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Exports a model into ONNX format.\u001B[39;00m\n\u001B[1;32m    207\u001B[0m \n\u001B[1;32m    208\u001B[0m \u001B[38;5;124;03m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    501\u001B[0m \u001B[38;5;124;03m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001B[39;00m\n\u001B[1;32m    502\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 504\u001B[0m     \u001B[43m_export\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    505\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    506\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    507\u001B[0m \u001B[43m        \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    508\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexport_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    509\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    510\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    511\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    512\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    513\u001B[0m \u001B[43m        \u001B[49m\u001B[43moperator_export_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moperator_export_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    514\u001B[0m \u001B[43m        \u001B[49m\u001B[43mopset_version\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mopset_version\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    515\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdo_constant_folding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdo_constant_folding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    516\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdynamic_axes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdynamic_axes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    517\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_initializers_as_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_initializers_as_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    518\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcustom_opsets\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcustom_opsets\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    519\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexport_modules_as_functions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexport_modules_as_functions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    520\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/torch/onnx/utils.py:1529\u001B[0m, in \u001B[0;36m_export\u001B[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001B[0m\n\u001B[1;32m   1526\u001B[0m     dynamic_axes \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   1527\u001B[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001B[0;32m-> 1529\u001B[0m graph, params_dict, torch_out \u001B[38;5;241m=\u001B[39m \u001B[43m_model_to_graph\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1530\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1531\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1532\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1533\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1534\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1535\u001B[0m \u001B[43m    \u001B[49m\u001B[43moperator_export_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1536\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_do_constant_folding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1537\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfixed_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfixed_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1538\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1539\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdynamic_axes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdynamic_axes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1540\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1542\u001B[0m \u001B[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001B[39;00m\n\u001B[1;32m   1543\u001B[0m defer_weight_export \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1544\u001B[0m     export_type \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _exporter_states\u001B[38;5;241m.\u001B[39mExportTypes\u001B[38;5;241m.\u001B[39mPROTOBUF_FILE\n\u001B[1;32m   1545\u001B[0m )\n",
      "File \u001B[0;32m~/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/torch/onnx/utils.py:1115\u001B[0m, in \u001B[0;36m_model_to_graph\u001B[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001B[0m\n\u001B[1;32m   1112\u001B[0m params_dict \u001B[38;5;241m=\u001B[39m _get_named_param_dict(graph, params)\n\u001B[1;32m   1114\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1115\u001B[0m     graph \u001B[38;5;241m=\u001B[39m \u001B[43m_optimize_graph\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1116\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgraph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1117\u001B[0m \u001B[43m        \u001B[49m\u001B[43moperator_export_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1118\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_disable_torch_constant_prop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_disable_torch_constant_prop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1119\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfixed_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfixed_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1120\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1121\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdynamic_axes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdynamic_axes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1122\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1123\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1124\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1125\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1126\u001B[0m     torch\u001B[38;5;241m.\u001B[39monnx\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch IR graph at exception: \u001B[39m\u001B[38;5;124m\"\u001B[39m, graph)\n",
      "File \u001B[0;32m~/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/torch/onnx/utils.py:663\u001B[0m, in \u001B[0;36m_optimize_graph\u001B[0;34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\u001B[0m\n\u001B[1;32m    660\u001B[0m     _C\u001B[38;5;241m.\u001B[39m_jit_pass_onnx_set_dynamic_input_shape(graph, dynamic_axes, input_names)\n\u001B[1;32m    661\u001B[0m _C\u001B[38;5;241m.\u001B[39m_jit_pass_onnx_lint(graph)\n\u001B[0;32m--> 663\u001B[0m graph \u001B[38;5;241m=\u001B[39m \u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jit_pass_onnx\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgraph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moperator_export_type\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    664\u001B[0m _C\u001B[38;5;241m.\u001B[39m_jit_pass_onnx_lint(graph)\n\u001B[1;32m    665\u001B[0m _C\u001B[38;5;241m.\u001B[39m_jit_pass_lint(graph)\n",
      "File \u001B[0;32m~/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/torch/onnx/utils.py:1909\u001B[0m, in \u001B[0;36m_run_symbolic_function\u001B[0;34m(graph, block, node, inputs, env, operator_export_type)\u001B[0m\n\u001B[1;32m   1905\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m namespace \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124monnx\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1906\u001B[0m         \u001B[38;5;66;03m# Clone node to trigger ONNX shape inference\u001B[39;00m\n\u001B[1;32m   1907\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m graph_context\u001B[38;5;241m.\u001B[39mop(op_name, \u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mattrs, outputs\u001B[38;5;241m=\u001B[39mnode\u001B[38;5;241m.\u001B[39moutputsSize())  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[0;32m-> 1909\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m errors\u001B[38;5;241m.\u001B[39mUnsupportedOperatorError(\n\u001B[1;32m   1910\u001B[0m         domain,\n\u001B[1;32m   1911\u001B[0m         op_name,\n\u001B[1;32m   1912\u001B[0m         opset_version,\n\u001B[1;32m   1913\u001B[0m         symbolic_function_group\u001B[38;5;241m.\u001B[39mget_min_supported()\n\u001B[1;32m   1914\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m symbolic_function_group\n\u001B[1;32m   1915\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1916\u001B[0m     )\n\u001B[1;32m   1918\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m:\n\u001B[1;32m   1919\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m operator_export_type \u001B[38;5;241m==\u001B[39m _C_onnx\u001B[38;5;241m.\u001B[39mOperatorExportTypes\u001B[38;5;241m.\u001B[39mONNX_FALLTHROUGH:\n",
      "\u001B[0;31mUnsupportedOperatorError\u001B[0m: Exporting the operator 'aten::scatter_reduce' to ONNX opset version 14 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues"
     ]
    }
   ],
   "source": [
    "# Define the input shape\n",
    "batch_size = 1\n",
    "sequence_length = 29\n",
    "\n",
    "# Modify the input shape of the dummy_input dictionary\n",
    "dummy_input = {\n",
    "    \"input_ids\": torch.zeros((batch_size, sequence_length), dtype=torch.long),\n",
    "    \"attention_mask\": torch.zeros((batch_size, sequence_length), dtype=torch.long),\n",
    "    \"token_type_ids\": torch.zeros((batch_size, sequence_length, 7), dtype=torch.long),\n",
    "    #\"attention_mask_for_classification\": torch.zeros((batch_size, sequence_length), dtype=torch.float),\n",
    "}\n",
    "\n",
    "torch.onnx.export(\n",
    "    model=model,\n",
    "    args=(dummy_input,),\n",
    "    f=\"tapas.onnx\",\n",
    "    input_names=list(dummy_input.keys()),\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"token_type_ids\": {0: \"batch_size\", 1: \"sequence_length\", 2: \"num_labels\" },\n",
    "        #\"attention_mask_for_classification\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        #\"output\": {0: \"batch_size\", 1: \"sequence_length\", 2: \"num_labels\"},\n",
    "    },\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T13:25:51.125085600Z",
     "start_time": "2023-08-18T13:25:48.739647Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARNING ]  Please fix your imports. Module %s has been moved to %s. The old module will be deleted in version %s.\n"
     ]
    },
    {
     "ename": "OpConversionFailure",
     "evalue": "Check 'is_conversion_successful' failed at src/frontends/pytorch/src/frontend.cpp:138:\nFrontEnd API failed with OpConversionFailure:\nModel wasn't fully converted. Failed operations detailed log:\n-- prim::Constant with a message:\nString constant cannot be converted to OpenVINO opset and should be removed by consuming operation.\n-- prim::ListConstruct with a message:\nException happened during conversion of operation __module.model/prim::ListConstruct_1422 with schema (no schema)\nCheck 'c_node' failed at src/frontends/pytorch/src/op/list_construct.cpp:25:\nFrontEnd API failed with OpConversionFailure:\nTranslation for prim::ListConstruct support only constant inputs\n\n-- prim::ListUnpack with a message:\nprim::ListUnpack: unsupported input node: util::PtFrameworkNode __module.model/aten::broadcast_tensors_1424 (__module.model/prim::ListConstruct_1423[0]:dynamic[...]) -> (dynamic[...])\nSummary:\n-- No conversion rule found for operations: aten::broadcast_tensors, aten::logical_and, aten::scatter_reduce\n-- Conversion is failed for: prim::Constant, prim::ListConstruct, prim::ListUnpack\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOpConversionFailure\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mopenvino\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[43mopenvino\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexample_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdummy_input\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/openvino/tools/ovc/convert.py:121\u001B[0m, in \u001B[0;36mconvert_model\u001B[0;34m(input_model, input, output, example_input, extension, verbose, share_weights, example_output)\u001B[0m\n\u001B[1;32m    119\u001B[0m logger_state \u001B[38;5;241m=\u001B[39m get_logger_state()\n\u001B[1;32m    120\u001B[0m cli_parser \u001B[38;5;241m=\u001B[39m get_all_cli_parser()\n\u001B[0;32m--> 121\u001B[0m ov_model, _ \u001B[38;5;241m=\u001B[39m \u001B[43m_convert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcli_parser\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    122\u001B[0m restore_logger_state(logger_state)\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ov_model\n",
      "File \u001B[0;32m~/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/openvino/tools/ovc/convert_impl.py:518\u001B[0m, in \u001B[0;36m_convert\u001B[0;34m(cli_parser, args, python_api_used)\u001B[0m\n\u001B[1;32m    516\u001B[0m send_conversion_result(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfail\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m python_api_used:\n\u001B[0;32m--> 518\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    519\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, argv\n",
      "File \u001B[0;32m~/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/openvino/tools/ovc/convert_impl.py:470\u001B[0m, in \u001B[0;36m_convert\u001B[0;34m(cli_parser, args, python_api_used)\u001B[0m\n\u001B[1;32m    466\u001B[0m     argv\u001B[38;5;241m.\u001B[39moutput_model \u001B[38;5;241m=\u001B[39m get_model_name_from_args(argv)\n\u001B[1;32m    468\u001B[0m argv\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m=\u001B[39m model_framework\n\u001B[0;32m--> 470\u001B[0m ov_model \u001B[38;5;241m=\u001B[39m \u001B[43mdriver\u001B[49m\u001B[43m(\u001B[49m\u001B[43margv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mconversion_parameters\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_default_params\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    472\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inp_model_is_object \u001B[38;5;129;01mand\u001B[39;00m model_framework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpaddle\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    473\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m paddle_runtime_converter:\n",
      "File \u001B[0;32m~/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/openvino/tools/ovc/convert_impl.py:202\u001B[0m, in \u001B[0;36mdriver\u001B[0;34m(argv, non_default_params)\u001B[0m\n\u001B[1;32m    198\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;28mstr\u001B[39m(non_default_params))\n\u001B[1;32m    200\u001B[0m start_time \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mdatetime\u001B[38;5;241m.\u001B[39mnow()\n\u001B[0;32m--> 202\u001B[0m ov_model \u001B[38;5;241m=\u001B[39m moc_emit_ir(\u001B[43mprepare_ir\u001B[49m\u001B[43m(\u001B[49m\u001B[43margv\u001B[49m\u001B[43m)\u001B[49m, argv)\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m argv\u001B[38;5;241m.\u001B[39mverbose:\n\u001B[1;32m    205\u001B[0m     elapsed_time \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mdatetime\u001B[38;5;241m.\u001B[39mnow() \u001B[38;5;241m-\u001B[39m start_time\n",
      "File \u001B[0;32m~/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/openvino/tools/ovc/convert_impl.py:148\u001B[0m, in \u001B[0;36mprepare_ir\u001B[0;34m(argv)\u001B[0m\n\u001B[1;32m    146\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m extension \u001B[38;5;129;01min\u001B[39;00m argv\u001B[38;5;241m.\u001B[39mextension:\n\u001B[1;32m    147\u001B[0m             moc_front_end\u001B[38;5;241m.\u001B[39madd_extension(extension)\n\u001B[0;32m--> 148\u001B[0m     ov_model \u001B[38;5;241m=\u001B[39m \u001B[43mmoc_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43margv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmoc_front_end\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    149\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ov_model\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m argv\u001B[38;5;241m.\u001B[39minput_model:\n",
      "File \u001B[0;32m~/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/openvino/tools/ovc/moc_frontend/pipeline.py:235\u001B[0m, in \u001B[0;36mmoc_pipeline\u001B[0;34m(argv, moc_front_end)\u001B[0m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlayout_values\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m argv \u001B[38;5;129;01mand\u001B[39;00m argv\u001B[38;5;241m.\u001B[39mlayout_values:\n\u001B[1;32m    232\u001B[0m     layout_values \u001B[38;5;241m=\u001B[39m update_layout_to_dict(model_inputs, argv\u001B[38;5;241m.\u001B[39mlayout_values,\n\u001B[1;32m    233\u001B[0m                                           \u001B[38;5;28;01mlambda\u001B[39;00m input_place: input_place\u001B[38;5;241m.\u001B[39mget_names())\n\u001B[0;32m--> 235\u001B[0m ov_model \u001B[38;5;241m=\u001B[39m \u001B[43mmoc_front_end\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    237\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ov_model\n",
      "File \u001B[0;32m~/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/openvino/frontend/frontend.py:18\u001B[0m, in \u001B[0;36mFrontEnd.convert\u001B[0;34m(self, model)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvert\u001B[39m(\u001B[38;5;28mself\u001B[39m, model: Union[Model, InputModel]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Model:\n\u001B[0;32m---> 18\u001B[0m     converted_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, InputModel):\n\u001B[1;32m     20\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m Model(converted_model)\n",
      "\u001B[0;31mOpConversionFailure\u001B[0m: Check 'is_conversion_successful' failed at src/frontends/pytorch/src/frontend.cpp:138:\nFrontEnd API failed with OpConversionFailure:\nModel wasn't fully converted. Failed operations detailed log:\n-- prim::Constant with a message:\nString constant cannot be converted to OpenVINO opset and should be removed by consuming operation.\n-- prim::ListConstruct with a message:\nException happened during conversion of operation __module.model/prim::ListConstruct_1422 with schema (no schema)\nCheck 'c_node' failed at src/frontends/pytorch/src/op/list_construct.cpp:25:\nFrontEnd API failed with OpConversionFailure:\nTranslation for prim::ListConstruct support only constant inputs\n\n-- prim::ListUnpack with a message:\nprim::ListUnpack: unsupported input node: util::PtFrameworkNode __module.model/aten::broadcast_tensors_1424 (__module.model/prim::ListConstruct_1423[0]:dynamic[...]) -> (dynamic[...])\nSummary:\n-- No conversion rule found for operations: aten::broadcast_tensors, aten::logical_and, aten::scatter_reduce\n-- Conversion is failed for: prim::Constant, prim::ListConstruct, prim::ListUnpack\n"
     ]
    }
   ],
   "source": [
    "import openvino\n",
    "\n",
    "openvino.convert_model(\n",
    "    model,\n",
    "    example_input=dummy_input\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T13:27:32.451526900Z",
     "start_time": "2023-08-18T13:27:27.032525Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "UnsupportedOperatorError",
     "evalue": "Exporting the operator 'aten::scatter_reduce' to ONNX opset version 14 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mUnsupportedOperatorError\u001B[0m                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 13\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Modify the input shape of the dummy_input dictionary\u001B[39;00m\n\u001B[1;32m      6\u001B[0m dummy_input \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m: torch\u001B[38;5;241m.\u001B[39mzeros((batch_size, sequence_length), dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong),\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoken_type_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m: torch\u001B[38;5;241m.\u001B[39mzeros((batch_size, sequence_length, \u001B[38;5;241m7\u001B[39m), dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong),\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mposition_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m: torch\u001B[38;5;241m.\u001B[39mzeros((batch_size, sequence_length), dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong),\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;66;03m#\"inputs_embeds\": torch.zeros((batch_size, sequence_length, 1024), dtype=torch.float),\u001B[39;00m\n\u001B[1;32m     11\u001B[0m }\n\u001B[0;32m---> 13\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43monnx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexport\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdummy_input\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtapas.onnx\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdummy_input\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeys\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moutput\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdynamic_axes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbatch_size\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msequence_length\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtoken_type_ids\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbatch_size\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msequence_length\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnum_labels\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mposition_ids\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbatch_size\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msequence_length\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m#\"inputs_embeds\": {0: \"batch_size\", 1: \"sequence_length\", 2: \"num_labels\"},\u001B[39;49;00m\n\u001B[1;32m     24\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m#\"output\": {0: \"batch_size\", 1: \"sequence_length\", 2: \"num_labels\"},\u001B[39;49;00m\n\u001B[1;32m     25\u001B[0m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/torch/onnx/utils.py:504\u001B[0m, in \u001B[0;36mexport\u001B[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions)\u001B[0m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;129m@_beartype\u001B[39m\u001B[38;5;241m.\u001B[39mbeartype\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mexport\u001B[39m(\n\u001B[1;32m    188\u001B[0m     model: Union[torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mModule, torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mScriptModule, torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mScriptFunction],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    204\u001B[0m     export_modules_as_functions: Union[\u001B[38;5;28mbool\u001B[39m, Collection[Type[torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mModule]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    205\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    206\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Exports a model into ONNX format.\u001B[39;00m\n\u001B[1;32m    207\u001B[0m \n\u001B[1;32m    208\u001B[0m \u001B[38;5;124;03m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    501\u001B[0m \u001B[38;5;124;03m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001B[39;00m\n\u001B[1;32m    502\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 504\u001B[0m     \u001B[43m_export\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    505\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    506\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    507\u001B[0m \u001B[43m        \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    508\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexport_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    509\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    510\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    511\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    512\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    513\u001B[0m \u001B[43m        \u001B[49m\u001B[43moperator_export_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moperator_export_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    514\u001B[0m \u001B[43m        \u001B[49m\u001B[43mopset_version\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mopset_version\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    515\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdo_constant_folding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdo_constant_folding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    516\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdynamic_axes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdynamic_axes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    517\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_initializers_as_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeep_initializers_as_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    518\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcustom_opsets\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcustom_opsets\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    519\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexport_modules_as_functions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexport_modules_as_functions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    520\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/torch/onnx/utils.py:1529\u001B[0m, in \u001B[0;36m_export\u001B[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions)\u001B[0m\n\u001B[1;32m   1526\u001B[0m     dynamic_axes \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   1527\u001B[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001B[0;32m-> 1529\u001B[0m graph, params_dict, torch_out \u001B[38;5;241m=\u001B[39m \u001B[43m_model_to_graph\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1530\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1531\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1532\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1533\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1534\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1535\u001B[0m \u001B[43m    \u001B[49m\u001B[43moperator_export_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1536\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_do_constant_folding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1537\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfixed_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfixed_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1538\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1539\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdynamic_axes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdynamic_axes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1540\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1542\u001B[0m \u001B[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001B[39;00m\n\u001B[1;32m   1543\u001B[0m defer_weight_export \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1544\u001B[0m     export_type \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _exporter_states\u001B[38;5;241m.\u001B[39mExportTypes\u001B[38;5;241m.\u001B[39mPROTOBUF_FILE\n\u001B[1;32m   1545\u001B[0m )\n",
      "File \u001B[0;32m~/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/torch/onnx/utils.py:1115\u001B[0m, in \u001B[0;36m_model_to_graph\u001B[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001B[0m\n\u001B[1;32m   1112\u001B[0m params_dict \u001B[38;5;241m=\u001B[39m _get_named_param_dict(graph, params)\n\u001B[1;32m   1114\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1115\u001B[0m     graph \u001B[38;5;241m=\u001B[39m \u001B[43m_optimize_graph\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1116\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgraph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1117\u001B[0m \u001B[43m        \u001B[49m\u001B[43moperator_export_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1118\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_disable_torch_constant_prop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_disable_torch_constant_prop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1119\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfixed_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfixed_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1120\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1121\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdynamic_axes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdynamic_axes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1122\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1123\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1124\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1125\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1126\u001B[0m     torch\u001B[38;5;241m.\u001B[39monnx\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch IR graph at exception: \u001B[39m\u001B[38;5;124m\"\u001B[39m, graph)\n",
      "File \u001B[0;32m~/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/torch/onnx/utils.py:663\u001B[0m, in \u001B[0;36m_optimize_graph\u001B[0;34m(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict, dynamic_axes, input_names, module)\u001B[0m\n\u001B[1;32m    660\u001B[0m     _C\u001B[38;5;241m.\u001B[39m_jit_pass_onnx_set_dynamic_input_shape(graph, dynamic_axes, input_names)\n\u001B[1;32m    661\u001B[0m _C\u001B[38;5;241m.\u001B[39m_jit_pass_onnx_lint(graph)\n\u001B[0;32m--> 663\u001B[0m graph \u001B[38;5;241m=\u001B[39m \u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jit_pass_onnx\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgraph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moperator_export_type\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    664\u001B[0m _C\u001B[38;5;241m.\u001B[39m_jit_pass_onnx_lint(graph)\n\u001B[1;32m    665\u001B[0m _C\u001B[38;5;241m.\u001B[39m_jit_pass_lint(graph)\n",
      "File \u001B[0;32m~/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/torch/onnx/utils.py:1909\u001B[0m, in \u001B[0;36m_run_symbolic_function\u001B[0;34m(graph, block, node, inputs, env, operator_export_type)\u001B[0m\n\u001B[1;32m   1905\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m namespace \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124monnx\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1906\u001B[0m         \u001B[38;5;66;03m# Clone node to trigger ONNX shape inference\u001B[39;00m\n\u001B[1;32m   1907\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m graph_context\u001B[38;5;241m.\u001B[39mop(op_name, \u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mattrs, outputs\u001B[38;5;241m=\u001B[39mnode\u001B[38;5;241m.\u001B[39moutputsSize())  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[0;32m-> 1909\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m errors\u001B[38;5;241m.\u001B[39mUnsupportedOperatorError(\n\u001B[1;32m   1910\u001B[0m         domain,\n\u001B[1;32m   1911\u001B[0m         op_name,\n\u001B[1;32m   1912\u001B[0m         opset_version,\n\u001B[1;32m   1913\u001B[0m         symbolic_function_group\u001B[38;5;241m.\u001B[39mget_min_supported()\n\u001B[1;32m   1914\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m symbolic_function_group\n\u001B[1;32m   1915\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1916\u001B[0m     )\n\u001B[1;32m   1918\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m:\n\u001B[1;32m   1919\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m operator_export_type \u001B[38;5;241m==\u001B[39m _C_onnx\u001B[38;5;241m.\u001B[39mOperatorExportTypes\u001B[38;5;241m.\u001B[39mONNX_FALLTHROUGH:\n",
      "\u001B[0;31mUnsupportedOperatorError\u001B[0m: Exporting the operator 'aten::scatter_reduce' to ONNX opset version 14 is not supported. Please feel free to request support or submit a pull request on PyTorch GitHub: https://github.com/pytorch/pytorch/issues"
     ]
    }
   ],
   "source": [
    "# Define the input shape\n",
    "batch_size = 1\n",
    "sequence_length = 29\n",
    "\n",
    "# Modify the input shape of the dummy_input dictionary\n",
    "dummy_input = {\n",
    "    \"input_ids\": torch.zeros((batch_size, sequence_length), dtype=torch.long),\n",
    "    \"token_type_ids\": torch.zeros((batch_size, sequence_length, 7), dtype=torch.long),\n",
    "    \"position_ids\": torch.zeros((batch_size, sequence_length), dtype=torch.long),\n",
    "    #\"inputs_embeds\": torch.zeros((batch_size, sequence_length, 1024), dtype=torch.float),\n",
    "}\n",
    "\n",
    "torch.onnx.export(\n",
    "    model=model,\n",
    "    args=(dummy_input,),\n",
    "    f=\"tapas.onnx\",\n",
    "    input_names=list(dummy_input.keys()),\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"token_type_ids\": {0: \"batch_size\", 1: \"sequence_length\", 2: \"num_labels\"},\n",
    "        \"position_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        #\"inputs_embeds\": {0: \"batch_size\", 1: \"sequence_length\", 2: \"num_labels\"},\n",
    "        #\"output\": {0: \"batch_size\", 1: \"sequence_length\", 2: \"num_labels\"},\n",
    "    },\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-08-18T13:26:00.170546500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "OpConversionFailure",
     "evalue": "Check 'is_conversion_successful' failed at src/frontends/pytorch/src/frontend.cpp:138:\nFrontEnd API failed with OpConversionFailure:\nModel wasn't fully converted. Failed operations detailed log:\n-- prim::Constant with a message:\nString constant cannot be converted to OpenVINO opset and should be removed by consuming operation.\n-- prim::ListConstruct with a message:\nException happened during conversion of operation __module.model/prim::ListConstruct_1422 with schema (no schema)\nCheck 'c_node' failed at src/frontends/pytorch/src/op/list_construct.cpp:25:\nFrontEnd API failed with OpConversionFailure:\nTranslation for prim::ListConstruct support only constant inputs\n\n-- prim::ListUnpack with a message:\nprim::ListUnpack: unsupported input node: util::PtFrameworkNode __module.model/aten::broadcast_tensors_1424 (__module.model/prim::ListConstruct_1423[0]:dynamic[...]) -> (dynamic[...])\nSummary:\n-- No conversion rule found for operations: aten::broadcast_tensors, aten::logical_and, aten::scatter_reduce\n-- Conversion is failed for: prim::Constant, prim::ListConstruct, prim::ListUnpack\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOpConversionFailure\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mopenvino\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[43mopenvino\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexample_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdummy_input\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/openvino/tools/ovc/convert.py:121\u001B[0m, in \u001B[0;36mconvert_model\u001B[0;34m(input_model, input, output, example_input, extension, verbose, share_weights, example_output)\u001B[0m\n\u001B[1;32m    119\u001B[0m logger_state \u001B[38;5;241m=\u001B[39m get_logger_state()\n\u001B[1;32m    120\u001B[0m cli_parser \u001B[38;5;241m=\u001B[39m get_all_cli_parser()\n\u001B[0;32m--> 121\u001B[0m ov_model, _ \u001B[38;5;241m=\u001B[39m \u001B[43m_convert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcli_parser\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    122\u001B[0m restore_logger_state(logger_state)\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ov_model\n",
      "File \u001B[0;32m~/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/openvino/tools/ovc/convert_impl.py:518\u001B[0m, in \u001B[0;36m_convert\u001B[0;34m(cli_parser, args, python_api_used)\u001B[0m\n\u001B[1;32m    516\u001B[0m send_conversion_result(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfail\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m python_api_used:\n\u001B[0;32m--> 518\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    519\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, argv\n",
      "File \u001B[0;32m~/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/openvino/tools/ovc/convert_impl.py:470\u001B[0m, in \u001B[0;36m_convert\u001B[0;34m(cli_parser, args, python_api_used)\u001B[0m\n\u001B[1;32m    466\u001B[0m     argv\u001B[38;5;241m.\u001B[39moutput_model \u001B[38;5;241m=\u001B[39m get_model_name_from_args(argv)\n\u001B[1;32m    468\u001B[0m argv\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m=\u001B[39m model_framework\n\u001B[0;32m--> 470\u001B[0m ov_model \u001B[38;5;241m=\u001B[39m \u001B[43mdriver\u001B[49m\u001B[43m(\u001B[49m\u001B[43margv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mconversion_parameters\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_default_params\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    472\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inp_model_is_object \u001B[38;5;129;01mand\u001B[39;00m model_framework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpaddle\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    473\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m paddle_runtime_converter:\n",
      "File \u001B[0;32m~/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/openvino/tools/ovc/convert_impl.py:202\u001B[0m, in \u001B[0;36mdriver\u001B[0;34m(argv, non_default_params)\u001B[0m\n\u001B[1;32m    198\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;28mstr\u001B[39m(non_default_params))\n\u001B[1;32m    200\u001B[0m start_time \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mdatetime\u001B[38;5;241m.\u001B[39mnow()\n\u001B[0;32m--> 202\u001B[0m ov_model \u001B[38;5;241m=\u001B[39m moc_emit_ir(\u001B[43mprepare_ir\u001B[49m\u001B[43m(\u001B[49m\u001B[43margv\u001B[49m\u001B[43m)\u001B[49m, argv)\n\u001B[1;32m    204\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m argv\u001B[38;5;241m.\u001B[39mverbose:\n\u001B[1;32m    205\u001B[0m     elapsed_time \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mdatetime\u001B[38;5;241m.\u001B[39mnow() \u001B[38;5;241m-\u001B[39m start_time\n",
      "File \u001B[0;32m~/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/openvino/tools/ovc/convert_impl.py:148\u001B[0m, in \u001B[0;36mprepare_ir\u001B[0;34m(argv)\u001B[0m\n\u001B[1;32m    146\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m extension \u001B[38;5;129;01min\u001B[39;00m argv\u001B[38;5;241m.\u001B[39mextension:\n\u001B[1;32m    147\u001B[0m             moc_front_end\u001B[38;5;241m.\u001B[39madd_extension(extension)\n\u001B[0;32m--> 148\u001B[0m     ov_model \u001B[38;5;241m=\u001B[39m \u001B[43mmoc_pipeline\u001B[49m\u001B[43m(\u001B[49m\u001B[43margv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmoc_front_end\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    149\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ov_model\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m argv\u001B[38;5;241m.\u001B[39minput_model:\n",
      "File \u001B[0;32m~/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/openvino/tools/ovc/moc_frontend/pipeline.py:235\u001B[0m, in \u001B[0;36mmoc_pipeline\u001B[0;34m(argv, moc_front_end)\u001B[0m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlayout_values\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m argv \u001B[38;5;129;01mand\u001B[39;00m argv\u001B[38;5;241m.\u001B[39mlayout_values:\n\u001B[1;32m    232\u001B[0m     layout_values \u001B[38;5;241m=\u001B[39m update_layout_to_dict(model_inputs, argv\u001B[38;5;241m.\u001B[39mlayout_values,\n\u001B[1;32m    233\u001B[0m                                           \u001B[38;5;28;01mlambda\u001B[39;00m input_place: input_place\u001B[38;5;241m.\u001B[39mget_names())\n\u001B[0;32m--> 235\u001B[0m ov_model \u001B[38;5;241m=\u001B[39m \u001B[43mmoc_front_end\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_model\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    237\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ov_model\n",
      "File \u001B[0;32m~/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/openvino/frontend/frontend.py:18\u001B[0m, in \u001B[0;36mFrontEnd.convert\u001B[0;34m(self, model)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvert\u001B[39m(\u001B[38;5;28mself\u001B[39m, model: Union[Model, InputModel]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Model:\n\u001B[0;32m---> 18\u001B[0m     converted_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, InputModel):\n\u001B[1;32m     20\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m Model(converted_model)\n",
      "\u001B[0;31mOpConversionFailure\u001B[0m: Check 'is_conversion_successful' failed at src/frontends/pytorch/src/frontend.cpp:138:\nFrontEnd API failed with OpConversionFailure:\nModel wasn't fully converted. Failed operations detailed log:\n-- prim::Constant with a message:\nString constant cannot be converted to OpenVINO opset and should be removed by consuming operation.\n-- prim::ListConstruct with a message:\nException happened during conversion of operation __module.model/prim::ListConstruct_1422 with schema (no schema)\nCheck 'c_node' failed at src/frontends/pytorch/src/op/list_construct.cpp:25:\nFrontEnd API failed with OpConversionFailure:\nTranslation for prim::ListConstruct support only constant inputs\n\n-- prim::ListUnpack with a message:\nprim::ListUnpack: unsupported input node: util::PtFrameworkNode __module.model/aten::broadcast_tensors_1424 (__module.model/prim::ListConstruct_1423[0]:dynamic[...]) -> (dynamic[...])\nSummary:\n-- No conversion rule found for operations: aten::broadcast_tensors, aten::logical_and, aten::scatter_reduce\n-- Conversion is failed for: prim::Constant, prim::ListConstruct, prim::ListUnpack\n"
     ]
    }
   ],
   "source": [
    "import openvino\n",
    "\n",
    "openvino.convert_model(\n",
    "    model,\n",
    "    example_input=dummy_input\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T13:27:51.873500100Z",
     "start_time": "2023-08-18T13:27:46.786216Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
