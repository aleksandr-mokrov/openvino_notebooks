{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8722e6-2550-4e4b-a8eb-60b10c21b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q diffusers accelerate datasets gradio transformers \"nncf==2.10.0\" \"openvino>=2024.1.0\" \"torch>=2.1\" --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "017afb72-5bfa-441d-bda9-d6fbe6e15409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-03 23:06:03.939583: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-03 23:06:03.940048: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-03 23:06:03.942559: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-03 23:06:03.975149: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-03 23:06:04.680757: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346b627610444a128d70e94d03571031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed430c6149224ba8a5dc49eeed984327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableCascadeDecoderPipeline, StableCascadePriorPipeline, StableCascadeUNet\n",
    "\n",
    "prompt = \"an image of a shiba inu, donning a spacesuit and helmet\"\n",
    "negative_prompt = \"\"\n",
    "\n",
    "prior_unet = StableCascadeUNet.from_pretrained(\"stabilityai/stable-cascade-prior\", subfolder=\"prior_lite\")\n",
    "decoder_unet = StableCascadeUNet.from_pretrained(\"stabilityai/stable-cascade\", subfolder=\"decoder_lite\")\n",
    "\n",
    "prior = StableCascadePriorPipeline.from_pretrained(\"stabilityai/stable-cascade-prior\", prior=prior_unet)\n",
    "decoder = StableCascadeDecoderPipeline.from_pretrained(\"stabilityai/stable-cascade\", decoder=decoder_unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d721904d-f7e1-48db-8431-10c37d440c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "import openvino as ov\n",
    "import nncf\n",
    "\n",
    "\n",
    "MODELS_DIR = Path(\"models\")\n",
    "\n",
    "def convert(model: torch.nn.Module, xml_path: str, example_input, input_shape=None):\n",
    "    xml_path = Path(xml_path)\n",
    "    if not xml_path.exists():\n",
    "        model.eval()\n",
    "        xml_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with torch.no_grad():\n",
    "            if not input_shape:\n",
    "                converted_model = ov.convert_model(model, example_input=example_input)\n",
    "            else:\n",
    "                converted_model = ov.convert_model(model, example_input=example_input, input=input_shape)\n",
    "        #converted_model = nncf.compress_weights(converted_model)\n",
    "        ov.save_model(converted_model, xml_path)\n",
    "        del converted_model\n",
    "        \n",
    "        # cleanup memory\n",
    "        torch._C._jit_clear_class_registry()\n",
    "        torch.jit._recursive.concrete_type_store = torch.jit._recursive.ConcreteTypeStore()\n",
    "        torch.jit._state._clear_class_state()\n",
    "\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa3cbc26-53ae-42f3-aec5-b06290ed703e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maleksandr/test_notebooks/update_ultralytics/openvino_notebooks/notebooks/yolov8-optimization/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_stable_cascade.py:549: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if skip is not None and (x.size(-1) != skip.size(-1) or x.size(-2) != skip.size(-2)):\n"
     ]
    }
   ],
   "source": [
    "PRIOR_PRIOR_MODEL_OV_PATH = MODELS_DIR / \"prior_prior_model.xml\"\n",
    "\n",
    "convert(\n",
    "    prior_unet,\n",
    "    PRIOR_PRIOR_MODEL_OV_PATH,\n",
    "    example_input={\n",
    "        \"sample\": torch.zeros(2, 16, 24, 24),\n",
    "        \"timestep_ratio\": torch.ones(2),\n",
    "        \"clip_text_pooled\": torch.zeros(2, 1, 1280),\n",
    "        \"clip_text\": torch.zeros(2, 77, 1280),\n",
    "        \"clip_img\": torch.zeros(2, 1, 768),\n",
    "    },\n",
    "    input_shape=[((2, 16, 24, 24),), ((2),), ((2, 1, 1280),), ((2, 77, 1280),), (2, 1, 768)],\n",
    ")\n",
    "del prior.prior\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bde0a2b-c55b-4cbe-9bf5-2c8c188a6695",
   "metadata": {},
   "outputs": [],
   "source": [
    "DECODER_DECODER_MODEL_OV_PATH = MODELS_DIR / \"decoder_decoder_model.xml\"\n",
    "\n",
    "convert(\n",
    "    decoder.decoder,\n",
    "    DECODER_DECODER_MODEL_OV_PATH,\n",
    "    example_input={\n",
    "        \"sample\": torch.zeros(1, 4, 256, 256),\n",
    "        \"timestep_ratio\": torch.ones(1),\n",
    "        \"clip_text_pooled\": torch.zeros(1, 1, 1280),\n",
    "        \"effnet\": torch.zeros(1, 16, 24, 24)\n",
    "    },\n",
    "    input_shape=[((1, 4, 256, 256),), ((1),), ((1, 1, 1280),), ((1, 16, 24, 24),)],\n",
    ")\n",
    "del decoder.decoder\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2ab61de-1926-4f40-a994-90d98386d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "core = ov.Core()\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "class PriorPriorWrapper:\n",
    "    def __init__(self, prior_path):\n",
    "        self.prior = core.compile_model(prior_path, \"CPU\")\n",
    "        self.config = namedtuple(\"PriorWrapperConfig\", [\"clip_image_in_channels\", \"in_channels\"])(768, 16)  # accessed in the original workflow\n",
    "        self.parameters = lambda: (torch.zeros(i, dtype=torch.float32) for i in range(1)) # accessed in the original workflow\n",
    "\n",
    "    def __call__(self, sample, timestep_ratio, clip_text_pooled, clip_text=None, clip_img=None, **kwargs):\n",
    "        inputs = {\n",
    "            \"sample\": sample,\n",
    "            \"timestep_ratio\": timestep_ratio,\n",
    "            \"clip_text_pooled\": clip_text_pooled,\n",
    "            \"clip_text\": clip_text,\n",
    "            \"clip_img\": clip_img,\n",
    "        }\n",
    "        output = self.prior(inputs)\n",
    "        return [torch.from_numpy(output[0])]\n",
    "\n",
    "\n",
    "prior.prior = PriorPriorWrapper(PRIOR_PRIOR_MODEL_OV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7fddaba-200d-4dad-8393-110555781ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWrapper:\n",
    "    dtype = torch.float32  # accessed in the original workflow\n",
    "    \n",
    "    def __init__(self, decoder_path):\n",
    "        self.decoder = core.compile_model(decoder_path, \"CPU\")\n",
    "\n",
    "    def __call__(self, sample, timestep_ratio, clip_text_pooled, effnet, **kwargs):\n",
    "        inputs = {\n",
    "            \"sample\": sample,\n",
    "            \"timestep_ratio\": timestep_ratio,\n",
    "            \"clip_text_pooled\": clip_text_pooled,\n",
    "            \"effnet\": effnet\n",
    "        }\n",
    "        output = self.decoder(inputs)\n",
    "        return [torch.from_numpy(output[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3625280-6c23-4619-a78f-d2a68800e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior.prior = PriorPriorWrapper(PRIOR_PRIOR_MODEL_OV_PATH)\n",
    "decoder.decoder = DecoderWrapper(DECODER_DECODER_MODEL_OV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fe79b4d-8161-466b-9629-23184408c2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompiledModelDecorator(ov.CompiledModel):\n",
    "    def __init__(self, compiled_model):\n",
    "        super().__init__(compiled_model)\n",
    "        self.data_cache = []\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        self.data_cache.append(*args)\n",
    "        return super().__call__(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c2e1423-fd98-4d2b-ae55-9c6720541c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datasets\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import set_seed\n",
    "\n",
    "set_seed(1)\n",
    "\n",
    "def collect_calibration_data(prior, decoder, subset_size):\n",
    "    prior_calibration_dataset_filepath = Path(f\"prior_calibration_dataset/{subset_size}.pkl\")\n",
    "    decoder_calibration_dataset_filepath = Path(f\"decoder_calibration_dataset/{subset_size}.pkl\")\n",
    "    if not prior_calibration_dataset_filepath.exists():\n",
    "        original_prior = prior.prior.prior\n",
    "        original_decoder = decoder.decoder.decoder\n",
    "        prior.prior.prior = CompiledModelDecorator(original_prior)\n",
    "        decoder.decoder.decoder = CompiledModelDecorator(original_decoder)\n",
    "    \n",
    "        dataset = datasets.load_dataset(\"conceptual_captions\", split=\"train\").shuffle(seed=42)\n",
    "        pbar = tqdm(total=subset_size)\n",
    "        diff = 0\n",
    "        for batch in dataset:\n",
    "            prompt = batch[\"caption\"]\n",
    "            if len(prompt) > prior.tokenizer.model_max_length:\n",
    "                continue\n",
    "            prior_output = prior(\n",
    "                prompt=prompt,\n",
    "                height=1024,\n",
    "                width=1024,\n",
    "                negative_prompt=negative_prompt,\n",
    "                guidance_scale=4.0,\n",
    "                num_images_per_prompt=1,\n",
    "                num_inference_steps=20\n",
    "            )\n",
    "            \n",
    "            _ = decoder(\n",
    "                image_embeddings=prior_output.image_embeddings,\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                guidance_scale=0.0,\n",
    "                output_type=\"pil\",\n",
    "                num_inference_steps=20\n",
    "            )\n",
    "            collected_subset_size = len(prior.prior.prior.data_cache)\n",
    "            if collected_subset_size >= subset_size:\n",
    "                pbar.update(subset_size - pbar.n)\n",
    "                break\n",
    "            pbar.update(collected_subset_size - diff)\n",
    "            diff = collected_subset_size \n",
    "    \n",
    "        prior_calibration_dataset = prior.prior.prior.data_cache\n",
    "        decoder_calibration_dataset = decoder.decoder.decoder.data_cache\n",
    "        prior.prior.prior = original_prior\n",
    "        decoder.decoder.decoder = original_decoder\n",
    "        prior_calibration_dataset_filepath.parent.mkdir(exist_ok=True, parents=True)\n",
    "        with open(prior_calibration_dataset_filepath, 'wb') as f:\n",
    "            pickle.dump(prior_calibration_dataset, f)\n",
    "        decoder_calibration_dataset_filepath.parent.mkdir(exist_ok=True, parents=True)\n",
    "        with open(decoder_calibration_dataset_filepath, 'wb') as f:\n",
    "            pickle.dump(decoder_calibration_dataset, f)\n",
    "    else:   \n",
    "        with open(prior_calibration_dataset_filepath, 'rb') as f:\n",
    "            prior_calibration_dataset = pickle.load(f)\n",
    "        with open(decoder_calibration_dataset_filepath, 'rb') as f:\n",
    "            decoder_calibration_dataset = pickle.load(f)\n",
    "    \n",
    "    return prior_calibration_dataset, decoder_calibration_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28904b38-a2e4-4a7a-a5c7-131cda0a8423",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIOR_PRIOR_INT8_PATH = MODELS_DIR / \"prior_prior_int8.xml\"\n",
    "DECODER_INT8_PATH = MODELS_DIR / \"decoder_int8.xml\"\n",
    "\n",
    "if not (PRIOR_PRIOR_INT8_PATH.exists() and DECODER_INT8_PATH.exists()):\n",
    "    subset_size = 40\n",
    "    prior_calibration_dataset, decoder_calibration_dataset = collect_calibration_data(prior, decoder, subset_size=subset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "313c1123-144e-4cf9-bf26-8c2361d1e6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e467fbcd0ee94458af7bebfdaae3fe12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e227a07bb44cc4aebc7b09e8e02a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m PRIOR_PRIOR_INT8_PATH\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m      6\u001b[0m     prior_model \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mread_model(PRIOR_PRIOR_MODEL_OV_PATH)\n\u001b[0;32m----> 7\u001b[0m     quantized_prior_prior \u001b[38;5;241m=\u001b[39m \u001b[43mnncf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprior_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcalibration_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnncf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprior_calibration_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnncf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQuantizationPreset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPERFORMANCE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnncf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModelType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRANSFORMER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alpha=-1)\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alphas=AdvancedSmoothQuantParameters(matmul=-1))\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     ov\u001b[38;5;241m.\u001b[39msave_model(quantized_prior_prior, PRIOR_PRIOR_INT8_PATH)\n",
      "File \u001b[0;32m~/test_notebooks/update_ultralytics/openvino_notebooks/notebooks/yolov8-optimization/venv/lib/python3.10/site-packages/nncf/quantization/quantize_model.py:151\u001b[0m, in \u001b[0;36mquantize\u001b[0;34m(model, calibration_dataset, mode, preset, target_device, subset_size, fast_bias_correction, model_type, ignored_scope, advanced_parameters)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m BackendType\u001b[38;5;241m.\u001b[39mOPENVINO:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnncf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenvino\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantize_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quantize_impl\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquantize_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcalibration_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcalibration_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfast_bias_correction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfast_bias_correction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignored_scope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignored_scope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43madvanced_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madvanced_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m BackendType\u001b[38;5;241m.\u001b[39mONNX:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnncf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantize_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quantize_impl\n",
      "File \u001b[0;32m~/test_notebooks/update_ultralytics/openvino_notebooks/notebooks/yolov8-optimization/venv/lib/python3.10/site-packages/nncf/openvino/quantization/quantize_model.py:326\u001b[0m, in \u001b[0;36mquantize_impl\u001b[0;34m(model, calibration_dataset, mode, preset, target_device, subset_size, fast_bias_correction, model_type, ignored_scope, advanced_parameters)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_number_if_op(model) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    324\u001b[0m     quantize_fn \u001b[38;5;241m=\u001b[39m native_quantize_if_op_impl\n\u001b[0;32m--> 326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquantize_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcalibration_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcalibration_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubset_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfast_bias_correction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfast_bias_correction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignored_scope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignored_scope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43madvanced_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madvanced_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/test_notebooks/update_ultralytics/openvino_notebooks/notebooks/yolov8-optimization/venv/lib/python3.10/site-packages/nncf/telemetry/decorator.py:72\u001b[0m, in \u001b[0;36mtracked_function.__call__.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m events:\n\u001b[1;32m     65\u001b[0m         telemetry\u001b[38;5;241m.\u001b[39msend_event(\n\u001b[1;32m     66\u001b[0m             event_category\u001b[38;5;241m=\u001b[39mcategory,\n\u001b[1;32m     67\u001b[0m             event_action\u001b[38;5;241m=\u001b[39mevent\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m     68\u001b[0m             event_label\u001b[38;5;241m=\u001b[39mevent\u001b[38;5;241m.\u001b[39mdata,\n\u001b[1;32m     69\u001b[0m             event_value\u001b[38;5;241m=\u001b[39mevent\u001b[38;5;241m.\u001b[39mint_data,\n\u001b[1;32m     70\u001b[0m         )\n\u001b[0;32m---> 72\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m category \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m category \u001b[38;5;241m!=\u001b[39m previous_category:\n\u001b[1;32m     75\u001b[0m     telemetry\u001b[38;5;241m.\u001b[39mend_session(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_category)\n",
      "File \u001b[0;32m~/test_notebooks/update_ultralytics/openvino_notebooks/notebooks/yolov8-optimization/venv/lib/python3.10/site-packages/nncf/openvino/quantization/quantize_model.py:143\u001b[0m, in \u001b[0;36mnative_quantize_impl\u001b[0;34m(model, calibration_dataset, mode, preset, target_device, subset_size, fast_bias_correction, model_type, ignored_scope, advanced_parameters)\u001b[0m\n\u001b[1;32m    141\u001b[0m graph \u001b[38;5;241m=\u001b[39m GraphConverter\u001b[38;5;241m.\u001b[39mcreate_nncf_graph(model)\n\u001b[1;32m    142\u001b[0m warning_model_no_batchwise_support(graph, advanced_parameters, model_type, OPERATIONS_OUTPUT_HAS_NO_BATCH_AXIS)\n\u001b[0;32m--> 143\u001b[0m quantized_model \u001b[38;5;241m=\u001b[39m \u001b[43mquantization_algorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcalibration_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_weight_compression_needed(advanced_parameters):\n\u001b[1;32m    146\u001b[0m     compress_quantize_weights_transformation(quantized_model)\n",
      "File \u001b[0;32m~/test_notebooks/update_ultralytics/openvino_notebooks/notebooks/yolov8-optimization/venv/lib/python3.10/site-packages/nncf/quantization/algorithms/post_training/algorithm.py:112\u001b[0m, in \u001b[0;36mPostTrainingQuantization.apply\u001b[0;34m(self, model, graph, statistic_points, dataset)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m statistic_points:\n\u001b[1;32m    110\u001b[0m     step_index_to_statistics \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: statistic_points}\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_from_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_index_to_statistics\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/test_notebooks/update_ultralytics/openvino_notebooks/notebooks/yolov8-optimization/venv/lib/python3.10/site-packages/nncf/quantization/algorithms/pipeline.py:164\u001b[0m, in \u001b[0;36mPipeline.run_from_step\u001b[0;34m(self, model, dataset, graph, start_step_index, step_index_to_statistics)\u001b[0m\n\u001b[1;32m    161\u001b[0m         step_statistics \u001b[38;5;241m=\u001b[39m collect_statistics(statistic_points, step_model, step_graph, dataset)\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Run current pipeline step\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     step_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_statistics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     step_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# We should rebuild the graph for the next pipeline step\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_model\n",
      "File \u001b[0;32m~/test_notebooks/update_ultralytics/openvino_notebooks/notebooks/yolov8-optimization/venv/lib/python3.10/site-packages/nncf/quantization/algorithms/pipeline.py:119\u001b[0m, in \u001b[0;36mPipeline.run_step\u001b[0;34m(self, step_index, step_statistics, model, graph)\u001b[0m\n\u001b[1;32m    117\u001b[0m     current_model \u001b[38;5;241m=\u001b[39m algorithm\u001b[38;5;241m.\u001b[39mapply(current_model, current_graph, step_statistics)\n\u001b[1;32m    118\u001b[0m     current_graph \u001b[38;5;241m=\u001b[39m NNCFGraphFactory\u001b[38;5;241m.\u001b[39mcreate(current_model)\n\u001b[0;32m--> 119\u001b[0m current_model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_step\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_statistics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m current_model\n",
      "File \u001b[0;32m~/test_notebooks/update_ultralytics/openvino_notebooks/notebooks/yolov8-optimization/venv/lib/python3.10/site-packages/nncf/quantization/algorithms/smooth_quant/algorithm.py:164\u001b[0m, in \u001b[0;36mSmoothQuant.apply\u001b[0;34m(self, model, graph, statistic_points, dataset)\u001b[0m\n\u001b[1;32m    161\u001b[0m     weight_update_command \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend_entity\u001b[38;5;241m.\u001b[39mweight_update_command(node_to_smooth, scaled_weight\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m    162\u001b[0m     transformation_layout\u001b[38;5;241m.\u001b[39mregister(weight_update_command)\n\u001b[0;32m--> 164\u001b[0m activations_shape \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_output_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_node\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43msource_output_port_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtensor_shape\n\u001b[1;32m    165\u001b[0m activation_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_activation_scale(best_scale, activations_shape, nodes, graph)\n\u001b[1;32m    167\u001b[0m scale_node_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_scale_node_name(source_node\u001b[38;5;241m.\u001b[39mnode_name, source_output_port_id)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import nncf\n",
    "from nncf.scopes import IgnoredScope\n",
    "from nncf.quantization.advanced_parameters import AdvancedSmoothQuantParameters\n",
    "\n",
    "if not PRIOR_PRIOR_INT8_PATH.exists():\n",
    "    prior_model = core.read_model(PRIOR_PRIOR_MODEL_OV_PATH)\n",
    "    quantized_prior_prior = nncf.quantize(\n",
    "        model=prior_model,\n",
    "        subset_size=subset_size,\n",
    "        calibration_dataset=nncf.Dataset(prior_calibration_dataset),\n",
    "        preset=nncf.QuantizationPreset.PERFORMANCE,\n",
    "        model_type=nncf.ModelType.TRANSFORMER,\n",
    "        #advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alpha=-1)\n",
    "        #advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alphas=AdvancedSmoothQuantParameters(matmul=-1))\n",
    "    )\n",
    "    ov.save_model(quantized_prior_prior, PRIOR_PRIOR_INT8_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c4496a4-741c-47d3-9fae-45404879eda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2efc6a89eee04c88a7eedc40562c86df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4df4cbb48ce4c1e971a81fc0d2aec18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m DECODER_INT8_PATH\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m      2\u001b[0m     decoder_model \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mread_model(DECODER_DECODER_MODEL_OV_PATH)\n\u001b[0;32m----> 3\u001b[0m     quantized_decoder \u001b[38;5;241m=\u001b[39m \u001b[43mnncf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcalibration_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnncf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_calibration_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdecoder_calibration_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnncf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModelType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRANSFORMER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alpha=-1)\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# advanced_parameters=nncf.AdvancedQuantizationParameters(\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#     disable_bias_correction=True\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# )\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alphas=AdvancedSmoothQuantParameters(matmul=-1))\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     ov\u001b[38;5;241m.\u001b[39msave_model(quantized_decoder, DECODER_INT8_PATH)\n",
      "File \u001b[0;32m~/test_notebooks/update_ultralytics/openvino_notebooks/notebooks/yolov8-optimization/venv/lib/python3.10/site-packages/nncf/quantization/quantize_model.py:151\u001b[0m, in \u001b[0;36mquantize\u001b[0;34m(model, calibration_dataset, mode, preset, target_device, subset_size, fast_bias_correction, model_type, ignored_scope, advanced_parameters)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m BackendType\u001b[38;5;241m.\u001b[39mOPENVINO:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnncf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenvino\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantize_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quantize_impl\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquantize_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcalibration_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcalibration_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubset_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfast_bias_correction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfast_bias_correction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignored_scope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignored_scope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43madvanced_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madvanced_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m BackendType\u001b[38;5;241m.\u001b[39mONNX:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnncf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantize_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quantize_impl\n",
      "File \u001b[0;32m~/test_notebooks/update_ultralytics/openvino_notebooks/notebooks/yolov8-optimization/venv/lib/python3.10/site-packages/nncf/openvino/quantization/quantize_model.py:326\u001b[0m, in \u001b[0;36mquantize_impl\u001b[0;34m(model, calibration_dataset, mode, preset, target_device, subset_size, fast_bias_correction, model_type, ignored_scope, advanced_parameters)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_number_if_op(model) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    324\u001b[0m     quantize_fn \u001b[38;5;241m=\u001b[39m native_quantize_if_op_impl\n\u001b[0;32m--> 326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquantize_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcalibration_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcalibration_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubset_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfast_bias_correction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfast_bias_correction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignored_scope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignored_scope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43madvanced_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madvanced_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/test_notebooks/update_ultralytics/openvino_notebooks/notebooks/yolov8-optimization/venv/lib/python3.10/site-packages/nncf/telemetry/decorator.py:72\u001b[0m, in \u001b[0;36mtracked_function.__call__.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m events:\n\u001b[1;32m     65\u001b[0m         telemetry\u001b[38;5;241m.\u001b[39msend_event(\n\u001b[1;32m     66\u001b[0m             event_category\u001b[38;5;241m=\u001b[39mcategory,\n\u001b[1;32m     67\u001b[0m             event_action\u001b[38;5;241m=\u001b[39mevent\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m     68\u001b[0m             event_label\u001b[38;5;241m=\u001b[39mevent\u001b[38;5;241m.\u001b[39mdata,\n\u001b[1;32m     69\u001b[0m             event_value\u001b[38;5;241m=\u001b[39mevent\u001b[38;5;241m.\u001b[39mint_data,\n\u001b[1;32m     70\u001b[0m         )\n\u001b[0;32m---> 72\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m category \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m category \u001b[38;5;241m!=\u001b[39m previous_category:\n\u001b[1;32m     75\u001b[0m     telemetry\u001b[38;5;241m.\u001b[39mend_session(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_category)\n",
      "File \u001b[0;32m~/test_notebooks/update_ultralytics/openvino_notebooks/notebooks/yolov8-optimization/venv/lib/python3.10/site-packages/nncf/openvino/quantization/quantize_model.py:143\u001b[0m, in \u001b[0;36mnative_quantize_impl\u001b[0;34m(model, calibration_dataset, mode, preset, target_device, subset_size, fast_bias_correction, model_type, ignored_scope, advanced_parameters)\u001b[0m\n\u001b[1;32m    141\u001b[0m graph \u001b[38;5;241m=\u001b[39m GraphConverter\u001b[38;5;241m.\u001b[39mcreate_nncf_graph(model)\n\u001b[1;32m    142\u001b[0m warning_model_no_batchwise_support(graph, advanced_parameters, model_type, OPERATIONS_OUTPUT_HAS_NO_BATCH_AXIS)\n\u001b[0;32m--> 143\u001b[0m quantized_model \u001b[38;5;241m=\u001b[39m \u001b[43mquantization_algorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcalibration_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_weight_compression_needed(advanced_parameters):\n\u001b[1;32m    146\u001b[0m     compress_quantize_weights_transformation(quantized_model)\n",
      "File \u001b[0;32m~/test_notebooks/update_ultralytics/openvino_notebooks/notebooks/yolov8-optimization/venv/lib/python3.10/site-packages/nncf/quantization/algorithms/post_training/algorithm.py:112\u001b[0m, in \u001b[0;36mPostTrainingQuantization.apply\u001b[0;34m(self, model, graph, statistic_points, dataset)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m statistic_points:\n\u001b[1;32m    110\u001b[0m     step_index_to_statistics \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: statistic_points}\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_from_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_index_to_statistics\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/test_notebooks/update_ultralytics/openvino_notebooks/notebooks/yolov8-optimization/venv/lib/python3.10/site-packages/nncf/quantization/algorithms/pipeline.py:164\u001b[0m, in \u001b[0;36mPipeline.run_from_step\u001b[0;34m(self, model, dataset, graph, start_step_index, step_index_to_statistics)\u001b[0m\n\u001b[1;32m    161\u001b[0m         step_statistics \u001b[38;5;241m=\u001b[39m collect_statistics(statistic_points, step_model, step_graph, dataset)\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Run current pipeline step\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     step_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_statistics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     step_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# We should rebuild the graph for the next pipeline step\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_model\n",
      "File \u001b[0;32m~/test_notebooks/update_ultralytics/openvino_notebooks/notebooks/yolov8-optimization/venv/lib/python3.10/site-packages/nncf/quantization/algorithms/pipeline.py:119\u001b[0m, in \u001b[0;36mPipeline.run_step\u001b[0;34m(self, step_index, step_statistics, model, graph)\u001b[0m\n\u001b[1;32m    117\u001b[0m     current_model \u001b[38;5;241m=\u001b[39m algorithm\u001b[38;5;241m.\u001b[39mapply(current_model, current_graph, step_statistics)\n\u001b[1;32m    118\u001b[0m     current_graph \u001b[38;5;241m=\u001b[39m NNCFGraphFactory\u001b[38;5;241m.\u001b[39mcreate(current_model)\n\u001b[0;32m--> 119\u001b[0m current_model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_step\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_statistics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m current_model\n",
      "File \u001b[0;32m~/test_notebooks/update_ultralytics/openvino_notebooks/notebooks/yolov8-optimization/venv/lib/python3.10/site-packages/nncf/quantization/algorithms/smooth_quant/algorithm.py:164\u001b[0m, in \u001b[0;36mSmoothQuant.apply\u001b[0;34m(self, model, graph, statistic_points, dataset)\u001b[0m\n\u001b[1;32m    161\u001b[0m     weight_update_command \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend_entity\u001b[38;5;241m.\u001b[39mweight_update_command(node_to_smooth, scaled_weight\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m    162\u001b[0m     transformation_layout\u001b[38;5;241m.\u001b[39mregister(weight_update_command)\n\u001b[0;32m--> 164\u001b[0m activations_shape \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_output_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_node\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43msource_output_port_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtensor_shape\n\u001b[1;32m    165\u001b[0m activation_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_activation_scale(best_scale, activations_shape, nodes, graph)\n\u001b[1;32m    167\u001b[0m scale_node_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_scale_node_name(source_node\u001b[38;5;241m.\u001b[39mnode_name, source_output_port_id)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "if not DECODER_INT8_PATH.exists():\n",
    "    decoder_model = core.read_model(DECODER_DECODER_MODEL_OV_PATH)\n",
    "    quantized_decoder = nncf.quantize(\n",
    "        model=decoder_model,\n",
    "        calibration_dataset=nncf.Dataset(decoder_calibration_dataset),\n",
    "        subset_size=len(decoder_calibration_dataset),\n",
    "         model_type=nncf.ModelType.TRANSFORMER,\n",
    "        #advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alpha=-1)\n",
    "        # advanced_parameters=nncf.AdvancedQuantizationParameters(\n",
    "        #     disable_bias_correction=True\n",
    "        # )\n",
    "        #advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alphas=AdvancedSmoothQuantParameters(matmul=-1))\n",
    "    )\n",
    "    ov.save_model(quantized_decoder, DECODER_INT8_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
