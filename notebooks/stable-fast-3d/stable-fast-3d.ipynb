{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fc6b72c-989c-4dbe-b12c-468190868959",
   "metadata": {},
   "source": [
    "# Stable Fast 3D Mesh Reconstruction and OpenVINO\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> <b>Important note:</b> This notebook requires python >= 3.9. Please make sure that your environment fulfill to this requirement before running it </div>\n",
    "\n",
    "[Stable Fast 3D (SF3D)](https://huggingface.co/stabilityai/stable-fast-3d) is a large reconstruction model based on [TripoSR](https://huggingface.co/spaces/stabilityai/TripoSR), which takes in a single image of an object and generates a textured UV-unwrapped 3D mesh asset.\n",
    "\n",
    "You can find [the source code on GitHub](https://github.com/Stability-AI/stable-fast-3d) and read the paper [SF3D: Stable Fast 3D Mesh Reconstruction with UV-unwrapping and Illumination Disentanglement](https://arxiv.org/abs/2408.00653).\n",
    "\n",
    "![Teaser Video](https://github.com/Stability-AI/stable-fast-3d/blob/main/demo_files/teaser.gif?raw=true)\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Get the original model](#Get-the-original-model)\n",
    "- [Convert the model to OpenVINO IR](#Convert-the-model-to-OpenVINO-IR)\n",
    "- [Compiling models and prepare pipeline](#Compiling-models-and-prepare-pipeline)\n",
    "- [Interactive inference](#Interactive-inference)\n",
    "\n",
    "### Installation Instructions\n",
    "\n",
    "This is a self-contained example that relies solely on its own code.\n",
    "\n",
    "We recommend  running the notebook in a virtual environment. You only need a Jupyter server to start.\n",
    "For details, please refer to [Installation Guide](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/README.md#-installation-guide).\n",
    "\n",
    "<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=5b5a4db0-7875-4bfb-bdbd-01698b5b1a77&file=notebooks/stable-fast-3d/stable-fast-3d.ipynb\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991ab76-9c05-45a8-a901-24692730eb01",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05865a94-9556-4e38-b98f-d42cf8822430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    ")\n",
    "open(\"notebook_utils.py\", \"w\").write(r.text)\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/pip_helper.py\",\n",
    ")\n",
    "open(\"pip_helper.py\", \"w\").write(r.text)\n",
    "\n",
    "from pip_helper import pip_install\n",
    "\n",
    "\n",
    "pip_install(\"-q\", \"gradio>=4.19\", \"openvino>=2024.3.0\", \"wheel\", \"gradio-litmodel3d==0.0.1\")\n",
    "\n",
    "pip_install(\n",
    "    \"-q\",\n",
    "    \"torch>=2.2.2\",\n",
    "    \"torchvision\",\n",
    "    \"transformers>=4.42.3\",\n",
    "    \"rembg==2.0.57\",\n",
    "    \"trimesh==4.4.1\",\n",
    "    \"einops==0.7.0\",\n",
    "    \"omegaconf>=2.3.0\",\n",
    "    \"jaxtyping==0.2.31\",\n",
    "    \"gpytoolbox==0.2.0\",\n",
    "    \"open_clip_torch==2.24.0\",\n",
    "    \"git+https://github.com/vork/PyNanoInstantMeshes.git\",\n",
    "    \"--extra-index-url\",\n",
    "    \"https://download.pytorch.org/whl/cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb66f24b-fc3a-4852-8f55-10b316e343fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path(\"stable-fast-3d\").exists():\n",
    "    !git clone https://github.com/Stability-AI/stable-fast-3d\n",
    "\n",
    "sys.path.append(\"stable-fast-3d\")\n",
    "pip_install(\"-q\", \"stable-fast-3d/texture_baker/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde31441-436f-4317-b655-781bb958f753",
   "metadata": {},
   "source": [
    "## Get the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cd23dd-ce40-4b52-a1cb-c5484aba5621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sf3d.system import SF3D\n",
    "\n",
    "\n",
    "model = SF3D.from_pretrained(\n",
    "    \"stabilityai/stable-fast-3d\",\n",
    "    config_name=\"config.yaml\",\n",
    "    weight_name=\"model.safetensors\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ab6c02-6a53-41a3-9247-f16ab77934f5",
   "metadata": {},
   "source": [
    "### Convert the model to OpenVINO IR\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891b4031-180c-4519-9e4c-b0beb68fdea4",
   "metadata": {},
   "source": [
    "Define the conversion function for PyTorch modules. We use `ov.convert_model` function to obtain OpenVINO Intermediate Representation object and `ov.save_model` function to save it as XML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef6cd5f-3db8-4729-b35a-628ecde1808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import openvino as ov\n",
    "\n",
    "\n",
    "def convert(model: torch.nn.Module, xml_path: str, example_input):\n",
    "    xml_path = Path(xml_path)\n",
    "    if not xml_path.exists():\n",
    "        xml_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with torch.no_grad():\n",
    "            converted_model = ov.convert_model(model, example_input=example_input)\n",
    "        ov.save_model(converted_model, xml_path, compress_to_fp16=False)\n",
    "\n",
    "        # cleanup memory\n",
    "        torch._C._jit_clear_class_registry()\n",
    "        torch.jit._recursive.concrete_type_store = torch.jit._recursive.ConcreteTypeStore()\n",
    "        torch.jit._state._clear_class_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b00b578-5bac-4ad6-b73b-d3148eb591d1",
   "metadata": {},
   "source": [
    "The original model is a pipeline of several models. There are `image_tokenizer`, `tokenizer`, `backbone`, `post_processor`, `camera_embedder`, `decoder`, `image_estimator` and `global_estimator`. Convert all internal models one by one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4725cad5-99e0-40c3-94e7-f8b0201e3043",
   "metadata": {},
   "source": [
    "`image_tokenizer` contains `Dinov2Embeddings` that call `nn.functional.interpolate` in its method `interpolate_pos_encoding`. This method accepts a tuple of floats as `scale_factor`, but during conversion a tuple of floats converts to a tuple of tensors due to conversion specific. It raises an error. So, we need to patch it by converting in float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70574d93-739a-4238-bf6e-dac4603213a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import types\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from sf3d.models.tokenizers.dinov2 import Dinov2Embeddings\n",
    "\n",
    "\n",
    "class Dinov2EmbeddingsPatched(Dinov2Embeddings):\n",
    "    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n",
    "        resolution images.\n",
    "\n",
    "        Source:\n",
    "        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n",
    "        \"\"\"\n",
    "\n",
    "        num_patches = embeddings.shape[1] - 1\n",
    "        num_positions = self.position_embeddings.shape[1] - 1\n",
    "        if num_patches == num_positions and height == width:\n",
    "            return self.position_embeddings\n",
    "        class_pos_embed = self.position_embeddings[:, 0]\n",
    "        patch_pos_embed = self.position_embeddings[:, 1:]\n",
    "        dim = embeddings.shape[-1]\n",
    "        height = height // self.config.patch_size\n",
    "        width = width // self.config.patch_size\n",
    "        # we add a small number to avoid floating point error in the interpolation\n",
    "        # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
    "        height, width = height + 0.1, width + 0.1\n",
    "        patch_pos_embed = patch_pos_embed.reshape(1, int(math.sqrt(num_positions)), int(math.sqrt(num_positions)), dim)\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n",
    "\n",
    "        scale_factor = (\n",
    "            (\n",
    "                height / math.sqrt(num_positions),\n",
    "                width / math.sqrt(num_positions),\n",
    "            ),\n",
    "        )\n",
    "        patch_pos_embed = nn.functional.interpolate(\n",
    "            patch_pos_embed,\n",
    "            scale_factor=(\n",
    "                float(height / math.sqrt(num_positions)),\n",
    "                float(width / math.sqrt(num_positions)),\n",
    "            ),\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        if int(height) != patch_pos_embed.shape[-2] or int(width) != patch_pos_embed.shape[-1]:\n",
    "            raise ValueError(\"Width or height does not match with the interpolated position embeddings\")\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n",
    "\n",
    "\n",
    "model.image_tokenizer.model.embeddings.interpolate_pos_encoding = types.MethodType(\n",
    "    Dinov2EmbeddingsPatched.interpolate_pos_encoding, model.image_tokenizer.model.embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47aacc7-3a13-462c-bd50-2188e422ae93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_input = {\n",
    "    \"images\": torch.rand([1, 1, 3, 512, 512], dtype=torch.float32),\n",
    "    \"modulation_cond\": torch.rand([1, 1, 768], dtype=torch.float32),\n",
    "}\n",
    "\n",
    "IMAGE_TOKENIZER_OV_PATH = Path(\"models/image_tokenizer_ir.xml\")\n",
    "convert(model.image_tokenizer, IMAGE_TOKENIZER_OV_PATH, example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098f6de6-d81c-4b31-b343-2e0e92792057",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_OV_PATH = Path(\"models/tokenizer_ir.xml\")\n",
    "convert(model.tokenizer, TOKENIZER_OV_PATH, torch.tensor(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277018bd-dc70-459b-a4a8-9474c3959c5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_input = {\n",
    "    \"hidden_states\": torch.rand([1, 1024, 27648], dtype=torch.float32),\n",
    "    \"encoder_hidden_states\": torch.rand([1, 1297, 1024], dtype=torch.float32),\n",
    "}\n",
    "\n",
    "BACKBONE_OV_PATH = Path(\"models/backbone_ir.xml\")\n",
    "convert(model.backbone, BACKBONE_OV_PATH, example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9100f1f8-502d-4400-b375-522db67c79f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "POST_PROCESSOR_OV_PATH = Path(\"models/post_processor_ir.xml\")\n",
    "convert(\n",
    "    model.post_processor,\n",
    "    POST_PROCESSOR_OV_PATH,\n",
    "    torch.rand([1, 3, 1024, 32, 32], dtype=torch.float32),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb89337-851f-4fe8-9a3f-f77d19f680ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAMERA_EMBEDDER_OV_PATH = Path(\"models/camera_embedder_ir.xml\")\n",
    "\n",
    "\n",
    "class CameraEmbedderWrapper(torch.nn.Module):\n",
    "    def __init__(self, camera_embedder):\n",
    "        super().__init__()\n",
    "        self.camera_embedder = camera_embedder\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        rgb_cond=None,\n",
    "        mask_cond=None,\n",
    "        c2w_cond=None,\n",
    "        intrinsic_cond=None,\n",
    "        intrinsic_normed_cond=None,\n",
    "    ):\n",
    "        kwargs = {\n",
    "            \"rgb_cond\": rgb_cond,\n",
    "            \"mask_cond\": mask_cond,\n",
    "            \"c2w_cond\": c2w_cond,\n",
    "            \"intrinsic_cond\": intrinsic_cond,\n",
    "            \"intrinsic_normed_cond\": intrinsic_normed_cond,\n",
    "        }\n",
    "        embedding = self.camera_embedder(**kwargs)\n",
    "\n",
    "        return embedding\n",
    "\n",
    "\n",
    "example_input = {\n",
    "    \"rgb_cond\": torch.rand([1, 1, 512, 512, 3], dtype=torch.float32),\n",
    "    \"mask_cond\": torch.rand([1, 1, 512, 512, 1], dtype=torch.float32),\n",
    "    \"c2w_cond\": torch.rand([1, 1, 1, 4, 4], dtype=torch.float32),\n",
    "    \"intrinsic_cond\": torch.rand([1, 1, 1, 3, 3], dtype=torch.float32),\n",
    "    \"intrinsic_normed_cond\": torch.rand([1, 1, 1, 3, 3], dtype=torch.float32),\n",
    "}\n",
    "convert(\n",
    "    CameraEmbedderWrapper(model.camera_embedder),\n",
    "    CAMERA_EMBEDDER_OV_PATH,\n",
    "    example_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0e6f73-1414-481c-abc2-5380f5c1fff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEstimatorWrapper(torch.nn.Module):\n",
    "    def __init__(self, image_estimator):\n",
    "        super().__init__()\n",
    "        self.image_estimator = image_estimator\n",
    "\n",
    "    def forward(self, cond_image):\n",
    "        outputs = self.image_estimator(cond_image)\n",
    "        filtered_ouptuts = {}\n",
    "        for k, v in outputs.items():\n",
    "            if k.startswith(\"decoder_\"):\n",
    "                filtered_ouptuts[k] = v\n",
    "        return filtered_ouptuts\n",
    "\n",
    "\n",
    "IMAGE_ESTIMATOR_OV_PATH = Path(\"models/image_estimator_ir.xml\")\n",
    "example_input = {\n",
    "    \"cond_image\": torch.rand([1, 1, 512, 512, 3], dtype=torch.float32),\n",
    "}\n",
    "convert(\n",
    "    ImageEstimatorWrapper(model.image_estimator),\n",
    "    IMAGE_ESTIMATOR_OV_PATH,\n",
    "    torch.rand([1, 1, 512, 512, 3], dtype=torch.float32),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d733e264-c5bb-432c-9fa7-ab6c5759131f",
   "metadata": {},
   "source": [
    "The decoder accepts lists of include or exclude heads in forward method and uses them to choose a part of heads. We can't accept a list of strings in ir-model, but we can build 2 decoders with required structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516ed124-d99a-47c2-8e13-298582d5f83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "include_cfg_decoder = [h for h in model.decoder.cfg.heads if h.name in [\"vertex_offset\", \"density\"]]\n",
    "exclude_cfg_decoder = [h for h in model.decoder.cfg.heads if h.name not in [\"density\", \"vertex_offset\"]]\n",
    "\n",
    "\n",
    "INCLUDE_DECODER_OV_PATH = Path(\"models/include_decoder_ir.xml\")\n",
    "EXCLUDE_DECODER_OV_PATH = Path(\"models/exclude_decoder_ir.xml\")\n",
    "\n",
    "\n",
    "model.decoder.cfg_heads = include_cfg_decoder\n",
    "convert(\n",
    "    model.decoder,\n",
    "    INCLUDE_DECODER_OV_PATH,\n",
    "    torch.rand([1, 535882, 120], dtype=torch.float32),\n",
    ")\n",
    "\n",
    "\n",
    "model.decoder.cfg_heads = exclude_cfg_decoder\n",
    "convert(\n",
    "    model.decoder,\n",
    "    EXCLUDE_DECODER_OV_PATH,\n",
    "    torch.rand([263302, 120], dtype=torch.float32),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78044cc7-cd4d-4026-9ca7-ca715233ad49",
   "metadata": {},
   "source": [
    "## Compiling models and prepare pipeline\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b3405c-f35b-4d73-a3c2-d2ba1df1f73f",
   "metadata": {},
   "source": [
    "Select device from dropdown list for running inference using OpenVINO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65fc34c-8fd9-45ad-bd22-c27caedada91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_utils import device_widget\n",
    "\n",
    "device = device_widget()\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5a3970-0727-44d6-934c-49cefdfb33c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "core = ov.Core()\n",
    "\n",
    "compiled_image_tokenizer = core.compile_model(IMAGE_TOKENIZER_OV_PATH, device.value)\n",
    "compiled_tokenizer = core.compile_model(TOKENIZER_OV_PATH, device.value)\n",
    "compiled_backbone = core.compile_model(BACKBONE_OV_PATH, device.value)\n",
    "compiled_post_processor = core.compile_model(POST_PROCESSOR_OV_PATH, device.value)\n",
    "compiled_camera_embedder = core.compile_model(CAMERA_EMBEDDER_OV_PATH, device.value)\n",
    "compiled_image_estimator = core.compile_model(IMAGE_ESTIMATOR_OV_PATH, device.value)\n",
    "compiled_include_decoder = core.compile_model(INCLUDE_DECODER_OV_PATH, device.value)\n",
    "compiled_exclude_decoder = core.compile_model(EXCLUDE_DECODER_OV_PATH, device.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d2b6c4-6c87-4c97-abf9-b6b06ddbf1e0",
   "metadata": {},
   "source": [
    "Let's create callable wrapper classes for compiled models to allow interaction with original `SF3D` class. Note that all of wrapper classes return `torch.Tensor`s instead of `np.array`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735dc4b0-ca2a-4d90-9b3c-3d2dd6a0342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class ImageTokenizerWrapper(torch.nn.Module):\n",
    "    def __init__(self, image_tokenizer):\n",
    "        super().__init__()\n",
    "        self.image_tokenizer = image_tokenizer\n",
    "\n",
    "    def forward(self, images, modulation_cond):\n",
    "        inputs = {\n",
    "            \"images\": images,\n",
    "            \"modulation_cond\": modulation_cond,\n",
    "        }\n",
    "        outs = self.image_tokenizer(inputs)[0]\n",
    "\n",
    "        return torch.from_numpy(outs)\n",
    "\n",
    "\n",
    "class TokenizerWrapper(torch.nn.Module):\n",
    "    def __init__(self, tokenizer, model):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.detokenize = model.detokenize\n",
    "\n",
    "    def forward(self, batch_size):\n",
    "        outs = self.tokenizer(batch_size)[0]\n",
    "\n",
    "        return torch.from_numpy(outs)\n",
    "\n",
    "\n",
    "class BackboneWrapper(torch.nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "    def forward(self, hidden_states, encoder_hidden_states, **kwargs):\n",
    "        inputs = {\n",
    "            \"hidden_states\": hidden_states,\n",
    "            \"encoder_hidden_states\": encoder_hidden_states.detach().numpy(),\n",
    "        }\n",
    "\n",
    "        outs = self.backbone(inputs)[0]\n",
    "\n",
    "        return torch.from_numpy(outs)\n",
    "\n",
    "\n",
    "class PostProcessorWrapper(torch.nn.Module):\n",
    "    def __init__(self, post_processor):\n",
    "        super().__init__()\n",
    "        self.post_processor = post_processor\n",
    "\n",
    "    def forward(self, triplanes):\n",
    "        outs = self.post_processor(triplanes)[0]\n",
    "\n",
    "        return torch.from_numpy(outs)\n",
    "\n",
    "\n",
    "class CameraEmbedderWrapper(torch.nn.Module):\n",
    "    def __init__(self, camera_embedder):\n",
    "        super().__init__()\n",
    "        self.camera_embedder = camera_embedder\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        outs = self.camera_embedder(kwargs)[0]\n",
    "\n",
    "        return torch.from_numpy(outs)\n",
    "\n",
    "\n",
    "class ImageEstimatorWrapper(torch.nn.Module):\n",
    "    def __init__(self, image_estimator):\n",
    "        super().__init__()\n",
    "        self.image_estimator = image_estimator\n",
    "\n",
    "    def forward(self, cond_image):\n",
    "        outs = self.image_estimator(cond_image)\n",
    "\n",
    "        results = {}\n",
    "        for k, v in outs.to_dict().items():\n",
    "            results[k.names.pop()] = torch.from_numpy(v)\n",
    "        return results\n",
    "\n",
    "\n",
    "class DecoderWrapper(torch.nn.Module):\n",
    "    def __init__(self, include_decoder, exclude_decoder):\n",
    "        super().__init__()\n",
    "        self.include_decoder = include_decoder\n",
    "        self.exclude_decoder = exclude_decoder\n",
    "\n",
    "    def forward(self, x, include: Optional[List] = None, exclude: Optional[List] = None):\n",
    "        if include is not None:\n",
    "            outs = self.include_decoder(x)\n",
    "        else:\n",
    "            outs = self.exclude_decoder(x)\n",
    "        results = {}\n",
    "        for k, v in outs.to_dict().items():\n",
    "            results[k.names.pop()] = torch.from_numpy(v)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7868c13-458f-42a7-9117-fcd4ee21e1d2",
   "metadata": {},
   "source": [
    "Replace all models in the original model by wrappers instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d874b148-5625-4c8f-a8db-06ac897bae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.image_tokenizer = ImageTokenizerWrapper(compiled_image_tokenizer)\n",
    "model.tokenizer = TokenizerWrapper(compiled_tokenizer, model.tokenizer)\n",
    "model.backbone = BackboneWrapper(compiled_backbone)\n",
    "model.post_processor = PostProcessorWrapper(compiled_post_processor)\n",
    "model.camera_embedder = CameraEmbedderWrapper(compiled_camera_embedder)\n",
    "model.image_estimator = ImageEstimatorWrapper(compiled_image_estimator)\n",
    "model.decoder = DecoderWrapper(compiled_include_decoder, compiled_exclude_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0864266-7743-4336-af8e-994d190df1b0",
   "metadata": {},
   "source": [
    "## Interactive inference\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "It's taken from the original `gradio_app.py`, but the model is replaced with the one defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5a2031-195e-4d77-bb74-a9d4cd3dc7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "from functools import lru_cache\n",
    "from typing import Any\n",
    "\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import rembg\n",
    "import torch\n",
    "from gradio_litmodel3d import LitModel3D\n",
    "from PIL import Image\n",
    "\n",
    "import sf3d.utils as sf3d_utils\n",
    "from sf3d.system import SF3D\n",
    "\n",
    "os.environ[\"GRADIO_TEMP_DIR\"] = os.path.join(os.environ.get(\"TMPDIR\", \"/tmp\"), \"gradio\")\n",
    "\n",
    "rembg_session = rembg.new_session()\n",
    "\n",
    "COND_WIDTH = 512\n",
    "COND_HEIGHT = 512\n",
    "COND_DISTANCE = 1.6\n",
    "COND_FOVY_DEG = 40\n",
    "BACKGROUND_COLOR = [0.5, 0.5, 0.5]\n",
    "\n",
    "# Cached. Doesn't change\n",
    "c2w_cond = sf3d_utils.default_cond_c2w(COND_DISTANCE)\n",
    "intrinsic, intrinsic_normed_cond = sf3d_utils.create_intrinsic_from_fov_deg(COND_FOVY_DEG, COND_HEIGHT, COND_WIDTH)\n",
    "\n",
    "generated_files = []\n",
    "\n",
    "example_files = [os.path.join(\"stable-fast-3d/demo_files/examples\", f) for f in os.listdir(\"stable-fast-3d/demo_files/examples\")]\n",
    "\n",
    "\n",
    "def run_model(input_image, remesh_option, vertex_count, texture_size):\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        with nullcontext():\n",
    "            model_batch = create_batch(input_image)\n",
    "            model_batch = {k: v.to(\"cpu\") for k, v in model_batch.items()}\n",
    "            print(f\"{model_batch.keys()=}\")\n",
    "            print(f\"{texture_size=}\")\n",
    "            print(f\"{remesh_option=}\")\n",
    "            print(f\"{vertex_count=}\")\n",
    "            trimesh_mesh, _glob_dict = model.generate_mesh(model_batch, texture_size, remesh_option, vertex_count)\n",
    "            trimesh_mesh = trimesh_mesh[0]\n",
    "\n",
    "    # Create new tmp file\n",
    "    tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".glb\")\n",
    "\n",
    "    trimesh_mesh.export(tmp_file.name, file_type=\"glb\", include_normals=True)\n",
    "    generated_files.append(tmp_file.name)\n",
    "\n",
    "    print(\"Generation took:\", time.time() - start, \"s\")\n",
    "\n",
    "    return tmp_file.name\n",
    "\n",
    "\n",
    "def create_batch(input_image: Image) -> dict[str, Any]:\n",
    "    img_cond = torch.from_numpy(np.asarray(input_image.resize((COND_WIDTH, COND_HEIGHT))).astype(np.float32) / 255.0).float().clip(0, 1)\n",
    "    mask_cond = img_cond[:, :, -1:]\n",
    "    rgb_cond = torch.lerp(torch.tensor(BACKGROUND_COLOR)[None, None, :], img_cond[:, :, :3], mask_cond)\n",
    "\n",
    "    batch_elem = {\n",
    "        \"rgb_cond\": rgb_cond,\n",
    "        \"mask_cond\": mask_cond,\n",
    "        \"c2w_cond\": c2w_cond.unsqueeze(0),\n",
    "        \"intrinsic_cond\": intrinsic.unsqueeze(0),\n",
    "        \"intrinsic_normed_cond\": intrinsic_normed_cond.unsqueeze(0),\n",
    "    }\n",
    "    # Add batch dim\n",
    "    batched = {k: v.unsqueeze(0) for k, v in batch_elem.items()}\n",
    "    return batched\n",
    "\n",
    "\n",
    "@lru_cache\n",
    "def checkerboard(squares: int, size: int, min_value: float = 0.5):\n",
    "    base = np.zeros((squares, squares)) + min_value\n",
    "    base[1::2, ::2] = 1\n",
    "    base[::2, 1::2] = 1\n",
    "\n",
    "    repeat_mult = size // squares\n",
    "    return base.repeat(repeat_mult, axis=0).repeat(repeat_mult, axis=1)[:, :, None].repeat(3, axis=-1)\n",
    "\n",
    "\n",
    "def remove_background(input_image: Image) -> Image:\n",
    "    return rembg.remove(input_image, session=rembg_session)\n",
    "\n",
    "\n",
    "def resize_foreground(\n",
    "    image: Image,\n",
    "    ratio: float,\n",
    ") -> Image:\n",
    "    image = np.array(image)\n",
    "    assert image.shape[-1] == 4\n",
    "    alpha = np.where(image[..., 3] > 0)\n",
    "    y1, y2, x1, x2 = (\n",
    "        alpha[0].min(),\n",
    "        alpha[0].max(),\n",
    "        alpha[1].min(),\n",
    "        alpha[1].max(),\n",
    "    )\n",
    "    # crop the foreground\n",
    "    fg = image[y1:y2, x1:x2]\n",
    "    # pad to square\n",
    "    size = max(fg.shape[0], fg.shape[1])\n",
    "    ph0, pw0 = (size - fg.shape[0]) // 2, (size - fg.shape[1]) // 2\n",
    "    ph1, pw1 = size - fg.shape[0] - ph0, size - fg.shape[1] - pw0\n",
    "    new_image = np.pad(\n",
    "        fg,\n",
    "        ((ph0, ph1), (pw0, pw1), (0, 0)),\n",
    "        mode=\"constant\",\n",
    "        constant_values=((0, 0), (0, 0), (0, 0)),\n",
    "    )\n",
    "\n",
    "    # compute padding according to the ratio\n",
    "    new_size = int(new_image.shape[0] / ratio)\n",
    "    # pad to size, double side\n",
    "    ph0, pw0 = (new_size - size) // 2, (new_size - size) // 2\n",
    "    ph1, pw1 = new_size - size - ph0, new_size - size - pw0\n",
    "    new_image = np.pad(\n",
    "        new_image,\n",
    "        ((ph0, ph1), (pw0, pw1), (0, 0)),\n",
    "        mode=\"constant\",\n",
    "        constant_values=((0, 0), (0, 0), (0, 0)),\n",
    "    )\n",
    "    new_image = Image.fromarray(new_image, mode=\"RGBA\").resize((COND_WIDTH, COND_HEIGHT))\n",
    "    return new_image\n",
    "\n",
    "\n",
    "def square_crop(input_image: Image) -> Image:\n",
    "    # Perform a center square crop\n",
    "    min_size = min(input_image.size)\n",
    "    left = (input_image.size[0] - min_size) // 2\n",
    "    top = (input_image.size[1] - min_size) // 2\n",
    "    right = (input_image.size[0] + min_size) // 2\n",
    "    bottom = (input_image.size[1] + min_size) // 2\n",
    "    return input_image.crop((left, top, right, bottom)).resize((COND_WIDTH, COND_HEIGHT))\n",
    "\n",
    "\n",
    "def show_mask_img(input_image: Image) -> Image:\n",
    "    img_numpy = np.array(input_image)\n",
    "    alpha = img_numpy[:, :, 3] / 255.0\n",
    "    chkb = checkerboard(32, 512) * 255\n",
    "    new_img = img_numpy[..., :3] * alpha[:, :, None] + chkb * (1 - alpha[:, :, None])\n",
    "    return Image.fromarray(new_img.astype(np.uint8), mode=\"RGB\")\n",
    "\n",
    "\n",
    "def run_button(\n",
    "    run_btn,\n",
    "    input_image,\n",
    "    background_state,\n",
    "    foreground_ratio,\n",
    "    remesh_option,\n",
    "    vertex_count,\n",
    "    texture_size,\n",
    "):\n",
    "    if run_btn == \"Run\":\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "        glb_file: str = run_model(background_state, remesh_option.lower(), vertex_count, texture_size)\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"Peak Memory:\", torch.cuda.max_memory_allocated() / 1024 / 1024, \"MB\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            print(\"Peak Memory:\", torch.mps.driver_allocated_memory() / 1024 / 1024, \"MB\")\n",
    "\n",
    "        return (\n",
    "            gr.update(),\n",
    "            gr.update(),\n",
    "            gr.update(),\n",
    "            gr.update(),\n",
    "            gr.update(value=glb_file, visible=True),\n",
    "            gr.update(visible=True),\n",
    "        )\n",
    "    elif run_btn == \"Remove Background\":\n",
    "        rem_removed = remove_background(input_image)\n",
    "\n",
    "        sqr_crop = square_crop(rem_removed)\n",
    "        fr_res = resize_foreground(sqr_crop, foreground_ratio)\n",
    "\n",
    "        return (\n",
    "            gr.update(value=\"Run\", visible=True),\n",
    "            sqr_crop,\n",
    "            fr_res,\n",
    "            gr.update(value=show_mask_img(fr_res), visible=True),\n",
    "            gr.update(value=None, visible=False),\n",
    "            gr.update(visible=False),\n",
    "        )\n",
    "\n",
    "\n",
    "def requires_bg_remove(image, fr):\n",
    "    if image is None:\n",
    "        return (\n",
    "            gr.update(visible=False, value=\"Run\"),\n",
    "            None,\n",
    "            None,\n",
    "            gr.update(value=None, visible=False),\n",
    "            gr.update(visible=False),\n",
    "            gr.update(visible=False),\n",
    "        )\n",
    "    alpha_channel = np.array(image.getchannel(\"A\"))\n",
    "    min_alpha = alpha_channel.min()\n",
    "\n",
    "    if min_alpha == 0:\n",
    "        print(\"Already has alpha\")\n",
    "        sqr_crop = square_crop(image)\n",
    "        fr_res = resize_foreground(sqr_crop, fr)\n",
    "        return (\n",
    "            gr.update(value=\"Run\", visible=True),\n",
    "            sqr_crop,\n",
    "            fr_res,\n",
    "            gr.update(value=show_mask_img(fr_res), visible=True),\n",
    "            gr.update(visible=False),\n",
    "            gr.update(visible=False),\n",
    "        )\n",
    "    return (\n",
    "        gr.update(value=\"Remove Background\", visible=True),\n",
    "        None,\n",
    "        None,\n",
    "        gr.update(value=None, visible=False),\n",
    "        gr.update(visible=False),\n",
    "        gr.update(visible=False),\n",
    "    )\n",
    "\n",
    "\n",
    "def update_foreground_ratio(img_proc, fr):\n",
    "    foreground_res = resize_foreground(img_proc, fr)\n",
    "    return (\n",
    "        foreground_res,\n",
    "        gr.update(value=show_mask_img(foreground_res)),\n",
    "    )\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    img_proc_state = gr.State()\n",
    "    background_remove_state = gr.State()\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "    # SF3D: Stable Fast 3D Mesh Reconstruction with UV-unwrapping and Illumination Disentanglement\n",
    "\n",
    "    **SF3D** is a state-of-the-art method for 3D mesh reconstruction from a single image.\n",
    "    This demo allows you to upload an image and generate a 3D mesh model from it.\n",
    "\n",
    "    **Tips**\n",
    "    1. If the image already has an alpha channel, you can skip the background removal step.\n",
    "    2. You can adjust the foreground ratio to control the size of the foreground object. This can influence the shape\n",
    "    3. You can select the remeshing option to control the mesh topology. This can introduce artifacts in the mesh on thin surfaces and should be turned off in such cases.\n",
    "    4. You can upload your own HDR environment map to light the 3D model.\n",
    "    \"\"\"\n",
    "    )\n",
    "    with gr.Row(variant=\"panel\"):\n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                input_img = gr.Image(type=\"pil\", label=\"Input Image\", sources=\"upload\", image_mode=\"RGBA\")\n",
    "                preview_removal = gr.Image(\n",
    "                    label=\"Preview Background Removal\",\n",
    "                    type=\"pil\",\n",
    "                    image_mode=\"RGB\",\n",
    "                    interactive=False,\n",
    "                    visible=False,\n",
    "                )\n",
    "\n",
    "            foreground_ratio = gr.Slider(\n",
    "                label=\"Foreground Ratio\",\n",
    "                minimum=0.5,\n",
    "                maximum=1.0,\n",
    "                value=0.85,\n",
    "                step=0.05,\n",
    "            )\n",
    "\n",
    "            foreground_ratio.change(\n",
    "                update_foreground_ratio,\n",
    "                inputs=[img_proc_state, foreground_ratio],\n",
    "                outputs=[background_remove_state, preview_removal],\n",
    "            )\n",
    "\n",
    "            remesh_option = gr.Radio(\n",
    "                choices=[\"None\", \"Triangle\", \"Quad\"],\n",
    "                label=\"Remeshing\",\n",
    "                value=\"None\",\n",
    "                visible=True,\n",
    "            )\n",
    "\n",
    "            vertex_count_slider = gr.Slider(\n",
    "                label=\"Target Vertex Count\",\n",
    "                minimum=1000,\n",
    "                maximum=20000,\n",
    "                value=10000,\n",
    "                step=1000,\n",
    "                visible=True,\n",
    "            )\n",
    "\n",
    "            texture_size = gr.Slider(\n",
    "                label=\"Texture Size\",\n",
    "                minimum=512,\n",
    "                maximum=2048,\n",
    "                value=1024,\n",
    "                step=256,\n",
    "                visible=True,\n",
    "            )\n",
    "\n",
    "            run_btn = gr.Button(\"Run\", variant=\"primary\", visible=False)\n",
    "\n",
    "        with gr.Column():\n",
    "            output_3d = LitModel3D(\n",
    "                label=\"3D Model\",\n",
    "                visible=False,\n",
    "                clear_color=[0.0, 0.0, 0.0, 0.0],\n",
    "                tonemapping=\"aces\",\n",
    "                contrast=1.0,\n",
    "                scale=1.0,\n",
    "            )\n",
    "            with gr.Column(visible=False, scale=1.0) as hdr_row:\n",
    "                gr.Markdown(\n",
    "                    \"\"\"## HDR Environment Map\n",
    "\n",
    "                Select an HDR environment map to light the 3D model. You can also upload your own HDR environment maps.\n",
    "                \"\"\"\n",
    "                )\n",
    "\n",
    "                with gr.Row():\n",
    "                    hdr_illumination_file = gr.File(label=\"HDR Env Map\", file_types=[\".hdr\"], file_count=\"single\")\n",
    "                    example_hdris = [os.path.join(\"stable-fast-3d/demo_files/hdri\", f) for f in os.listdir(\"stable-fast-3d/demo_files/hdri\")]\n",
    "                    hdr_illumination_example = gr.Examples(\n",
    "                        examples=example_hdris,\n",
    "                        inputs=hdr_illumination_file,\n",
    "                    )\n",
    "\n",
    "                    hdr_illumination_file.change(\n",
    "                        lambda x: gr.update(env_map=x.name if x is not None else None),\n",
    "                        inputs=hdr_illumination_file,\n",
    "                        outputs=[output_3d],\n",
    "                    )\n",
    "\n",
    "    examples = gr.Examples(\n",
    "        examples=example_files,\n",
    "        inputs=input_img,\n",
    "    )\n",
    "\n",
    "    input_img.change(\n",
    "        requires_bg_remove,\n",
    "        inputs=[input_img, foreground_ratio],\n",
    "        outputs=[\n",
    "            run_btn,\n",
    "            img_proc_state,\n",
    "            background_remove_state,\n",
    "            preview_removal,\n",
    "            output_3d,\n",
    "            hdr_row,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    run_btn.click(\n",
    "        run_button,\n",
    "        inputs=[\n",
    "            run_btn,\n",
    "            input_img,\n",
    "            background_remove_state,\n",
    "            foreground_ratio,\n",
    "            remesh_option,\n",
    "            vertex_count_slider,\n",
    "            texture_size,\n",
    "        ],\n",
    "        outputs=[\n",
    "            run_btn,\n",
    "            img_proc_state,\n",
    "            background_remove_state,\n",
    "            preview_removal,\n",
    "            output_3d,\n",
    "            hdr_row,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "try:\n",
    "    demo.queue().launch(debug=True)\n",
    "except Exception:\n",
    "    demo.queue().launch(debug=True, share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "openvino_notebooks": {
   "imageUrl": "https://github.com/Stability-AI/stable-fast-3d/blob/main/demo_files/teaser.gif?raw=true",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Image-to-3D"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
