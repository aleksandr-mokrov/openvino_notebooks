{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07baa7d5-89a4-4b22-9dc2-7e43ec2ff9a9",
   "metadata": {},
   "source": [
    "# Text-to-Video retrieval with S3D MIL-NCE and OpenVINO\n",
    "\n",
    "This tutorial based on [the TenosrFlow tutorial](https://www.tensorflow.org/hub/tutorials/text_to_video_retrieval_with_s3d_milnce) that demonstrates how to use the [S3D MIL-NCE](https://tfhub.dev/deepmind/mil-nce/s3d/1) model from TensorFlow Hub to do text-to-video retrieval to find the most similar videos for a given text query.\n",
    "\n",
    "#### Table of contents:\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [The original inference](#The-original-inference)\n",
    "- [Convert the model to OpenVINO IR](#Convert-the-model-to-OpenVINO-IR)\n",
    "- [Compiling models](#Compiling-models)\n",
    "- [Inference](#Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac784f8-5511-4631-ad4c-d3dd816f9d07",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ec6500-5ce4-4630-a2a4-813059126a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "\n",
    "%pip uninstall -q -y openvino-dev openvino openvino-nightly\n",
    "%pip install -q --upgrade openvino-nightly\n",
    "%pip install -q openvino-tokenizers --upgrade --pre --extra-index-url \"https://storage.openvinotoolkit.org/simple/wheels/nightly\"\n",
    "%pip install -q \"tensorflow-macos>=2.5; sys_platform == 'darwin' and platform_machine == 'arm64' and python_version > '3.8'\" # macOS M1 and M2\n",
    "%pip install -q \"tensorflow-macos>=2.5,<=2.12.0; sys_platform == 'darwin' and platform_machine == 'arm64' and python_version <= '3.8'\" # macOS M1 and M2\n",
    "%pip install -q \"tensorflow>=2.5; sys_platform == 'darwin' and platform_machine != 'arm64' and python_version > '3.8'\" # macOS x86\n",
    "%pip install -q \"tensorflow>=2.5,<=2.12.0; sys_platform == 'darwin' and platform_machine != 'arm64' and python_version <= '3.8'\" # macOS x86\n",
    "%pip install -q \"tensorflow>=2.5; sys_platform != 'darwin' and python_version > '3.8'\"\n",
    "%pip install -q \"tensorflow>=2.5,<=2.12.0; sys_platform != 'darwin' and python_version <= '3.8'\"\n",
    "\n",
    "%pip install -q tensorflow_hub tf_keras numpy\n",
    "if platform.system() != \"Windows\":\n",
    "    %pip install -q \"matplotlib>=3.4\"\n",
    "else:\n",
    "    %pip install -q \"matplotlib>=3.4,<3.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0153bf4b-94e1-44a8-99e6-4a2433b29c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from IPython import display\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b3c5db-ae42-45a0-ada0-7f1319221621",
   "metadata": {},
   "source": [
    "Download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dea4952-780b-42e1-a973-c45fd4b8508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_handle = \"https://tfhub.dev/deepmind/mil-nce/s3d/1\"\n",
    "hub_model = hub.load(hub_handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a962845-20cf-454a-b796-59dc7891ebcf",
   "metadata": {},
   "source": [
    "The model has 2 signatures, one for generating video embeddings and one for generating text embeddings. We will use these embedding to find the nearest neighbors in the embedding space as in the original turorial. Below we will define auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74e1ea65-7bae-4a49-a520-8e5898adbbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(model, input_frames, input_words):\n",
    "    \"\"\"Generate embeddings from the model from video frames and input words.\"\"\"\n",
    "    # Input_frames must be normalized in [0, 1] and of the shape Batch x T x H x W x 3\n",
    "    vision_output = model.signatures[\"video\"](tf.constant(tf.cast(input_frames, dtype=tf.float32)))\n",
    "    text_output = model.signatures[\"text\"](tf.constant(input_words))\n",
    "\n",
    "    return vision_output[\"video_embedding\"], text_output[\"text_embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d762ea2b-77a4-4d1b-9121-e3b4861534cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define video loading and visualization functions  { display-mode: \"form\" }\n",
    "\n",
    "\n",
    "# Utilities to open video files using CV2\n",
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(video_url, max_frames=32, resize=(224, 224)):\n",
    "    path = tf.keras.utils.get_file(os.path.basename(video_url)[-128:], video_url)\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    frames = np.array(frames)\n",
    "    if len(frames) < max_frames:\n",
    "        n_repeat = int(math.ceil(max_frames / float(len(frames))))\n",
    "        frames = frames.repeat(n_repeat, axis=0)\n",
    "    frames = frames[:max_frames]\n",
    "    return frames / 255.0\n",
    "\n",
    "\n",
    "def display_video(urls):\n",
    "    html = \"<table>\"\n",
    "    html += \"<tr><th>Video 1</th><th>Video 2</th><th>Video 3</th></tr><tr>\"\n",
    "    for url in urls:\n",
    "        html += \"<td>\"\n",
    "        html += '<img src=\"{}\" height=\"224\">'.format(url)\n",
    "        html += \"</td>\"\n",
    "    html += \"</tr></table>\"\n",
    "    return display.HTML(html)\n",
    "\n",
    "\n",
    "def display_query_and_results_video(query, urls, scores):\n",
    "    \"\"\"Display a text query and the top result videos and scores.\"\"\"\n",
    "    sorted_ix = np.argsort(-scores)\n",
    "    html = \"\"\n",
    "    html += \"<h2>Input query: <i>{}</i> </h2><div>\".format(query)\n",
    "    html += \"Results: <div>\"\n",
    "    html += \"<table>\"\n",
    "    html += \"<tr><th>Rank #1, Score:{:.2f}</th>\".format(scores[sorted_ix[0]])\n",
    "    html += \"<th>Rank #2, Score:{:.2f}</th>\".format(scores[sorted_ix[1]])\n",
    "    html += \"<th>Rank #3, Score:{:.2f}</th></tr><tr>\".format(scores[sorted_ix[2]])\n",
    "    for i, idx in enumerate(sorted_ix):\n",
    "        url = urls[sorted_ix[i]]\n",
    "        html += \"<td>\"\n",
    "        html += '<img src=\"{}\" height=\"224\">'.format(url)\n",
    "        html += \"</td>\"\n",
    "    html += \"</tr></table>\"\n",
    "\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09e0047d-6fb8-4b18-a66d-e8e8a37154ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>Video 1</th><th>Video 2</th><th>Video 3</th></tr><tr><td><img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b0/YosriAirTerjun.gif\" height=\"224\"></td><td><img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e6/Guitar_solo_gif.gif\" height=\"224\"></td><td><img src=\"https://upload.wikimedia.org/wikipedia/commons/3/30/2009-08-16-autodrift-by-RalfR-gif-by-wau.gif\" height=\"224\"></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @title Load example videos and define text queries  { display-mode: \"form\" }\n",
    "\n",
    "video_1_url = \"https://upload.wikimedia.org/wikipedia/commons/b/b0/YosriAirTerjun.gif\"  # @param {type:\"string\"}\n",
    "video_2_url = \"https://upload.wikimedia.org/wikipedia/commons/e/e6/Guitar_solo_gif.gif\"  # @param {type:\"string\"}\n",
    "video_3_url = \"https://upload.wikimedia.org/wikipedia/commons/3/30/2009-08-16-autodrift-by-RalfR-gif-by-wau.gif\"  # @param {type:\"string\"}\n",
    "\n",
    "video_1 = load_video(video_1_url)\n",
    "video_2 = load_video(video_2_url)\n",
    "video_3 = load_video(video_3_url)\n",
    "all_videos = [video_1, video_2, video_3]\n",
    "\n",
    "query_1_video = \"waterfall\"  # @param {type:\"string\"}\n",
    "query_2_video = \"playing guitar\"  # @param {type:\"string\"}\n",
    "query_3_video = \"car drifting\"  # @param {type:\"string\"}\n",
    "all_queries_video = [query_1_video, query_2_video, query_3_video]\n",
    "all_videos_urls = [video_1_url, video_2_url, video_3_url]\n",
    "display_video(all_videos_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b91a14a-d565-4526-bd81-7a6c3e15c235",
   "metadata": {},
   "source": [
    "## The original inference\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0563060f-0ff4-428f-8ac6-9f7b240e570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare video inputs.\n",
    "videos_np = np.stack(all_videos, axis=0)\n",
    "\n",
    "# Prepare text input.\n",
    "words_np = np.array(all_queries_video)\n",
    "\n",
    "# Generate the video and text embeddings.\n",
    "video_embd, text_embd = generate_embeddings(hub_model, videos_np, words_np)\n",
    "\n",
    "# Scores between video and text is computed by dot products.\n",
    "all_scores = np.dot(text_embd, tf.transpose(video_embd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fac7925-676e-46b5-b48b-d2b3397d51de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Input query: <i>waterfall</i> </h2><div>Results: <div><table><tr><th>Rank #1, Score:4.71</th><th>Rank #2, Score:-1.63</th><th>Rank #3, Score:-4.17</th></tr><tr><td><img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b0/YosriAirTerjun.gif\" height=\"224\"></td><td><img src=\"https://upload.wikimedia.org/wikipedia/commons/3/30/2009-08-16-autodrift-by-RalfR-gif-by-wau.gif\" height=\"224\"></td><td><img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e6/Guitar_solo_gif.gif\" height=\"224\"></td></tr></table><br><h2>Input query: <i>playing guitar</i> </h2><div>Results: <div><table><tr><th>Rank #1, Score:6.50</th><th>Rank #2, Score:-1.79</th><th>Rank #3, Score:-2.67</th></tr><tr><td><img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e6/Guitar_solo_gif.gif\" height=\"224\"></td><td><img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b0/YosriAirTerjun.gif\" height=\"224\"></td><td><img src=\"https://upload.wikimedia.org/wikipedia/commons/3/30/2009-08-16-autodrift-by-RalfR-gif-by-wau.gif\" height=\"224\"></td></tr></table><br><h2>Input query: <i>car drifting</i> </h2><div>Results: <div><table><tr><th>Rank #1, Score:8.78</th><th>Rank #2, Score:-1.07</th><th>Rank #3, Score:-2.17</th></tr><tr><td><img src=\"https://upload.wikimedia.org/wikipedia/commons/3/30/2009-08-16-autodrift-by-RalfR-gif-by-wau.gif\" height=\"224\"></td><td><img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b0/YosriAirTerjun.gif\" height=\"224\"></td><td><img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e6/Guitar_solo_gif.gif\" height=\"224\"></td></tr></table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display results.\n",
    "html = \"\"\n",
    "for i, words in enumerate(words_np):\n",
    "    html += display_query_and_results_video(words, all_videos_urls, all_scores[i, :])\n",
    "    html += \"<br>\"\n",
    "display.HTML(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a0373a-ee9e-4728-97b2-355d1ed501a3",
   "metadata": {},
   "source": [
    "## Convert the model to OpenVINO IR\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "OpenVINO supports TensorFlow models via conversion into Intermediate Representation (IR) format. We need to provide a model object, input data for model tracing to `ov.convert_model` function to obtain OpenVINO `ov.Model` object instance. Model can be saved on disk for next deployment using `ov.save_model` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1f51b7b-a215-44af-90be-83eb9d2cfd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino_tokenizers  # need to import conversion and operation extensions\n",
    "import openvino as ov\n",
    "\n",
    "model_path = hub.resolve(hub_handle)\n",
    "# infer on random data\n",
    "images_data = np.random.rand(3, 32, 224, 224, 3).astype(np.float32)\n",
    "words_data = np.array([\"First sentence\", \"Second one\", \"Abracadabra\"], dtype=str)\n",
    "\n",
    "ov_model = ov.convert_model(model_path, input=[(\"words\", [3]), (\"images\", [3, 32, 224, 224, 3])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62128306-2021-41eb-a965-2494fa8abbf2",
   "metadata": {},
   "source": [
    "## Compiling models\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Select device from dropdown list for running inference using OpenVINO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3f0dd83-4803-41f8-8af2-1117b25db19f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e5c0220a4d4ba8903ff500bcc7fbb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=4, options=('CPU', 'GPU.0', 'GPU.1', 'GPU.2', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "core = ov.Core()\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3306fcc-bcfc-41f9-adbb-79afe259e4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = core.compile_model(ov_model, device.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632d8390-36b8-4e46-9226-2546d3be678b",
   "metadata": {},
   "source": [
    "## Inference\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e653b36-2c45-4609-b24f-64401ed0ca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine `generate_embeddings` function to make it possible to use the compile IR model.\n",
    "def generate_embeddings(model, input_frames, input_words):\n",
    "    \"\"\"Generate embeddings from the model from video frames and input words.\"\"\"\n",
    "    # Input_frames must be normalized in [0, 1] and of the shape Batch x T x H x W x 3\n",
    "    output = compiled_model({\"words\": input_words, \"images\": tf.cast(input_frames, dtype=tf.float32)})\n",
    "\n",
    "    return output[\"video_embedding\"], output[\"text_embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a28a44c9-9c1a-427f-b7a8-eab007d0f4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the video and text embeddings.\n",
    "video_embd, text_embd = generate_embeddings(compiled_model, videos_np, words_np)\n",
    "\n",
    "# Scores between video and text is computed by dot products.\n",
    "all_scores = np.dot(text_embd, tf.transpose(video_embd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "545a657b-849f-454a-a8c9-17f1a653f3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>Input query: <i>waterfall</i> </h2><div>Results: <div><table><tr><th>Rank #1, Score:4.71</th><th>Rank #2, Score:-1.63</th><th>Rank #3, Score:-4.17</th></tr><tr><td><img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b0/YosriAirTerjun.gif\" height=\"224\"></td><td><img src=\"https://upload.wikimedia.org/wikipedia/commons/3/30/2009-08-16-autodrift-by-RalfR-gif-by-wau.gif\" height=\"224\"></td><td><img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e6/Guitar_solo_gif.gif\" height=\"224\"></td></tr></table><br><h2>Input query: <i>playing guitar</i> </h2><div>Results: <div><table><tr><th>Rank #1, Score:6.50</th><th>Rank #2, Score:-1.79</th><th>Rank #3, Score:-2.67</th></tr><tr><td><img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e6/Guitar_solo_gif.gif\" height=\"224\"></td><td><img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b0/YosriAirTerjun.gif\" height=\"224\"></td><td><img src=\"https://upload.wikimedia.org/wikipedia/commons/3/30/2009-08-16-autodrift-by-RalfR-gif-by-wau.gif\" height=\"224\"></td></tr></table><br><h2>Input query: <i>car drifting</i> </h2><div>Results: <div><table><tr><th>Rank #1, Score:8.78</th><th>Rank #2, Score:-1.07</th><th>Rank #3, Score:-2.17</th></tr><tr><td><img src=\"https://upload.wikimedia.org/wikipedia/commons/3/30/2009-08-16-autodrift-by-RalfR-gif-by-wau.gif\" height=\"224\"></td><td><img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b0/YosriAirTerjun.gif\" height=\"224\"></td><td><img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e6/Guitar_solo_gif.gif\" height=\"224\"></td></tr></table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display results.\n",
    "html = \"\"\n",
    "for i, words in enumerate(words_np):\n",
    "    html += display_query_and_results_video(words, all_videos_urls, all_scores[i, :])\n",
    "    html += \"<br>\"\n",
    "display.HTML(html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "openvino_notebooks": {
    "imageUrl": "https://github.com/openvinotoolkit/openvino_notebooks/assets/76171391/ba516a81-f6f7-4258-9e3b-931d6db7728c",
    "tags": {
     "categories": [
      "Model Demos"
     ],
     "libraries": [],
     "other": [],
     "tasks": [
      "Text-to-Video Retrieval"
     ]
    }
   }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
