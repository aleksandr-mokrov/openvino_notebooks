{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a name='1'></a>\n",
    "## Prerequisites"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6eed66854c398c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%pip install -q \"openvino>=2023.1.0\"\n",
    "!git clone https://github.com/svc-develop-team/so-vits-svc -b 4.1-Stable\n",
    "%cd so-vits-svc\n",
    "%pip install --upgrade pip setuptools\n",
    "%pip install -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu118"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1aaa6bb335f4efe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/hubert_base.pt -O pretrain/checkpoint_best_legacy_500.pt\n",
    "!wget \"https://huggingface.co/therealvul/so-vits-svc-4.0/resolve/main/Rainbow%20Dash%20(singing)/kmeans_10000.pt\" -O logs/44k/kmeans_10000.pt\n",
    "!wget \"https://huggingface.co/therealvul/so-vits-svc-4.0/resolve/main/Rainbow%20Dash%20(singing)/config.json\" -O configs/config.json\n",
    "!wget \"https://huggingface.co/therealvul/so-vits-svc-4.0/resolve/main/Rainbow%20Dash%20(singing)/G_30400.pth\" -O logs/44k/G_30400.pth\n",
    "!wget \"https://huggingface.co/therealvul/so-vits-svc-4.0/resolve/main/Rainbow%20Dash%20(singing)/D_30400.pth\" -O logs/44k/D_30400.pth\n",
    "!wget \"https://huggingface.co/datasets/santifiorino/spinetta/resolve/main/spinetta/000.wav\" -O raw/000.wav"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ca855b365c4e0e7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use the original model to run an inference"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f53cf28ef37988f"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/amokrov/PycharmProjects/default/my_openvino_notebooks/notebooks/259-sofvc-voice-conversion/so-vits-svc\n"
     ]
    }
   ],
   "source": [
    "# %cd so-vits-svc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T16:23:39.582765900Z",
     "start_time": "2023-10-11T16:23:39.441185100Z"
    }
   },
   "id": "324125b043342928"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 18:24:01.873828: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-11 18:24:02.234937: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-11 18:24:04.713462: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:xformers:WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.0.1+cu118 with CUDA 1108 (you have 2.1.0+cu118)\n",
      "    Python  3.8.16 (you have 3.8.10)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "load model(s) from pretrain/checkpoint_best_legacy_500.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amokrov/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "from inference.infer_tool import Svc\n",
    "\n",
    "model = Svc(\"logs/44k/G_30400.pth\", \"configs/config.json\", device='cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T16:24:11.594409400Z",
     "start_time": "2023-10-11T16:23:48.236884800Z"
    }
   },
   "id": "8cba0ed03ac900ce"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#=====segment start, 8.44s======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/amokrov/PycharmProjects/default/my_openvino_notebooks/notebooks/259-sofvc-voice-conversion/so-vits-svc/inference/infer_tool.py:270: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vits use time:5.225666522979736\n",
      "#=====segment start, 4.62s======\n",
      "vits use time:3.4846408367156982\n",
      "#=====segment start, 0.002s======\n",
      "jump empty segment\n"
     ]
    }
   ],
   "source": [
    "kwarg = {\n",
    "    'raw_audio_path': 'raw/000.wav', \n",
    "    'spk': 'Rainbow Dash (singing)', \n",
    "    'tran': 0, \n",
    "    'slice_db': -40, \n",
    "    'cluster_infer_ratio': 0, \n",
    "    'auto_predict_f0': False, \n",
    "    'noice_scale': 0.4, \n",
    "    'pad_seconds': 0.5, \n",
    "    'clip_seconds': 0, \n",
    "    'lg_num': 0, \n",
    "    'lgr_num': 0.75, \n",
    "    'f0_predictor': 'pm', \n",
    "    'enhancer_adaptive_key': 0, \n",
    "    'cr_threshold': 0.05, \n",
    "    'k_step': 100, \n",
    "    'use_spk_mix': False, \n",
    "    'second_encoding': False, \n",
    "    'loudness_envelope_adjustment': 1\n",
    "}\n",
    "\n",
    "audio = model.slice_inference(**kwarg)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T16:24:34.084985700Z",
     "start_time": "2023-10-11T16:24:22.823731700Z"
    }
   },
   "id": "364d72774d2863d0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "ipd.Audio(audio, rate=model.target_sample)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a941d393fb0d9f5b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert to OpenVINO IR model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae244d48f7982d4c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "dummy_c = torch.randn(1, 256, 813)\n",
    "dummy_f0 = torch.randn(1, 813)\n",
    "dummy_uv = torch.ones(1, 813)\n",
    "dummy_g = torch.tensor([[0]])\n",
    "model.net_g_ms.forward = model.net_g_ms.infer\n",
    "\n",
    "#input_info = [(\"c\", dummy_c.shape, torch.float32),(\"f0\", dummy_f0.shape, torch.float32),(\"uv\", dummy_uv.shape, torch.float32), (\"g\", dummy_g.shape, torch.int64)]\n",
    "net_g_kwargs = {\n",
    "    'c': dummy_c,\n",
    "    'f0': dummy_f0,\n",
    "    'uv': dummy_uv,\n",
    "    'g': dummy_g,\n",
    "    \n",
    "    # 'noice_scale': 0.35,\n",
    "    # 'seed': 52468,\n",
    "    # 'predict_f0': False,\n",
    "    # 'vol':  0\n",
    "}\n",
    "core = ov.Core()\n",
    "\n",
    "converted_model = ov.convert_model(model.net_g_ms, example_input=net_g_kwargs)\n",
    "\n",
    "net_g_model_xml_path = Path('models/ov_net_g_model.xml')\n",
    "\n",
    "net_g_model_xml_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "ov.save_model(converted_model, net_g_model_xml_path)\n",
    "#compiled_net_g_model = core.compile_model(net_g_model_xml_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "259b86a26d06f881"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import openvino as ov\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value='AUTO',\n",
    "    description='Device:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b907034e797533dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class NetGModelWrapper:\n",
    "    def __init__(self, net_g_model_xml_path):\n",
    "        super().__init__()\n",
    "        self.net_g_model = core.compile_model(net_g_model_xml_path, device.value)\n",
    "        \n",
    "    def infer(self, c, *, f0, uv, g, noice_scale=0.35, seed=52468, predict_f0=False, vol = None):\n",
    "        print(self.net_g_model)\n",
    "        print(c.shape, f0.shape, uv.shape, g.shape)\n",
    "        #results = self.net_g_model({'c': c, 'f0': f0, 'uv': uv, 'g': g})[0]\n",
    "        results = self.net_g_model((c, f0, uv, g))[0]\n",
    "        return results\n",
    "\n",
    "        \n",
    "#compiled_net_g_model.create_infer_request()\n",
    "model.net_g_ms = NetGModelWrapper(net_g_model_xml_path)\n",
    "#compiled_net_g_model.infer = compiled_net_g_model.__call__\n",
    "audio = model.slice_inference(**kwarg)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb890ffe86cd0a84"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert model by parts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c77eb5d3da65c1e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Svc(\"logs/44k/G_30400.pth\", \"configs/config.json\", device='cpu')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "944a4a405f3eb9d3"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/amokrov/PycharmProjects/default/my_openvino_notebooks/notebooks/259-sofvc-voice-conversion/so-vits-svc/modules/attentions.py:217: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert t_s == t_t, \"Relative attention is only available for self-attention.\"\n",
      "/mnt/c/Users/amokrov/PycharmProjects/default/my_openvino_notebooks/notebooks/259-sofvc-voice-conversion/so-vits-svc/modules/attentions.py:262: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  pad_length = max(length - (self.window_size + 1), 0)\n",
      "/mnt/c/Users/amokrov/PycharmProjects/default/my_openvino_notebooks/notebooks/259-sofvc-voice-conversion/so-vits-svc/modules/attentions.py:263: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  slice_start_position = max((self.window_size + 1) - length, 0)\n",
      "/mnt/c/Users/amokrov/PycharmProjects/default/my_openvino_notebooks/notebooks/259-sofvc-voice-conversion/so-vits-svc/modules/attentions.py:265: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if pad_length > 0:\n"
     ]
    }
   ],
   "source": [
    "# x.shape=torch.Size([1, 192, 813]), x_mask.shape=torch.Size([1, 1, 813]), f0_to_coarse(f0).shape=torch.Size([1, 813]), noice_scale=0.4\n",
    "\n",
    "import openvino as ov\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from utils import f0_to_coarse\n",
    "\n",
    "\n",
    "dummy_x = torch.randn(1, 192, 813)\n",
    "dummy_x_mask = torch.randn(1, 1, 813)\n",
    "f0 = torch.randn(1, 813)\n",
    "noice_scale = torch.tensor(0.4)\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "#converted_enc_p_model = ov.convert_model(model.net_g_ms.enc_p, example_input=(dummy_x, dummy_x_mask, f0_to_coarse(f0), noice_scale))\n",
    "converted_enc_p_model = ov.convert_model(model.net_g_ms.enc_p.enc_, example_input=(dummy_x, dummy_x_mask))\n",
    "\n",
    "enc_p_model_xml_path = Path('models/ov_enc_p_model.xml')\n",
    "\n",
    "enc_p_model_xml_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "ov.save_model(converted_enc_p_model, enc_p_model_xml_path)\n",
    "#compiled_enc_p_model = core.compile_model(enc_p_model_xml_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T16:25:01.475931500Z",
     "start_time": "2023-10-11T16:24:57.814011800Z"
    }
   },
   "id": "78298cef54bd38a"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Exception from src/inference/src/infer_request.cpp:224:\n[ PARAMETER_MISMATCH ] Can not clone with new dims. Descriptor's shape: {1, ? - ?, 96} is incompatible with provided dimensions: {1, 1625, 96}.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m compiled_enc_p_model \u001B[38;5;241m=\u001B[39m core\u001B[38;5;241m.\u001B[39mcompile_model(enc_p_model_xml_path)\n\u001B[0;32m----> 2\u001B[0m \u001B[43mcompiled_enc_p_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdummy_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdummy_x_mask\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/openvino/runtime/ie_api.py:384\u001B[0m, in \u001B[0;36mCompiledModel.__call__\u001B[0;34m(self, inputs, share_inputs, share_outputs, shared_memory)\u001B[0m\n\u001B[1;32m    381\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_infer_request \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    382\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_infer_request \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_infer_request()\n\u001B[0;32m--> 384\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_infer_request\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minfer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    385\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    386\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshare_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_deprecated_memory_arg\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshared_memory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshare_inputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    387\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshare_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshare_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    388\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/my_openvino_notebooks/lib/python3.8/site-packages/openvino/runtime/ie_api.py:143\u001B[0m, in \u001B[0;36mInferRequest.infer\u001B[0;34m(self, inputs, share_inputs, share_outputs, shared_memory)\u001B[0m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minfer\u001B[39m(\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     70\u001B[0m     inputs: Any \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     74\u001B[0m     shared_memory: Any \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     75\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m OVDict:\n\u001B[1;32m     76\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Infers specified input(s) in synchronous mode.\u001B[39;00m\n\u001B[1;32m     77\u001B[0m \n\u001B[1;32m     78\u001B[0m \u001B[38;5;124;03m    Blocks all methods of InferRequest while request is running.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;124;03m    :rtype: OVDict\u001B[39;00m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 143\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m OVDict(\u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minfer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_data_dispatch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    144\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    145\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    146\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_shared\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_deprecated_memory_arg\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshared_memory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshare_inputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    147\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshare_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshare_outputs\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Exception from src/inference/src/infer_request.cpp:224:\n[ PARAMETER_MISMATCH ] Can not clone with new dims. Descriptor's shape: {1, ? - ?, 96} is incompatible with provided dimensions: {1, 1625, 96}.\n"
     ]
    }
   ],
   "source": [
    "compiled_enc_p_model = core.compile_model(enc_p_model_xml_path)\n",
    "compiled_enc_p_model((dummy_x, dummy_x_mask))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T16:25:06.335491100Z",
     "start_time": "2023-10-11T16:25:04.601471900Z"
    }
   },
   "id": "59867c4fadb3883f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# z_p.shape=torch.Size([1, 192, 813]), c_mask.shape=torch.Size([1, 1, 813]), g.shape=torch.Size([1, 256, 1])\n",
    "dummy_z_p = torch.randn(1, 192, 813)\n",
    "dummy_c_mask = torch.randn(1, 1, 813)\n",
    "dummy_g = torch.randn(1, 256, 1)\n",
    "dummy_reverse = torch.tensor(True)\n",
    "\n",
    "converted_flow_model = ov.convert_model(model.net_g_ms.flow, example_input=(dummy_z_p, dummy_c_mask, dummy_g, dummy_reverse))\n",
    "\n",
    "flow_model_xml_path = Path('models/ov_flow_model.xml')\n",
    "\n",
    "flow_model_xml_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "ov.save_model(converted_flow_model, flow_model_xml_path)\n",
    "#compiled_flow_model = core.compile_model(flow_model_xml_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe7409caab5e27a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# (z * c_mask).shape=torch.Size([1, 192, 813]), g.shape=torch.Size([1, 256, 1]), f0.shape=torch.Size([1, 813])\n",
    "\n",
    "dummy_z_c_mask = torch.randn(1, 192, 813)\n",
    "dummy_g = torch.randn(1, 256, 1)\n",
    "f0 = torch.randn(1, 813)\n",
    "\n",
    "\n",
    "converted_dec_model = ov.convert_model(model.net_g_ms.dec, example_input=(dummy_z_c_mask, f0, dummy_g))\n",
    "\n",
    "dec_model_xml_path = Path('models/ov_dec_model.xml')\n",
    "\n",
    "dec_model_xml_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "ov.save_model(converted_dec_model, dec_model_xml_path)\n",
    "#compiled_dec_model = core.compile_model(dec_model_xml_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8bed7b04be399ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Svc(\"logs/44k/G_30400.pth\", \"configs/config.json\", device='cpu')\n",
    "\n",
    "class EncPModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model_path):\n",
    "        super().__init__()\n",
    "        self.compiled_model = core.compile_model(model_path, device.value)\n",
    "\n",
    "    # def forward(self, x, x_mask, f0=None, noice_scale=1):\n",
    "    #     return self.compiled_model((x, x_mask, f0, noice_scale))[0]\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        x = self.compiled_model((x, x_mask))[0]\n",
    "        return x\n",
    "\n",
    "\n",
    "class FlowWrapper(torch.nn.Module):\n",
    "    def __init__(self, model_path):\n",
    "        super().__init__()\n",
    "        self.compiled_model = core.compile_model(model_path, device.value)\n",
    "\n",
    "    def forward(self, x, x_mask, g=None, reverse=False):\n",
    "        z = self.compiled_model((x, x_mask, g, reverse))[0]\n",
    "        return torch.tensor(z)\n",
    "\n",
    "\n",
    "class DecWrapper(torch.nn.Module):\n",
    "    def __init__(self, model_path):\n",
    "        super().__init__()\n",
    "        self.compiled_model = core.compile_model(model_path, device.value)\n",
    "\n",
    "    def forward(self, z_c_mask, *, g, f0):\n",
    "        o = self.compiled_model((z_c_mask, f0, g))[0]\n",
    "        return torch.tensor(o)\n",
    "\n",
    "\n",
    "# model.net_g_ms.enc_p.enc_ = EncPModelWrapper(enc_p_model_xml_path)\n",
    "model.net_g_ms.flow = FlowWrapper(flow_model_xml_path)\n",
    "model.net_g_ms.dec = DecWrapper(dec_model_xml_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c3b2468a16cfe3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.net_g_ms.enc_p.enc_.compiled_model((dummy_x, dummy_x_mask))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b975ad947a16d5dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "audio = model.slice_inference(**kwarg)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79ee71559e1217a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "ipd.Audio(audio, rate=model.target_sample)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bea8d8b65b7fb95d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
