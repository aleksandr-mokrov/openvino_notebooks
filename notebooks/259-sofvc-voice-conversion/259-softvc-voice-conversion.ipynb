{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a name='1'></a>\n",
    "## Prerequisites"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6eed66854c398c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%pip install -q datasets \"transformers>=4.33.1\" accelerate \"openvino>=2023.1.0\"\n",
    "%git clone https://github.com/svc-develop-team/so-vits-svc -b 4.1-Stable\n",
    "%cd so-vits-svc\n",
    "%pip install --upgrade pip setuptools\n",
    "%pip install -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu118"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1aaa6bb335f4efe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wget -P pretrain/ https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/hubert_base.pt -O checkpoint_best_legacy_500.pt\n",
    "!wget -P logs/44k/ https://huggingface.co/Sucial/so-vits-svc4.1-sanwu/resolve/main/kmeans_10000.pt -O kmeans_10000.pt\n",
    "!wget -P logs/44k/ https://huggingface.co/Sucial/so-vits-svc4.1-sanwu/resolve/main/sanwu_100800.pth -O sanwu_100800.pth\n",
    "!wget -P config/ https://huggingface.co/Sucial/so-vits-svc4.1-sanwu/resolve/main/config.json -O config.json\n",
    "!wget -P logs/44k/ https://huggingface.co/Sucial/so-vits-svc4.1-sanwu/resolve/main/feature_and_index.pkl -O feature_and_index.pkl\n",
    "!wget -P logs/44k/ \"https://huggingface.co/therealvul/so-vits-svc-4.0/resolve/main/Rainbow%20Dash%20(singing)/G_30400.pth\" -O G_30400.pth\n",
    "!wget -P logs/44k/ \"https://huggingface.co/therealvul/so-vits-svc-4.0/resolve/main/Rainbow%20Dash%20(singing)/D_30400.pth\" -O D_30400.pth"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ca855b365c4e0e7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use the original model to run an inference"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f53cf28ef37988f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from inference.infer_tool import Svc\n",
    "\n",
    "model = Svc(\"logs/44k/G_30400.pth\", \"configs/config.json\", device='cpu')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cba0ed03ac900ce"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kwarg = {\n",
    "    'raw_audio_path': 'raw/p226_002.wav', \n",
    "    'spk': 'sanwu', \n",
    "    'tran': 0, \n",
    "    'slice_db': -40, \n",
    "    'cluster_infer_ratio': 0, \n",
    "    'auto_predict_f0': False, \n",
    "    'noice_scale': 0.4, \n",
    "    'pad_seconds': 0.5, \n",
    "    'clip_seconds': 0, \n",
    "    'lg_num': 0, \n",
    "    'lgr_num': 0.75, \n",
    "    'f0_predictor': 'pm', \n",
    "    'enhancer_adaptive_key': 0, \n",
    "    'cr_threshold': 0.05, \n",
    "    'k_step': 100, \n",
    "    'use_spk_mix': False, \n",
    "    'second_encoding': False, \n",
    "    'loudness_envelope_adjustment': 1\n",
    "}\n",
    "\n",
    "audio = model.slice_inference(**kwarg)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "364d72774d2863d0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "ipd.Audio(audio, rate=model.target_sample)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a941d393fb0d9f5b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert to OpenVINO IR model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae244d48f7982d4c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "dummy_c = torch.randn(1, 768, 457)\n",
    "dummy_f0 = torch.randn(1, 457)\n",
    "dummy_uv = torch.ones(1, 457)\n",
    "dummy_g = torch.tensor([[0]])\n",
    "model.net_g_ms.forward = model.net_g_ms.infer\n",
    "\n",
    "#input_info = [(\"c\", dummy_c.shape, torch.float32),(\"f0\", dummy_f0.shape, torch.float32),(\"uv\", dummy_uv.shape, torch.float32), (\"g\", dummy_g.shape, torch.int64)]\n",
    "net_g_kwargs = {\n",
    "    'c': dummy_c,\n",
    "    'f0': dummy_f0,\n",
    "    'uv': dummy_uv,\n",
    "    'g': dummy_g,\n",
    "    \n",
    "    # 'noice_scale': 0.35,\n",
    "    # 'seed': 52468,\n",
    "    # 'predict_f0': False,\n",
    "    # 'vol':  0\n",
    "}\n",
    "core = ov.Core()\n",
    "\n",
    "converted_model = ov.convert_model(model.net_g_ms, example_input=net_g_kwargs)\n",
    "\n",
    "net_g_model_xml_path = Path('models/ov_net_g_model.xml')\n",
    "\n",
    "net_g_model_xml_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "ov.save_model(converted_model, net_g_model_xml_path)\n",
    "#compiled_net_g_model = core.compile_model(net_g_model_xml_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "259b86a26d06f881"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import openvino as ov\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value='AUTO',\n",
    "    description='Device:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b907034e797533dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class NetGModelWrapper:\n",
    "    def __init__(self, net_g_model_xml_path):\n",
    "        super().__init__()\n",
    "        self.net_g_model = core.compile_model(net_g_model_xml_path, device.value)\n",
    "        \n",
    "    def infer(self, c, *, f0, uv, g, noice_scale=0.35, seed=52468, predict_f0=False, vol = None):\n",
    "        print(self.net_g_model)\n",
    "        print(c.shape, f0.shape, uv.shape, g.shape)\n",
    "        #results = self.net_g_model({'c': c, 'f0': f0, 'uv': uv, 'g': g})[0]\n",
    "        results = self.net_g_model((c, f0, uv, g))[0]\n",
    "        return results\n",
    "\n",
    "        \n",
    "\n",
    "kwarg = {\n",
    "    'raw_audio_path': 'raw/p226_002.wav', \n",
    "    'spk': 'sanwu', \n",
    "    'tran': 0, \n",
    "    'slice_db': -40, \n",
    "    'cluster_infer_ratio': 0, \n",
    "    'auto_predict_f0': False, \n",
    "    'noice_scale': 0.4, \n",
    "    'pad_seconds': 0.5, \n",
    "    'clip_seconds': 0, \n",
    "    'lg_num': 0, \n",
    "    'lgr_num': 0.75, \n",
    "    'f0_predictor': 'pm', \n",
    "    'enhancer_adaptive_key': 0, \n",
    "    'cr_threshold': 0.05, \n",
    "    'k_step': 100, \n",
    "    'use_spk_mix': False, \n",
    "    'second_encoding': False, \n",
    "    'loudness_envelope_adjustment': 1\n",
    "}\n",
    "#compiled_net_g_model.create_infer_request()\n",
    "model.net_g_ms = NetGModelWrapper(net_g_model_xml_path)\n",
    "#compiled_net_g_model.infer = compiled_net_g_model.__call__\n",
    "audio = model.slice_inference(**kwarg)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb890ffe86cd0a84"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert model by parts"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c77eb5d3da65c1e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Svc(\"logs/44k/G_30400.pth\", \"configs/config.json\", device='cpu')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "944a4a405f3eb9d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#(x * x_mask).shape=torch.Size([1, 192, 457]), x_mask.shape=torch.Size([1, 1, 457])\n",
    "# x.shape=torch.Size([1, 192, 457]), x_mask.shape=torch.Size([1, 1, 457]), f0_to_coarse(f0).shape=torch.Size([1, 457]), noice_scale=0.4\n",
    "# x.dtype=torch.float32, x_mask.dtype=torch.float32, f0_to_coarse(f0).dtype=torch.int64, noice_scale=0.4\n",
    "\n",
    "import openvino as ov\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from utils import f0_to_coarse\n",
    "\n",
    "\n",
    "dummy_x = torch.randn(1, 192, 457)\n",
    "dummy_x_mask = torch.randn(1, 1, 457)\n",
    "f0 = torch.randn(1, 457)\n",
    "noice_scale = torch.tensor(0.4)\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "#converted_enc_p_model = ov.convert_model(model.net_g_ms.enc_p, example_input=(dummy_x, dummy_x_mask, f0_to_coarse(f0), noice_scale))\n",
    "converted_enc_p_model = ov.convert_model(model.net_g_ms.enc_p.enc_, example_input=(dummy_x, dummy_x_mask))\n",
    "\n",
    "enc_p_model_xml_path = Path('models/ov_enc_p_model.xml')\n",
    "\n",
    "enc_p_model_xml_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "ov.save_model(converted_enc_p_model, enc_p_model_xml_path)\n",
    "#compiled_enc_p_model = core.compile_model(enc_p_model_xml_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78298cef54bd38a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "compiled_enc_p_model = core.compile_model(enc_p_model_xml_path)\n",
    "compiled_enc_p_model(dummy_x, dummy_x_mask)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59867c4fadb3883f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# z_p.shape=torch.Size([1, 192, 457]), c_mask.shape=torch.Size([1, 1, 457]), g.shape=torch.Size([1, 768, 1])\n",
    "dummy_z_p = torch.randn(1, 192, 457)\n",
    "dummy_c_mask = torch.randn(1, 1, 457)\n",
    "dummy_g = torch.randn(1, 768, 1)\n",
    "dummy_reverse = torch.tensor(True)\n",
    "\n",
    "converted_flow_model = ov.convert_model(model.net_g_ms.flow, example_input=(dummy_z_p, dummy_c_mask, dummy_g, dummy_reverse))\n",
    "\n",
    "flow_model_xml_path = Path('models/ov_flow_model.xml')\n",
    "\n",
    "flow_model_xml_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "ov.save_model(converted_flow_model, flow_model_xml_path)\n",
    "#compiled_flow_model = core.compile_model(flow_model_xml_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe7409caab5e27a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# (z * c_mask).shape=torch.Size([1, 192, 457]), g.shape=torch.Size([1, 768, 1]), f0.shape=torch.Size([1, 457])\n",
    "\n",
    "dummy_z_c_mask = torch.randn(1, 192, 457)\n",
    "dummy_g = torch.randn(1, 768, 1)\n",
    "f0 = torch.randn(1, 457)\n",
    "\n",
    "\n",
    "converted_dec_model = ov.convert_model(model.net_g_ms.dec, example_input=(dummy_z_c_mask, f0, dummy_g))\n",
    "\n",
    "dec_model_xml_path = Path('models/ov_dec_model.xml')\n",
    "\n",
    "dec_model_xml_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "ov.save_model(converted_dec_model, dec_model_xml_path)\n",
    "#compiled_dec_model = core.compile_model(dec_model_xml_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8bed7b04be399ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Svc(\"logs/44k/G_30400.pth\", \"configs/config.json\", device='cpu')\n",
    "\n",
    "class EncPModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model_path):\n",
    "        super().__init__()\n",
    "        self.compiled_model = core.compile_model(model_path, device.value)\n",
    "\n",
    "    # def forward(self, x, x_mask, f0=None, noice_scale=1):\n",
    "    #     return self.compiled_model((x, x_mask, f0, noice_scale))[0]\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        x = self.compiled_model((x, x_mask))[0]\n",
    "        return x\n",
    "\n",
    "\n",
    "class FlowWrapper(torch.nn.Module):\n",
    "    def __init__(self, model_path):\n",
    "        super().__init__()\n",
    "        self.compiled_model = core.compile_model(model_path, device.value)\n",
    "\n",
    "    def forward(self, x, x_mask, g=None, reverse=False):\n",
    "        z = self.compiled_model((x, x_mask, g, reverse))[0]\n",
    "        return torch.tensor(z)\n",
    "\n",
    "\n",
    "class DecWrapper(torch.nn.Module):\n",
    "    def __init__(self, model_path):\n",
    "        super().__init__()\n",
    "        self.compiled_model = core.compile_model(model_path, device.value)\n",
    "\n",
    "    def forward(self, z_c_mask, *, g, f0):\n",
    "        o = self.compiled_model((z_c_mask, f0, g))[0]\n",
    "        return torch.tensor(o)\n",
    "\n",
    "\n",
    "model.net_g_ms.enc_p.enc_ = EncPModelWrapper(enc_p_model_xml_path)\n",
    "model.net_g_ms.flow = FlowWrapper(flow_model_xml_path)\n",
    "model.net_g_ms.dec = DecWrapper(dec_model_xml_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c3b2468a16cfe3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.net_g_ms.enc_p.enc_.compiled_model((dummy_x, dummy_x_mask))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b975ad947a16d5dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kwarg = {\n",
    "    'raw_audio_path': 'raw/p226_002.wav', \n",
    "    'spk': 'sanwu', \n",
    "    'tran': 0, \n",
    "    'slice_db': -40, \n",
    "    'cluster_infer_ratio': 0, \n",
    "    'auto_predict_f0': False, \n",
    "    'noice_scale': 0.4, \n",
    "    'pad_seconds': 0.5, \n",
    "    'clip_seconds': 0, \n",
    "    'lg_num': 0, \n",
    "    'lgr_num': 0.75, \n",
    "    'f0_predictor': 'pm', \n",
    "    'enhancer_adaptive_key': 0, \n",
    "    'cr_threshold': 0.05, \n",
    "    'k_step': 100, \n",
    "    'use_spk_mix': False, \n",
    "    'second_encoding': False, \n",
    "    'loudness_envelope_adjustment': 1\n",
    "}\n",
    "\n",
    "audio = model.slice_inference(**kwarg)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bea8d8b65b7fb95d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
