{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14186cec-1749-4be0-96b9-56ab5518ddb6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Kosmos-2: Multimodal Large Language Model and OpenVINO\n",
    "\n",
    "[KOSMOS-2](https://github.com/microsoft/unilm/tree/master/kosmos-2) is a multimodal large language model (MLLM) that has new capabilities of multimodal grounding and referring. KOSMOS-2 can understand multimodal input, follow instructions, \n",
    "perceive object descriptions (e.g., bounding boxes), and ground language to the visual world.\n",
    "\n",
    "Multimodal Large Language Models (MLLMs) have successfully played a role as a general-purpose interface across a wide range of tasks, such as language, vision, and vision-language tasks. MLLMs can perceive general modalities, including texts, images, and audio, and generate responses using free-form texts under zero-shot and few-shot settings. \n",
    "\n",
    "[In this work](https://arxiv.org/abs/2306.14824), authors unlock the grounding capability for multimodal large language models. Grounding capability can provide a more convenient and efficient human-AI interaction for vision-language tasks. It enables the user to point to the object or region in the image directly rather than input detailed text descriptions to refer to it, the model can understand that image region with its spatial locations. Grounding capability also enables the model to respond with visual answers (i.e., bounding boxes), which can support more vision-language tasks such as referring expression comprehension. Visual answers are more accurate and resolve the coreference ambiguity compared with text-only responses. In addition, grounding capability can link noun phrases and referring expressions in the generated free-form text response to the image regions, providing more accurate, informational, and comprehensive answers.\n",
    "\n",
    "\n",
    "![image](https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/annotated_snowman.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663df1e2-6679-42e3-9b84-c55f5b4dc5a2",
   "metadata": {},
   "source": [
    "#### Table of contents:\n",
    "- [Install requirements](#Install-requirements)\n",
    "- [Original model inference](#Original-model-inference)\n",
    "- [Convert models to OpenVINO Intermediate representation (IR) format](#Convert-models-to-OpenVINO-Intermediate-representation-(IR)-format)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a75c7ce-6f94-43ba-8087-11d7ad717a6f",
   "metadata": {},
   "source": [
    "## Install requirements\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1176598f-a76b-4c3a-93c5-78f1bb4ea344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./venv/lib/python3.10/site-packages (23.3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001B[33mWARNING: Skipping openvino-dev as it is not installed.\u001B[0m\u001B[33m\n",
      "\u001B[0m\u001B[33mWARNING: Skipping openvino as it is not installed.\u001B[0m\u001B[33m\n",
      "\u001B[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install -q \"transformers>=4.35\" Pillow\n",
    "%pip install -q --extra-index-url https://download.pytorch.org/whl/cpu torch torchvision\n",
    "%pip uninstall -q -y openvino-dev openvino openvino-nightly\n",
    "%pip install -q \"openvino-nightly\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28152b98-b619-4f1b-9164-dd9c726ffb82",
   "metadata": {},
   "source": [
    "## Original model inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebad0822-fbb3-45ac-a91e-861ad725fcd4",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0, 64003,     4,     5,     6,     7,     8,     9,    10,    11,\n",
      "            12,    13,    14,    15,    16,    17,    18,    19,    20,    21,\n",
      "            22,    23,    24,    25,    26,    27,    28,    29,    30,    31,\n",
      "            32,    33,    34,    35,    36,    37,    38,    39,    40,    41,\n",
      "            42,    43,    44,    45,    46,    47,    48,    49,    50,    51,\n",
      "            52,    53,    54,    55,    56,    57,    58,    59,    60,    61,\n",
      "            62,    63,    64,    65,    66,    67, 64004, 64012,   712,  1648,\n",
      "             9, 64007,    10, 43867, 64008, 64009, 64057, 64876, 64010,  5950,\n",
      "           597,    32, 64007,    10,   646, 64008, 64009, 64018, 64924, 64010,\n",
      "             4,     2]])\n",
      "<image>. the, to and of as in I that' for is was- on’ it with The as at bet he have from by are \" you his “ this said not has an ( but had we her they will my or were their): up about out who one all been she can more would It</image><grounding> An image of<phrase> a snowman</phrase><object><patch_index_0044><patch_index_0863></object> warming himself by<phrase> a fire</phrase><object><patch_index_0005><patch_index_0911></object>.\n",
      "<grounding> An image of<phrase> a snowman</phrase><object><patch_index_0044><patch_index_0863></object> warming himself by<phrase> a fire</phrase><object><patch_index_0005><patch_index_0911></object>.\n",
      "An image of a snowman warming himself by a fire.\n",
      "[('a snowman', (12, 21), [(0.390625, 0.046875, 0.984375, 0.828125)]), ('a fire', (41, 47), [(0.171875, 0.015625, 0.484375, 0.890625)])]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "\n",
    "model = AutoModelForVision2Seq.from_pretrained(\"microsoft/kosmos-2-patch14-224\")\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/kosmos-2-patch14-224\")\n",
    "\n",
    "prompt = \"<grounding>An image of\"\n",
    "\n",
    "url = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.png\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# The original Kosmos-2 demo saves the image first then reload it. For some images, this will give slightly different image input and change the generation outputs.\n",
    "image.save(\"new_image.jpg\")\n",
    "image = Image.open(\"new_image.jpg\")\n",
    "\n",
    "inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    pixel_values=inputs[\"pixel_values\"],\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    image_embeds=None,\n",
    "    image_embeds_position_mask=inputs[\"image_embeds_position_mask\"],\n",
    "    use_cache=True,\n",
    "    max_new_tokens=128,\n",
    ")\n",
    "print(generated_ids)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_text)\n",
    "# Specify `cleanup_and_extract=False` in order to see the raw model generation.\n",
    "processed_text = processor.post_process_generation(generated_text, cleanup_and_extract=False)\n",
    "\n",
    "print(processed_text)\n",
    "# `<grounding> An image of<phrase> a snowman</phrase><object><patch_index_0044><patch_index_0863></object> warming himself by<phrase> a fire</phrase><object><patch_index_0005><patch_index_0911></object>.`\n",
    "\n",
    "# By default, the generated  text is cleanup and the entities are extracted.\n",
    "processed_text, entities = processor.post_process_generation(generated_text)\n",
    "\n",
    "print(processed_text)\n",
    "# `An image of a snowman warming himself by a fire.`\n",
    "\n",
    "print(entities)\n",
    "# `[('a snowman', (12, 21), [(0.390625, 0.046875, 0.984375, 0.828125)]), ('a fire', (41, 47), [(0.171875, 0.015625, 0.484375, 0.890625)])]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee206f85-101d-4276-8e31-50611c9a48ad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Convert models to OpenVINO Intermediate representation (IR) format\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1923eea-7e82-4e6d-92ed-c4a4d98dad60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 64003,     4,     5,     6,     7,     8,     9,    10,    11,\n",
       "            12,    13,    14,    15,    16,    17,    18,    19,    20,    21,\n",
       "            22,    23,    24,    25,    26,    27,    28,    29,    30,    31,\n",
       "            32,    33,    34,    35,    36,    37,    38,    39,    40,    41,\n",
       "            42,    43,    44,    45,    46,    47,    48,    49,    50,    51,\n",
       "            52,    53,    54,    55,    56,    57,    58,    59,    60,    61,\n",
       "            62,    63,    64,    65,    66,    67, 64004, 64012,   712,  1648,\n",
       "             9]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9caa46df-22db-4926-9a8c-9167a223dc58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([64007,    10, 43867, 64008, 64009, 64057, 64876, 64010,  5950,   597,\n",
       "           32, 64007,    10,   646, 64008, 64009, 64018, 64924, 64010,     4,\n",
       "            2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids[0][len(inputs[\"input_ids\"][0]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58406870-9efd-48b5-a1f3-5b1a790ce50a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Kosmos2Config {\n",
       "  \"_name_or_path\": \"microsoft/kosmos-2-patch14-224\",\n",
       "  \"architectures\": [\n",
       "    \"Kosmos2ForConditionalGeneration\"\n",
       "  ],\n",
       "  \"latent_query_num\": 64,\n",
       "  \"model_type\": \"kosmos-2\",\n",
       "  \"text_config\": {\n",
       "    \"_name_or_path\": \"\",\n",
       "    \"activation_dropout\": 0.0,\n",
       "    \"activation_function\": \"gelu\",\n",
       "    \"add_cross_attention\": false,\n",
       "    \"architectures\": null,\n",
       "    \"attention_dropout\": 0.1,\n",
       "    \"attention_heads\": 32,\n",
       "    \"bad_words_ids\": null,\n",
       "    \"begin_suppress_tokens\": null,\n",
       "    \"bos_token_id\": 0,\n",
       "    \"chunk_size_feed_forward\": 0,\n",
       "    \"cross_attention_hidden_size\": null,\n",
       "    \"decoder_start_token_id\": null,\n",
       "    \"diversity_penalty\": 0.0,\n",
       "    \"do_sample\": false,\n",
       "    \"dropout\": 0.1,\n",
       "    \"early_stopping\": false,\n",
       "    \"embed_dim\": 2048,\n",
       "    \"encoder_no_repeat_ngram_size\": 0,\n",
       "    \"eos_token_id\": 2,\n",
       "    \"exponential_decay_length_penalty\": null,\n",
       "    \"ffn_dim\": 8192,\n",
       "    \"finetuning_task\": null,\n",
       "    \"forced_bos_token_id\": null,\n",
       "    \"forced_eos_token_id\": null,\n",
       "    \"id2label\": {\n",
       "      \"0\": \"LABEL_0\",\n",
       "      \"1\": \"LABEL_1\"\n",
       "    },\n",
       "    \"init_std\": 0.02,\n",
       "    \"is_decoder\": false,\n",
       "    \"is_encoder_decoder\": false,\n",
       "    \"label2id\": {\n",
       "      \"LABEL_0\": 0,\n",
       "      \"LABEL_1\": 1\n",
       "    },\n",
       "    \"layer_norm_eps\": 1e-05,\n",
       "    \"layerdrop\": 0.0,\n",
       "    \"layers\": 24,\n",
       "    \"length_penalty\": 1.0,\n",
       "    \"max_length\": 20,\n",
       "    \"max_position_embeddings\": 2048,\n",
       "    \"min_length\": 0,\n",
       "    \"model_type\": \"kosmos_2_text_model\",\n",
       "    \"no_repeat_ngram_size\": 3,\n",
       "    \"num_beam_groups\": 1,\n",
       "    \"num_beams\": 1,\n",
       "    \"num_return_sequences\": 1,\n",
       "    \"output_attentions\": false,\n",
       "    \"output_hidden_states\": false,\n",
       "    \"output_scores\": false,\n",
       "    \"pad_token_id\": 1,\n",
       "    \"prefix\": null,\n",
       "    \"problem_type\": null,\n",
       "    \"pruned_heads\": {},\n",
       "    \"remove_invalid_values\": false,\n",
       "    \"repetition_penalty\": 1.0,\n",
       "    \"return_dict\": true,\n",
       "    \"return_dict_in_generate\": false,\n",
       "    \"scale_embedding\": true,\n",
       "    \"sep_token_id\": null,\n",
       "    \"suppress_tokens\": null,\n",
       "    \"task_specific_params\": null,\n",
       "    \"temperature\": 1.0,\n",
       "    \"tf_legacy_loss\": false,\n",
       "    \"tie_encoder_decoder\": false,\n",
       "    \"tie_word_embeddings\": true,\n",
       "    \"tokenizer_class\": null,\n",
       "    \"top_k\": 50,\n",
       "    \"top_p\": 1.0,\n",
       "    \"torch_dtype\": null,\n",
       "    \"torchscript\": false,\n",
       "    \"typical_p\": 1.0,\n",
       "    \"use_bfloat16\": false,\n",
       "    \"use_cache\": true,\n",
       "    \"vocab_size\": 65037\n",
       "  },\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.35.2\",\n",
       "  \"vision_config\": {\n",
       "    \"_name_or_path\": \"\",\n",
       "    \"add_cross_attention\": false,\n",
       "    \"architectures\": null,\n",
       "    \"attention_dropout\": 0.0,\n",
       "    \"bad_words_ids\": null,\n",
       "    \"begin_suppress_tokens\": null,\n",
       "    \"bos_token_id\": null,\n",
       "    \"chunk_size_feed_forward\": 0,\n",
       "    \"cross_attention_hidden_size\": null,\n",
       "    \"decoder_start_token_id\": null,\n",
       "    \"diversity_penalty\": 0.0,\n",
       "    \"do_sample\": false,\n",
       "    \"early_stopping\": false,\n",
       "    \"encoder_no_repeat_ngram_size\": 0,\n",
       "    \"eos_token_id\": null,\n",
       "    \"exponential_decay_length_penalty\": null,\n",
       "    \"finetuning_task\": null,\n",
       "    \"forced_bos_token_id\": null,\n",
       "    \"forced_eos_token_id\": null,\n",
       "    \"hidden_act\": \"quick_gelu\",\n",
       "    \"hidden_size\": 1024,\n",
       "    \"id2label\": {\n",
       "      \"0\": \"LABEL_0\",\n",
       "      \"1\": \"LABEL_1\"\n",
       "    },\n",
       "    \"image_size\": 224,\n",
       "    \"initializer_factor\": 1.0,\n",
       "    \"initializer_range\": 0.02,\n",
       "    \"intermediate_size\": 4096,\n",
       "    \"is_decoder\": false,\n",
       "    \"is_encoder_decoder\": false,\n",
       "    \"label2id\": {\n",
       "      \"LABEL_0\": 0,\n",
       "      \"LABEL_1\": 1\n",
       "    },\n",
       "    \"layer_norm_eps\": 1e-05,\n",
       "    \"length_penalty\": 1.0,\n",
       "    \"max_length\": 20,\n",
       "    \"min_length\": 0,\n",
       "    \"model_type\": \"kosmos_2_vision_model\",\n",
       "    \"no_repeat_ngram_size\": 0,\n",
       "    \"num_attention_heads\": 16,\n",
       "    \"num_beam_groups\": 1,\n",
       "    \"num_beams\": 1,\n",
       "    \"num_channels\": 3,\n",
       "    \"num_hidden_layers\": 24,\n",
       "    \"num_return_sequences\": 1,\n",
       "    \"output_attentions\": false,\n",
       "    \"output_hidden_states\": false,\n",
       "    \"output_scores\": false,\n",
       "    \"pad_token_id\": null,\n",
       "    \"patch_size\": 14,\n",
       "    \"prefix\": null,\n",
       "    \"problem_type\": null,\n",
       "    \"pruned_heads\": {},\n",
       "    \"remove_invalid_values\": false,\n",
       "    \"repetition_penalty\": 1.0,\n",
       "    \"return_dict\": true,\n",
       "    \"return_dict_in_generate\": false,\n",
       "    \"sep_token_id\": null,\n",
       "    \"suppress_tokens\": null,\n",
       "    \"task_specific_params\": null,\n",
       "    \"temperature\": 1.0,\n",
       "    \"tf_legacy_loss\": false,\n",
       "    \"tie_encoder_decoder\": false,\n",
       "    \"tie_word_embeddings\": true,\n",
       "    \"tokenizer_class\": null,\n",
       "    \"top_k\": 50,\n",
       "    \"top_p\": 1.0,\n",
       "    \"torch_dtype\": null,\n",
       "    \"torchscript\": false,\n",
       "    \"typical_p\": 1.0,\n",
       "    \"use_bfloat16\": false\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56c8e4c3-4e3b-459a-a518-d812d3f526a3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import openvino as ov\n",
    "\n",
    "\n",
    "model.config.torchscript = True\n",
    "\n",
    "models_base_folder = Path(\"models\")\n",
    "\n",
    "\n",
    "def cleanup_torchscript_cache():\n",
    "    \"\"\"\n",
    "    Helper for removing cached model representation\n",
    "    \"\"\"\n",
    "    torch._C._jit_clear_class_registry()\n",
    "    torch.jit._recursive.concrete_type_store = torch.jit._recursive.ConcreteTypeStore()\n",
    "    torch.jit._state._clear_class_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf0e035-8cc4-49c8-b8c4-cb3d9f7aea2f",
   "metadata": {},
   "source": [
    "### Convert the vision model\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82351be8-02df-409c-97fa-969f7915203a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maleksandr/openvino_notebooks/lcm-b1/openvino_notebooks/notebooks/248-stable-diffusion-xl/venv/lib/python3.10/site-packages/transformers/models/kosmos2/modeling_kosmos2.py:471: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n",
      "/home/maleksandr/openvino_notebooks/lcm-b1/openvino_notebooks/notebooks/248-stable-diffusion-xl/venv/lib/python3.10/site-packages/transformers/models/kosmos2/modeling_kosmos2.py:511: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision model successfully converted to IR\n"
     ]
    }
   ],
   "source": [
    "vision_model_ir_path = models_base_folder / \"vision_model.xml\"\n",
    "\n",
    "\n",
    "if not vision_model_ir_path.exists():\n",
    "    with torch.no_grad():\n",
    "        ov_model = ov.convert_model(model.vision_model, example_input=inputs[\"pixel_values\"])\n",
    "\n",
    "    ov.save_model(ov_model, vision_model_ir_path)\n",
    "    del ov_model\n",
    "    cleanup_torchscript_cache()\n",
    "    gc.collect()\n",
    "    print(\"Vision model successfully converted to IR\")\n",
    "else:\n",
    "    print(f\"Vision model will be loaded from {vision_model_ir_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11093a02-cb43-48de-a500-7cfb150c2180",
   "metadata": {},
   "source": [
    "### Convert Image To Text Projection model\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c36de38b-5e6d-4a91-8c5e-133ae5945217",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maleksandr/openvino_notebooks/lcm-b1/openvino_notebooks/notebooks/248-stable-diffusion-xl/venv/lib/python3.10/site-packages/torch/jit/_trace.py:160: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  if a.grad is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image To Text Projection model successfully converted to IR\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "image_to_text_projection_model_ir_path = models_base_folder / \"image_to_text_projection_model.xml\"\n",
    "\n",
    "\n",
    "def get_image_embeds(pixel_values):\n",
    "    vision_model_output = model.vision_model(pixel_values)\n",
    "    image_embeds = model.vision_model.model.post_layernorm(vision_model_output[0])\n",
    "    image_embeds = nn.functional.normalize(image_embeds, dim=-1)\n",
    "\n",
    "    return image_embeds\n",
    "\n",
    "\n",
    "if not image_to_text_projection_model_ir_path.exists():\n",
    "    image_embeds = get_image_embeds(inputs[\"pixel_values\"])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ov_model = ov.convert_model(model.image_to_text_projection, example_input=image_embeds)\n",
    "\n",
    "    ov.save_model(ov_model, image_to_text_projection_model_ir_path)\n",
    "    del ov_model\n",
    "    cleanup_torchscript_cache()\n",
    "    gc.collect()\n",
    "    print(\"Image To Text Projection model successfully converted to IR\")\n",
    "else:\n",
    "    print(f\"Image To Text Projection model will be loaded from {image_to_text_projection_model_ir_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84783d6-5a58-4cac-a11e-a33d925f38fc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Convert Text model \n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e124152-1e9c-4622-b461-c49685ced0ec",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maleksandr/openvino_notebooks/lcm-b1/openvino_notebooks/notebooks/248-stable-diffusion-xl/venv/lib/python3.10/site-packages/transformers/models/kosmos2/modeling_kosmos2.py:810: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if max_pos > self.weights.size(0):\n",
      "/home/maleksandr/openvino_notebooks/lcm-b1/openvino_notebooks/notebooks/248-stable-diffusion-xl/venv/lib/python3.10/site-packages/transformers/models/kosmos2/modeling_kosmos2.py:1119: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1:\n",
      "/home/maleksandr/openvino_notebooks/lcm-b1/openvino_notebooks/notebooks/248-stable-diffusion-xl/venv/lib/python3.10/site-packages/transformers/models/kosmos2/modeling_kosmos2.py:926: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (batch_size, 1, seq_length, src_len):\n",
      "/home/maleksandr/openvino_notebooks/lcm-b1/openvino_notebooks/notebooks/248-stable-diffusion-xl/venv/lib/python3.10/site-packages/transformers/models/kosmos2/modeling_kosmos2.py:1211: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Tuple, List\n",
    "import torch.nn.functional as F\n",
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "from transformers.models.kosmos2.modeling_kosmos2 import create_position_ids_from_input_ids\n",
    "\n",
    "\n",
    "first_stage_model_path = models_base_folder / \"cosmos_input_embed.xml\"\n",
    "second_stage_model_path = models_base_folder / \"cosmos_with_past.xml\"\n",
    "\n",
    "\n",
    "def get_image_embeds(pixel_values):\n",
    "    vision_model_output = model.vision_model(pixel_values)\n",
    "    image_embeds = model.vision_model.model.post_layernorm(vision_model_output[0])\n",
    "    image_embeds = nn.functional.normalize(image_embeds, dim=-1)\n",
    "    image_embeds, _ = model.image_to_text_projection(image_embeds)\n",
    "\n",
    "    return image_embeds\n",
    "\n",
    "\n",
    "def flattenize_inputs(inputs):\n",
    "    \"\"\"\n",
    "    Helper function for making nested inputs flattens\n",
    "    \"\"\"\n",
    "    flatten_inputs = []\n",
    "    for input_data in inputs:\n",
    "        if input_data is None:\n",
    "            continue\n",
    "        if isinstance(input_data, (list, tuple)):\n",
    "            flatten_inputs.extend(flattenize_inputs(input_data))\n",
    "        else:\n",
    "            flatten_inputs.append(input_data)\n",
    "    return flatten_inputs\n",
    "\n",
    "\n",
    "def postprocess_converted_model(ov_model, example_input=None, input_names=None, output_names=None, dynamic_shapes=None):\n",
    "    \"\"\"\n",
    "    Helper function for appling postprocessing on converted model with updating input names, shapes and output names\n",
    "    acording to requested specification\n",
    "    \"\"\"\n",
    "\n",
    "    flatten_example_inputs = flattenize_inputs(example_input) if example_input else []\n",
    "    if input_names:\n",
    "        for inp_name, m_input, input_data in zip(input_names, ov_model.inputs, flatten_example_inputs):\n",
    "            m_input.get_tensor().set_names({inp_name})\n",
    "    \n",
    "    if output_names:\n",
    "        for out, out_name in zip(ov_model.outputs, output_names):\n",
    "            out.get_tensor().set_names({out_name})\n",
    "\n",
    "    return ov_model\n",
    "\n",
    "\n",
    "def convert_text_model():\n",
    "    model.text_model.model.config.torchscript = True\n",
    "    model.text_model.config.torchscript = True\n",
    "    image_embeds = get_image_embeds(inputs[\"pixel_values\"])\n",
    "    conv_inputs = {\n",
    "        'input_ids': inputs[\"input_ids\"],\n",
    "        'attention_mask': inputs[\"attention_mask\"],\n",
    "        'image_embeds': image_embeds,\n",
    "        'image_embeds_position_mask': inputs[\"image_embeds_position_mask\"],\n",
    "    }\n",
    "    outs = model.text_model.model(**conv_inputs)\n",
    "    inputs_ = [\"input_ids\", 'attention_mask']\n",
    "    outputs = [\"logits\"]\n",
    "    dynamic_shapes = {\"input_ids\": {1: \"seq_len\"}, \"attention_mask\": {1: \"seq_len\"}, \"position_ids\": {0: \"seq_len\"}}\n",
    "    for idx in range(len(outs[1])):\n",
    "        inputs_.extend([f\"past_key_values.{idx}.key\", f\"past_key_values.{idx}.value\"])\n",
    "        dynamic_shapes[inputs_[-1]] = {2: \"past_sequence + sequence\"}\n",
    "        dynamic_shapes[inputs_[-2]] = {2: \"past_sequence + sequence\"}\n",
    "        outputs.extend([f\"present.{idx}.key\", f\"present.{idx}.value\"])\n",
    "\n",
    "    \n",
    "    \n",
    "    if not first_stage_model_path.exists():\n",
    "        ov_model = ov.convert_model(model.text_model.model, example_input=conv_inputs)\n",
    "        ov_model = postprocess_converted_model(ov_model, output_names=outputs)\n",
    "        ov.save_model(ov_model, first_stage_model_path, compress_to_fp16=False)\n",
    "        del ov_model\n",
    "        cleanup_torchscript_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    if not second_stage_model_path.exists():\n",
    "        position_ids = create_position_ids_from_input_ids(\n",
    "            inputs[\"input_ids\"],\n",
    "            padding_idx=model.text_model.config.pad_token_id,\n",
    "            past_key_values_length=0,\n",
    "        )[:, -1:]\n",
    "\n",
    "        example_input_second_stage = {\n",
    "            \"input_ids\": inputs[\"input_ids\"][:, -1:],\n",
    "            \"attention_mask\": inputs[\"input_ids\"].new_ones(1, inputs[\"input_ids\"].shape[1]+1),\n",
    "            'position_ids': position_ids,\n",
    "            \"past_key_values\": outs[1],\n",
    "        }\n",
    "        \n",
    "        ov_model = ov.convert_model(model.text_model.model, example_input=example_input_second_stage)\n",
    "        ov_model = postprocess_converted_model(\n",
    "            ov_model, \n",
    "            example_input=example_input_second_stage.values(), \n",
    "            input_names=inputs_, \n",
    "            output_names=outputs, \n",
    "            dynamic_shapes=dynamic_shapes\n",
    "        )\n",
    "        ov.save_model(ov_model, second_stage_model_path, compress_to_fp16=False)\n",
    "        del ov_model\n",
    "        cleanup_torchscript_cache()\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "convert_text_model()     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b58a25b-b114-4e52-b2e3-b97ad56e2744",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Select inference device\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Select device that will be used to do models inference using OpenVINO from the dropdown list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31013a5f-4d35-40ae-8e87-52af60f4c76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef143c2bda2480d9a992ecad3da3c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=1, options=('CPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "core = ov.Core()\n",
    "DEVICE = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value='AUTO',\n",
    "    description='Device:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc4dc42a-8e85-4f31-a96f-acd3208923bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class WraperInternalVisionModel:\n",
    "    post_layernorm = model.vision_model.model.post_layernorm\n",
    "    \n",
    "\n",
    "class VisionModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model_ir_path):\n",
    "        super().__init__()\n",
    "        self.model = WraperInternalVisionModel()\n",
    "        self.vision_model = core.compile_model(model_ir_path, DEVICE.value)\n",
    "\n",
    "    def forward(self, pixel_values, **kwargs):\n",
    "        vision_model_output = self.vision_model(pixel_values)[0]\n",
    "\n",
    "        return [torch.from_numpy(vision_model_output)]\n",
    "    \n",
    "\n",
    "class ImageToTextProjectionModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model_ir_path):\n",
    "        super().__init__()\n",
    "        self.image_to_text_projection = core.compile_model(model_ir_path, DEVICE.value)\n",
    "\n",
    "    def forward(self, image_embeds):\n",
    "        output = self.image_to_text_projection(image_embeds)\n",
    "        image_embeds = output[0]\n",
    "        projection_attentions = output[1]\n",
    "        return image_embeds, projection_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21b37d30-3829-4204-81ba-4aac25058401",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_model_ov = VisionModelWrapper(vision_model_ir_path)\n",
    "image_to_text_projection_ov = ImageToTextProjectionModelWrapper(image_to_text_projection_model_ir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ba7fd0fd-03f0-4d0c-a762-d4d9eaffbf73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31f711305af243df916c8b536c343ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Use original first stage:', index=1, options=(True, False), value=False)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "core = ov.Core()\n",
    "USE_ORIGINAL_FIRST_STAGE = widgets.Dropdown(\n",
    "    options=[True, False],\n",
    "    value=False,\n",
    "    description='Use original first stage:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "USE_ORIGINAL_FIRST_STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "22759703-9491-409d-bf5d-fd66aafc4467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation import GenerationConfig, GenerationMixin\n",
    "from transformers.models.kosmos2.modeling_kosmos2 import Kosmos2ForConditionalGenerationModelOutput\n",
    "from transformers import AutoConfig\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class KosmosForCausalLM(GenerationMixin):\n",
    "    def __init__(self, vision_model, image_to_text_projection_model, first_stage_model_path, second_stage_model_path, device):\n",
    "        self.model_stage_1 = core.compile_model(first_stage_model_path, DEVICE.value)\n",
    "        self.model_stage_2 = core.read_model(second_stage_model_path)\n",
    "        self.vision_model = vision_model_ov\n",
    "        self.image_to_text_projection = image_to_text_projection_ov\n",
    "        self.input_names = {\n",
    "            key.get_any_name(): idx for idx, key in enumerate(self.model_stage_2.inputs)\n",
    "        }\n",
    "        self.output_names = {\n",
    "            key.get_any_name(): idx for idx, key in enumerate(self.model_stage_2.outputs)\n",
    "        }\n",
    "        self.key_value_input_names = [\n",
    "            key for key in self.input_names if \"key_values\" in key\n",
    "        ]\n",
    "        self.key_value_output_names = [\n",
    "            key for key in self.output_names if \"present\" in key\n",
    "        ]\n",
    "        self.model_stage_2 = core.compile_model(self.model_stage_2, DEVICE.value)\n",
    "\n",
    "        self.request = self.model_stage_2.create_infer_request()\n",
    "        self.config = model.config\n",
    "        self.generation_config = GenerationConfig.from_model_config(model.config)\n",
    "        self.main_input_name = \"input_ids\"\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.num_pkv = 2\n",
    "        self.lm_head = nn.Linear(in_features=model.text_model.config.embed_dim, out_features=model.text_model.config.vocab_size, bias=False)\n",
    "\n",
    "\n",
    "    def get_input_embeddings(self) -> nn.Module:\n",
    "        return self.model.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.model.embed_tokens = value\n",
    "\n",
    "    def get_output_embeddings(self) -> nn.Module:\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def can_generate(self):\n",
    "        \"\"\"Returns True to validate the check that the model using `GenerationMixin.generate()` can indeed generate.\"\"\"\n",
    "        return True\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        image_embeds: Optional[torch.Tensor] = None,\n",
    "        image_embeds_position_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        return self.forward(\n",
    "            input_ids, attention_mask, image_embeds, image_embeds_position_mask, position_ids, past_key_values\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        \n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        image_embeds: Optional[torch.Tensor] = None,\n",
    "        image_embeds_position_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        \n",
    "        **kwargs\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n",
    "            `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n",
    "            ignored (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if past_key_values is None:\n",
    "            \n",
    "            outs = self.model_stage_1(\n",
    "                {\n",
    "                    'input_ids': input_ids,\n",
    "                    'attention_mask': attention_mask,\n",
    "                    'image_embeds': image_embeds,\n",
    "                    'image_embeds_position_mask': image_embeds_position_mask,\n",
    "                }\n",
    "            )\n",
    "            #outs = self.model_stage_1([input_ids, attention_mask, image_embeds, image_embeds_position_mask])\n",
    "            \n",
    "            lm_logits = model.text_model.lm_head(torch.from_numpy(outs[0]))\n",
    "            outs2 = model.text_model.model(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                image_embeds=torch.from_numpy(image_embeds),\n",
    "                image_embeds_position_mask=image_embeds_position_mask,\n",
    "            )\n",
    "\n",
    "            pkv = list(outs.values())[1:]\n",
    "            pkv = tuple(pkv[i : i + 2] for i in range(0, len(pkv), 2))\n",
    "\n",
    "            pkv2 = tuple((i.numpy(), j.numpy()) for i, j in outs2[1])\n",
    "            if USE_ORIGINAL_FIRST_STAGE.value:\n",
    "                return Kosmos2ForConditionalGenerationModelOutput(logits=lm_logits, past_key_values=pkv2)\n",
    "            else:\n",
    "                return Kosmos2ForConditionalGenerationModelOutput(logits=lm_logits, past_key_values=pkv)\n",
    "        \n",
    "        if past_key_values is not None:\n",
    "            past_key_values = tuple(\n",
    "                past_key_value\n",
    "                for pkv_per_layer in past_key_values\n",
    "                for past_key_value in pkv_per_layer\n",
    "            )\n",
    "            inputs_ = {\n",
    "                \"input_ids\": input_ids[:, -1].unsqueeze(-1),\n",
    "                \"attention_mask\": attention_mask,\n",
    "                'position_ids': position_ids\n",
    "            }\n",
    "            inputs_.update(dict(zip(self.key_value_input_names, past_key_values)))\n",
    "\n",
    "        # Run inference\n",
    "        self.request.start_async(inputs_, share_inputs=True)\n",
    "        self.request.wait()\n",
    "\n",
    "        logits = torch.from_numpy(self.request.get_tensor(\"logits\").data)\n",
    "        logits = model.text_model.lm_head(logits)\n",
    "\n",
    "        # Tuple of length equal to : number of layer * number of past_key_value per decoder layer (2 corresponds to the self-attention layer)\n",
    "        past_key_values = tuple(\n",
    "            self.request.get_tensor(key).data for key in self.key_value_output_names\n",
    "        )\n",
    "        # Tuple of tuple of length `n_layers`, with each tuple of length equal to 2 (k/v of self-attention)\n",
    "\n",
    "        past_key_values = tuple(\n",
    "            past_key_values[i : i + self.num_pkv]\n",
    "            for i in range(0, len(past_key_values), self.num_pkv)\n",
    "        )\n",
    "        \n",
    "        return Kosmos2ForConditionalGenerationModelOutput(logits=logits, past_key_values=past_key_values)\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self,\n",
    "        input_ids,\n",
    "        image_embeds=None,\n",
    "        image_embeds_position_mask=None,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        use_cache=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        input_shape = input_ids.shape\n",
    "        # # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n",
    "        if attention_mask is None:\n",
    "            attention_mask = input_ids.new_ones(input_shape)\n",
    "\n",
    "        position_ids = None\n",
    "\n",
    "        # cut input_ids if past_key_values is used\n",
    "        if past_key_values is not None:\n",
    "            position_ids = create_position_ids_from_input_ids(\n",
    "                input_ids,\n",
    "                padding_idx=model.text_model.config.pad_token_id,\n",
    "                past_key_values_length=0,\n",
    "            )[:, -1:]\n",
    "\n",
    "            input_ids = input_ids[:, -1:]\n",
    "            # the image info. is already encoded into the past keys/values\n",
    "            image_embeds = None\n",
    "            image_embeds_position_mask = None\n",
    "        elif image_embeds_position_mask is not None:\n",
    "            # appending `False` to `image_embeds_position_mask` (because `input_ids` grows during generation)\n",
    "            batch_size, seq_len = input_ids.size()\n",
    "            mask_len = image_embeds_position_mask.size()[-1]\n",
    "            image_embeds_position_mask = torch.cat(\n",
    "                (\n",
    "                    image_embeds_position_mask,\n",
    "                    torch.zeros(size=(batch_size, seq_len - mask_len), dtype=torch.bool, device=input_ids.device),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "        # return {\n",
    "        #     \"input_ids\": input_ids,\n",
    "        #     \"image_embeds\": image_embeds,\n",
    "        #     \"image_embeds_position_mask\": image_embeds_position_mask,\n",
    "        #     \"past_key_values\": past_key_values,\n",
    "        #     \"attention_mask\": attention_mask,\n",
    "        #     \"position_ids\": position_ids,\n",
    "        #     #\"use_cache\": use_cache,\n",
    "        #}\n",
    "        # if attention_mask is None:\n",
    "        #     attention_mask = input_ids.new_ones(input_shape)\n",
    "            \n",
    "        # past_len = 0\n",
    "        # if past_key_values is not None:\n",
    "        #     input_ids = input_ids[:, -1].unsqueeze(-1)\n",
    "        #     # past_len = past_key_values[-1][-1].shape[-2]\n",
    "        #     image_embeds = None\n",
    "        #     image_embeds_position_mask = None\n",
    "        # elif image_embeds_position_mask is not None:\n",
    "        #     # appending `False` to `image_embeds_position_mask` (because `input_ids` grows during generation)\n",
    "        #     batch_size, seq_len = input_ids.size()\n",
    "        #     mask_len = image_embeds_position_mask.size()[-1]\n",
    "        #     image_embeds_position_mask = torch.cat(\n",
    "        #         (\n",
    "        #             image_embeds_position_mask,\n",
    "        #             torch.zeros(size=(batch_size, seq_len - mask_len), dtype=torch.bool, device=input_ids.device),\n",
    "        #         ),\n",
    "        #         dim=1,\n",
    "        #     )\n",
    "        # attention_mask = kwargs.get(\n",
    "        #     \"attention_mask\",\n",
    "        #     torch.ones(input_ids.shape[0], input_ids.shape[1] + past_len),\n",
    "        # )\n",
    "        # if not kwargs.get(\"use_cache\", True):\n",
    "        #     raise NotImplementedError(\"MPT with prefix_lm=True does not support use_cache=False.\")\n",
    "        # else:\n",
    "        #     prefix_mask = None\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"image_embeds\": image_embeds,\n",
    "            \"image_embeds_position_mask\": image_embeds_position_mask,\n",
    "            'position_ids': position_ids,\n",
    "            \"past_key_values\": past_key_values,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    # Copied from transformers.models.umt5.modeling_umt5.UMT5ForConditionalGeneration._reorder_cache\n",
    "    def _reorder_cache(past_key_values, beam_idx):\n",
    "        reordered_past = ()\n",
    "        for layer_past in past_key_values:\n",
    "            reordered_past += (\n",
    "                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n",
    "            )\n",
    "        return reordered_past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0aef4494-50f7-4838-b3a4-9035c297d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_model = KosmosForCausalLM(vision_model_ov, image_to_text_projection_ov, first_stage_model_path, second_stage_model_path, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "83ce9245-082c-459f-965f-8b42410ff01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_model_output = vision_model_ov(inputs[\"pixel_values\"])\n",
    "image_embeds = model.vision_model.model.post_layernorm(vision_model_output[0])\n",
    "# normalized features\n",
    "image_embeds = nn.functional.normalize(image_embeds, dim=-1)\n",
    "image_embeds, projection_attentions = image_to_text_projection_ov(image_embeds.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d5382e1e-b076-4bdd-83e2-1dea22845f48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generated_ids_ = ov_model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    image_embeds=image_embeds,\n",
    "    image_embeds_position_mask=inputs[\"image_embeds_position_mask\"],\n",
    "    #use_cache=True,\n",
    "    max_new_tokens=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a2f90bab-0345-4417-a314-d5a57b7b6cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 64003,     4,     5,     6,     7,     8,     9,    10,    11,\n",
       "            12,    13,    14,    15,    16,    17,    18,    19,    20,    21,\n",
       "            22,    23,    24,    25,    26,    27,    28,    29,    30,    31,\n",
       "            32,    33,    34,    35,    36,    37,    38,    39,    40,    41,\n",
       "            42,    43,    44,    45,    46,    47,    48,    49,    50,    51,\n",
       "            52,    53,    54,    55,    56,    57,    58,    59,    60,    61,\n",
       "            62,    63,    64,    65,    66,    67, 64004, 64012,   712,  1648,\n",
       "             9,    10,   242,     8,    10,   460,    12,    10,   370,     6,\n",
       "            23,    10,   242,  1369,    10,  1402,     6,     8,    10,   242,\n",
       "            23,    10,  1402,    12,    10,   373,     6,    23,     5,   460,\n",
       "            12,     5,   373,     4,    24,   242,    23,     5,  1402,    17,\n",
       "          5229,    22,    26,     5,   460,     6,     8,     5,   460,    17,\n",
       "           383,    26,     5,   242,    23,    36,  1402,     4,    24,   460,\n",
       "            17,   490,     6,    34,   119,    15,    78,    40,   137,     7,\n",
       "            89,    38,   124,    24,   242,    17,   490,    34,   119,   123,\n",
       "            35,    15,   108,    40,   137,   124,    24,   460,   202,     6,\n",
       "            34,   823,    15,   108,   137,     7,  3346,    70,   124,    24,\n",
       "          1402,    17,  2362,    26,     5,   373,     6,     8,    63,    17,\n",
       "           490,     7,     5,   242,     6,    34,  4621,    15,    28,  3346,\n",
       "            70,  1119,    24,   242,   202,     6, 23732,   119,    15]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "947382aa-4cb8-462c-be31-e85eda17dfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>. the, to and of as in I that' for is was- on’ it with The as at bet he have from by are \" you his “ this said not has an ( but had we her they will my or were their): up about out who one all been she can more would It</image><grounding> An image of a man and a woman in a room, with a man holding a gun, and a man with a gun in a car, with the woman in the car. The man with the gun is pointing it at the woman, and the woman is looking at the man with his gun. The woman is saying, \"I'm not going to do this.\" The man is saying \"I know you're not going.\" The woman says, \"You're going to shoot me.\" The gun is pointed at the car, and she is saying to the man, \"Don't shoot me!\" The man says, \"...I'\n"
     ]
    }
   ],
   "source": [
    "generated_text = processor.batch_decode(generated_ids_, skip_special_tokens=True)[0]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e844208b-7c38-46ea-943b-efe382149e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<grounding> An image of a man and a woman in a room, with a man holding a gun, and a man with a gun in a car, with the woman in the car. The man with the gun is pointing it at the woman, and the woman is looking at the man with his gun. The woman is saying, \"I'm not going to do this.\" The man is saying \"I know you're not going.\" The woman says, \"You're going to shoot me.\" The gun is pointed at the car, and she is saying to the man, \"Don't shoot me!\" The man says, \"...I'\n",
      "An image of a man and a woman in a room, with a man holding a gun, and a man with a gun in a car, with the woman in the car. The man with the gun is pointing it at the woman, and the woman is looking at the man with his gun. The woman is saying, \"I'm not going to do this.\" The man is saying \"I know you're not going.\" The woman says, \"You're going to shoot me.\" The gun is pointed at the car, and she is saying to the man, \"Don't shoot me!\" The man says, \"...I'\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Specify `cleanup_and_extract=False` in order to see the raw model generation.\n",
    "processed_text = processor.post_process_generation(generated_text, cleanup_and_extract=False)\n",
    "\n",
    "print(processed_text)\n",
    "# `<grounding> An image of<phrase> a snowman</phrase><object><patch_index_0044><patch_index_0863></object> warming himself by<phrase> a fire</phrase><object><patch_index_0005><patch_index_0911></object>.`\n",
    "\n",
    "# By default, the generated  text is cleanup and the entities are extracted.\n",
    "processed_text, entities = processor.post_process_generation(generated_text)\n",
    "\n",
    "print(processed_text)\n",
    "# `An image of a snowman warming himself by a fire.`\n",
    "\n",
    "print(entities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
