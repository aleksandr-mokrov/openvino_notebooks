{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Kosmos-2: Multimodal Large Language Model and OpenVINO\n",
    "\n",
    "[KOSMOS-2](https://github.com/microsoft/unilm/tree/master/kosmos-2) is a multimodal large language model (MLLM) that has new capabilities of multimodal grounding and referring. KOSMOS-2 can understand multimodal input, follow instructions, \n",
    "perceive object descriptions (e.g., bounding boxes), and ground language to the visual world.\n",
    "\n",
    "Multimodal Large Language Models (MLLMs) have successfully played a role as a general-purpose interface across a wide range of tasks, such as language, vision, and vision-language tasks. MLLMs can perceive general modalities, including texts, images, and audio, and generate responses using free-form texts under zero-shot and few-shot settings. \n",
    "\n",
    "[In this work](https://arxiv.org/abs/2306.14824), authors unlock the grounding capability for multimodal large language models. Grounding capability can provide a more convenient and efficient human-AI interaction for vision-language tasks. It enables the user to point to the object or region in the image directly rather than input detailed text descriptions to refer to it, the model can understand that image region with its spatial locations. Grounding capability also enables the model to respond with visual answers (i.e., bounding boxes), which can support more vision-language tasks such as referring expression comprehension. Visual answers are more accurate and resolve the coreference ambiguity compared with text-only responses. In addition, grounding capability can link noun phrases and referring expressions in the generated free-form text response to the image regions, providing more accurate, informational, and comprehensive answers.\n",
    "\n",
    "\n",
    "![image](https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/annotated_snowman.jpg)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cd41ed71f1535bf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Table of contents:\n",
    "- [Install requirements](#Install-requirements)\n",
    "- [Original model inference](#Original-model-inference)\n",
    "- [Convert models to OpenVINO Intermediate representation (IR) format](#Convert-models-to-OpenVINO-Intermediate-representation-(IR)-format)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79e550dbd4bb45b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install requirements\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c5ef7038b251221"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%pip install -q \"transformers>=4.33\" Pillow \"torch==1.13.0\" \"torchvision==0.14.0\"\n",
    "%pip install -q \"openvino>=2023.2.0\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "609fe18134d39334"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Original model inference"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66748dfe4f063963"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "\n",
    "model = AutoModelForVision2Seq.from_pretrained(\"microsoft/kosmos-2-patch14-224\")\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/kosmos-2-patch14-224\")\n",
    "\n",
    "prompt = \"<grounding>An image of\"\n",
    "\n",
    "url = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.png\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# The original Kosmos-2 demo saves the image first then reload it. For some images, this will give slightly different image input and change the generation outputs.\n",
    "image.save(\"new_image.jpg\")\n",
    "image = Image.open(\"new_image.jpg\")\n",
    "\n",
    "inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    pixel_values=inputs[\"pixel_values\"],\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    image_embeds=None,\n",
    "    image_embeds_position_mask=inputs[\"image_embeds_position_mask\"],\n",
    "    use_cache=True,\n",
    "    max_new_tokens=128,\n",
    ")\n",
    "print(generated_ids)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_text)\n",
    "# Specify `cleanup_and_extract=False` in order to see the raw model generation.\n",
    "processed_text = processor.post_process_generation(generated_text, cleanup_and_extract=False)\n",
    "\n",
    "print(processed_text)\n",
    "# `<grounding> An image of<phrase> a snowman</phrase><object><patch_index_0044><patch_index_0863></object> warming himself by<phrase> a fire</phrase><object><patch_index_0005><patch_index_0911></object>.`\n",
    "\n",
    "# By default, the generated  text is cleanup and the entities are extracted.\n",
    "processed_text, entities = processor.post_process_generation(generated_text)\n",
    "\n",
    "print(processed_text)\n",
    "# `An image of a snowman warming himself by a fire.`\n",
    "\n",
    "print(entities)\n",
    "# `[('a snowman', (12, 21), [(0.390625, 0.046875, 0.984375, 0.828125)]), ('a fire', (41, 47), [(0.171875, 0.015625, 0.484375, 0.890625)])]`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3d0f6303cb25734"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert models to OpenVINO Intermediate representation (IR) format\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "862a652e94ffa494"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import openvino as ov\n",
    "\n",
    "\n",
    "model.config.torchscript = True\n",
    "\n",
    "models_base_folder = Path(\"models\")\n",
    "\n",
    "\n",
    "def cleanup_torchscript_cache():\n",
    "    \"\"\"\n",
    "    Helper for removing cached model representation\n",
    "    \"\"\"\n",
    "    torch._C._jit_clear_class_registry()\n",
    "    torch.jit._recursive.concrete_type_store = torch.jit._recursive.ConcreteTypeStore()\n",
    "    torch.jit._state._clear_class_state()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e43b67ef9881f6c7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convert the vision model\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2df10eedb8daf1ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vision_model_ir_path = models_base_folder / \"vision_model.xml\"\n",
    "\n",
    "\n",
    "if not vision_model_ir_path.exists():\n",
    "    with torch.no_grad():\n",
    "        ov_model = ov.convert_model(model.vision_model, example_input=inputs[\"pixel_values\"])\n",
    "\n",
    "    ov.save_model(ov_model, vision_model_ir_path)\n",
    "    del ov_model\n",
    "    cleanup_torchscript_cache()\n",
    "    gc.collect()\n",
    "    print(\"Vision model successfully converted to IR\")\n",
    "else:\n",
    "    print(f\"Vision model will be loaded from {vision_model_ir_path}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e693c9474dccf29c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convert Image To Text Projection model\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18eeca7bf0ba631b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "image_to_text_projection_model_ir_path = models_base_folder / \"image_to_text_projection_model.xml\"\n",
    "\n",
    "\n",
    "if not image_to_text_projection_model_ir_path.exists():\n",
    "    vision_model_output = model.vision_model(inputs[\"pixel_values\"])\n",
    "    image_embeds = model.vision_model.model.post_layernorm(vision_model_output[0])\n",
    "    image_embeds = nn.functional.normalize(image_embeds, dim=-1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ov_model = ov.convert_model(model.image_to_text_projection, example_input=image_embeds)\n",
    "\n",
    "    ov.save_model(ov_model, image_to_text_projection_model_ir_path)\n",
    "    del ov_model\n",
    "    cleanup_torchscript_cache()\n",
    "    gc.collect()\n",
    "    print(\"Image To Text Projection model successfully converted to IR\")\n",
    "else:\n",
    "    print(f\"Image To Text Projection model will be loaded from {image_to_text_projection_model_ir_path}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea50c3efd9fdbb24"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convert Text model \n",
    "[back to top ⬆️](#Table-of-contents:)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2fd9836054b84bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, List\n",
    "import torch.nn.functional as F\n",
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "\n",
    "\n",
    "first_stage_model_path = models_base_folder / \"cosmos_input_embed.xml\"\n",
    "second_stage_model_path = models_base_folder / \"cosmos_with_past.xml\"\n",
    "\n",
    "\n",
    "\n",
    "class ModelWrapper(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Model wrapper class for export for spliting original forward logic on preparing multimodal data and inference using it.\n",
    "    That allows us to sperate image encoder and token embeddings model from general flow. \n",
    "    \"\"\"\n",
    "    def __init__(self, model, config):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.lm_head = nn.Linear(in_features=config.embed_dim, out_features=config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        image_embeds: Optional[torch.Tensor] = None,\n",
    "        image_embeds_position_mask: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "        outputs = self.model.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            image_embeds=image_embeds,\n",
    "            image_embeds_position_mask=image_embeds_position_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=False,\n",
    "            #max_new_tokens=128,\n",
    "            # output_attentions=False,\n",
    "            # output_hidden_states=False,\n",
    "            #return_dict=True,\n",
    "        )\n",
    "        print(self.lm_head)\n",
    "        print(f'{outputs[0].shape=}') \n",
    "        lm_logits = self.lm_head(outputs[0])\n",
    "        #return (lm_logits, tuple(outputs.past_key_values))\n",
    "        return (lm_logits,) + outputs[1:]\n",
    "\n",
    "\n",
    "def convert_text_model():\n",
    "    conv_inputs = {\n",
    "        'input_ids': inputs[\"input_ids\"],\n",
    "        'attention_mask': inputs[\"attention_mask\"],\n",
    "        'image_embeds': torch.zeros((1, 64, 2048)),\n",
    "        'image_embeds_position_mask': inputs[\"image_embeds_position_mask\"]\n",
    "    }\n",
    "    # example_input_first_stage = {\n",
    "    #     \"input_ids\": torch.ones((1, 71), dtype=torch.long),\n",
    "    #     \"attention_mask\": torch.ones((1, 71), dtype=torch.long),\n",
    "    # }\n",
    "    model.text_model.model.config.torchscript = True\n",
    "    model.text_model.config.torchscript = True\n",
    "    model_wrap = ModelWrapper(model.text_model, model.text_model.config)\n",
    "    if not first_stage_model_path.exists():\n",
    "        # ov_model = ov.convert_model(model_wrap, example_input=conv_inputs)\n",
    "        ov_model = ov.convert_model(model.text_model, example_input=conv_inputs)\n",
    "        ov.save_model(ov_model, first_stage_model_path)\n",
    "        del ov_model\n",
    "        cleanup_torchscript_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    if not second_stage_model_path.exists():\n",
    "        outs = model.text_model(**conv_inputs)\n",
    "        example_input_second_stage = {\n",
    "            'input_ids': inputs[\"input_ids\"],\n",
    "            'attention_mask': inputs[\"attention_mask\"],\n",
    "            \"past_key_values\": outs[1],\n",
    "            \"image_embeds\": image_embeds,\n",
    "            \"image_embeds_position_mask\": inputs[\"image_embeds_position_mask\"],\n",
    "        }\n",
    "        ov_model = ov.convert_model(model_wrap, example_input=example_input_second_stage)\n",
    "        ov.save_model(ov_model, second_stage_model_path)\n",
    "        del ov_model\n",
    "        cleanup_torchscript_cache()\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "convert_text_model()     "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "585e5c5e84e82ad8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Select inference device\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Select device that will be used to do models inference using OpenVINO from the dropdown list:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4756dc2a40d78ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "core = ov.Core()\n",
    "DEVICE = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value='AUTO',\n",
    "    description='Device:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "DEVICE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2311135fdcff7b8f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class WraperInternalVisionModel:\n",
    "    post_layernorm = model.vision_model.model.post_layernorm\n",
    "    \n",
    "\n",
    "class VisionModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model_ir_path):\n",
    "        super().__init__()\n",
    "        self.model = WraperInternalVisionModel()\n",
    "        self.vision_model = core.compile_model(model_ir_path, DEVICE.value)\n",
    "\n",
    "    def forward(self, pixel_values, **kwargs):\n",
    "        vision_model_output = self.vision_model(pixel_values)[0]\n",
    "        # image_embeds = post_layernorm(torch.from_numpy(vision_model_output[0]))\n",
    "        # image_embeds = nn.functional.normalize(image_embeds, dim=-1)\n",
    "        print(vision_model_output.shape)\n",
    "        print(torch.from_numpy(vision_model_output).shape)\n",
    "        \n",
    "        return [torch.from_numpy(vision_model_output)]\n",
    "        \n",
    "    \n",
    "\n",
    "class ImageToTextProjectionModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model_ir_path):\n",
    "        super().__init__()\n",
    "        self.image_to_text_projection = core.compile_model(model_ir_path, DEVICE.value)\n",
    "\n",
    "    def forward(self, image_embeds):\n",
    "        print(image_embeds)\n",
    "        print(image_embeds.shape)\n",
    "        output = self.image_to_text_projection(image_embeds.detach().numpy())\n",
    "        image_embeds = output[0]\n",
    "        projection_attentions = output[1]\n",
    "        return image_embeds, projection_attentions\n",
    "\n",
    "\n",
    "class TextModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model_stage_1_ir_path, model_stage_2_ir_path):\n",
    "        super().__init__()\n",
    "        self.model_stage_1 = core.compile_model(model_stage_1_ir_path, DEVICE.value)\n",
    "        self.model_stage_2 = core.compile_model(model_stage_2_ir_path, DEVICE.value)\n",
    "        self.input_names = {\n",
    "            key.get_any_name(): idx for idx, key in enumerate(self.model_stage_2.inputs)\n",
    "        }\n",
    "        self.output_names = {\n",
    "            key.get_any_name(): idx for idx, key in enumerate(self.model_stage_2.outputs)\n",
    "        }\n",
    "        self.key_value_input_names = [\n",
    "            key for key in self.input_names if \"key_values\" in key\n",
    "        ]\n",
    "        self.key_value_output_names = [\n",
    "            key for key in self.output_names if \"present\" in key\n",
    "        ]\n",
    "        self.request = self.model_stage_2.create_infer_request()\n",
    "\n",
    "    def generate(self, input_ids, attention_mask, image_embeds, image_embeds_position_mask, **kwargs):\n",
    "        past_key_values = kwargs.get(\"past_key_values\")\n",
    "        print(f'past_key_values=')\n",
    "        if past_key_values is None:\n",
    "            outs = self.model_stage_1([input_ids, attention_mask])\n",
    "            logits = outs[0]\n",
    "            print(list(outs.keys())[1:])\n",
    "            # pkv = list(outs.values())[1:]\n",
    "            # pkv = tuple(pkv[i : i + 2] for i in range(0, len(pkv), 2))\n",
    "            return logits, outs[0]\n",
    "        else:\n",
    "            print('stage_two')\n",
    "            # past_key_values = tuple(\n",
    "            #     past_key_value\n",
    "            #     for pkv_per_layer in past_key_values\n",
    "            #     for past_key_value in pkv_per_layer\n",
    "            # )\n",
    "            # # Add the past_key_values to the decoder inputs\n",
    "            # inputs = dict(zip(self.key_value_input_names, past_key_values))\n",
    "            # inputs[\"input_ids\"] = input_ids\n",
    "            # # attention_mask = torch.ones(\n",
    "            # #     (input_ids.shape[0], past_key_values[-1][-1].shape[-2] + 1),\n",
    "            # #     dtype=input_ids.dtype,\n",
    "            # # )\n",
    "\n",
    "\n",
    "            # inputs[\"attention_mask\"] = attention_mask\n",
    "            # inputs[\"image_embeds_position_mask\"] = image_embeds_position_mask\n",
    "            # self.request.start_async(inputs, share_inputs=True)\n",
    "            # self.request.wait()\n",
    "            inputs = {\n",
    "                \"input_ids\": input_ids, \n",
    "                \"past_key_values\": past_key_values, \n",
    "                \"attention_mask\": attention_mask, \n",
    "                \"image_embeds\": image_embeds, \n",
    "                \"image_embeds_position_mask\": image_embeds_position_mask\n",
    "            }\n",
    "            outs = self.model_stage_2(inputs)[0]\n",
    "        return outs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ace90962945f2c4a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vision_model_ov = VisionModelWrapper(vision_model_ir_path)\n",
    "image_to_text_projection_ov = ImageToTextProjectionModelWrapper(image_to_text_projection_model_ir_path)\n",
    "text_model_ov = TextModelWrapper(first_stage_model_path, second_stage_model_path)\n",
    "\n",
    "\n",
    "model.vision_model = vision_model_ov\n",
    "model.image_to_text_projection = image_to_text_projection_ov\n",
    "model.text_model = text_model_ov"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2304933242cec43"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_ids, past_key_values = model.generate(\n",
    "    pixel_values=inputs[\"pixel_values\"],\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    image_embeds=None,\n",
    "    image_embeds_position_mask=inputs[\"image_embeds_position_mask\"],\n",
    "    use_cache=True,\n",
    "    max_new_tokens=128,\n",
    ")\n",
    "generated_ids = model.generate(\n",
    "    pixel_values=inputs[\"pixel_values\"],\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    past_key_values=past_key_values,\n",
    "    image_embeds=None,\n",
    "    image_embeds_position_mask=inputs[\"image_embeds_position_mask\"],\n",
    "    use_cache=True,\n",
    "    max_new_tokens=128,\n",
    ")\n",
    "\n",
    "print(generated_ids)\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_text)\n",
    "# Specify `cleanup_and_extract=False` in order to see the raw model generation.\n",
    "processed_text = processor.post_process_generation(generated_text, cleanup_and_extract=False)\n",
    "\n",
    "print(processed_text)\n",
    "# `<grounding> An image of<phrase> a snowman</phrase><object><patch_index_0044><patch_index_0863></object> warming himself by<phrase> a fire</phrase><object><patch_index_0005><patch_index_0911></object>.`\n",
    "\n",
    "# By default, the generated  text is cleanup and the entities are extracted.\n",
    "processed_text, entities = processor.post_process_generation(generated_text)\n",
    "\n",
    "print(processed_text)\n",
    "# `An image of a snowman warming himself by a fire.`\n",
    "\n",
    "print(entities)\n",
    "# `[('a snowman', (12, 21), [(0.390625, 0.046875, 0.984375, 0.828125)]), ('a fire', (41, 47), [(0.171875, 0.015625, 0.484375, 0.890625)])]`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b30f5074e7bffa96"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e96958a636c2f85b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
