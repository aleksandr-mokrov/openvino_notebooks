{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MMS: Scaling Speech Technology to 1000+ languages with OpenVINOâ„¢\n",
    "\n",
    "The Massively Multilingual Speech (MMS) project expands speech technology from about 100 languages to over 1,000 by building a single multilingual speech recognition model supporting over 1,100 languages (more than 10 times as many as before), language identification models able to identify over 4,000 languages (40 times more than before), pretrained models supporting over 1,400 languages, and text-to-speech models for over 1,100 languages.\n",
    "The MMS model was proposed in [Scaling Speech Technology to 1,000+ Languages](https://arxiv.org/abs/2305.13516).  The models and code are originally released [here](https://github.com/facebookresearch/fairseq/tree/main/examples/mms).\n",
    "There are the different models open sourced in the MMS project: Automatic Speech Recognition (ASR), Language Identification (LID) and Speech Synthesis (TTS).  In this example we are considering ASR and LID."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c87087c91122e3f8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"0\"></a>\n",
    "### Table of contents:\n",
    "- [Install prerequisites](#1)\n",
    "- [Automatic Speech Recognition (ASR)](#2)\n",
    "  - [Download pretrained model and processor](#3)\n",
    "  - [Prepare an example audio](#4)\n",
    "  - [Make inference with the original model](#5)\n",
    "  - [Convert to OpenVINO IR model and make inference](#6)\n",
    "- [Language Identification (LID)](#7)\n",
    "  - [Download pretrained model and processor](#8)\n",
    "  - [Make inference with the original model](#9)\n",
    "  - [Convert to OpenVINO IR model and make inference](#10)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa80166a11177e7a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name='1'></a>\n",
    "## Install prerequisites"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90c7a208b1fa497b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -q --upgrade pip \n",
    "!pip install -q datasets transformers accelerate \"openvino==2023.1.0.dev20230811\" torch soundfile"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T22:30:02.125023400Z",
     "start_time": "2023-08-21T22:30:02.120587200Z"
    }
   },
   "id": "bc1a0304b8213aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "import openvino"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T22:30:02.129288100Z",
     "start_time": "2023-08-21T22:30:02.126575400Z"
    }
   },
   "id": "dbac6fae86122d9b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name='2'></a>\n",
    "## Automatic Speech Recognition (ASR)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e010ed384d1e8ee7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name='3'></a>\n",
    "### Download pretrained model and processor\n",
    "Download pretrained model and processor. By default, MMS loads adapter weights for English. If you want to load adapter weights of another language make sure to specify `target_lang=<your-chosen-target-lang>` as well as `ignore_mismatched_sizes=True`. The `ignore_mismatched_sizes=True` keyword has to be passed to allow the language model head to be resized according to the vocabulary of the specified language. Similarly, the processor should be loaded with the same target language. \n",
    "It is also possible to change the supported language later."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe4536f63fe7e612"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC, AutoProcessor\n",
    "model_id = \"facebook/mms-1b-all\"\n",
    "\n",
    "asr_processor = AutoProcessor.from_pretrained(model_id)\n",
    "asr_model = Wav2Vec2ForCTC.from_pretrained(model_id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T22:30:02.150399500Z",
     "start_time": "2023-08-21T22:30:02.135655500Z"
    }
   },
   "id": "2b104f835667fb9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can look at all supported languages:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5896f5fd08f62071"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "asr_processor.tokenizer.vocab.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T22:30:02.150399500Z",
     "start_time": "2023-08-21T22:30:02.142905300Z"
    }
   },
   "id": "6b62341511f98ceb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name='4'></a>\n",
    "### Prepare an example audio\n",
    "Read an audio file and process the audio data. Make sure that the audio data is sampled to 16000 kHz.\n",
    "For this example we will use [a streamable version of the Multilingual LibriSpeech (MLS) dataset](https://huggingface.co/datasets/multilingual_librispeech). It support contains example on 7 languages: `'german', 'dutch', 'french', 'spanish', 'italian', 'portuguese', 'polish'`.\n",
    "Let's use `'german'`. Specify `streaming=True` to not download the entire dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d81ab16ec40431a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "mls = load_dataset(\"facebook/multilingual_librispeech\", \"german\", split=\"test\", streaming=True)\n",
    "mls = iter(mls)  # make it itarable\n",
    "\n",
    "example = next(mls)  # get one example"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T22:30:02.246545Z",
     "start_time": "2023-08-21T22:30:02.153223100Z"
    }
   },
   "id": "3e3b30952e08ee76"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Example has a dictionary structure. It contains an audio data and a text transcription."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68f9bb826d9a36dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(example)  # look at structure"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-21T22:30:02.254112200Z",
     "start_time": "2023-08-21T22:30:02.162197200Z"
    }
   },
   "id": "5f96bfecfd4bab51"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Switch out the language adapters by calling the `load_adapter()` function for the model and `set_target_lang()` for the tokenizer. Pass the target language as an input - `\"deu\"` for German."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "541c53d1c740d668"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "asr_processor.tokenizer.set_target_lang(\"deu\")\n",
    "asr_model.load_adapter(\"deu\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15f9f4e31170b3fa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name='5'></a>\n",
    "### Make inference with the original model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de68b1eac717cc26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs = asr_processor(example['audio']['array'], sampling_rate=16_000, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = asr_model(**inputs).logits\n",
    "\n",
    "ids = torch.argmax(outputs, dim=-1)[0]\n",
    "transcription = asr_processor.decode(ids)\n",
    "print(transcription)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4463e26404e16195"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name='6'></a>\n",
    "### Convert to OpenVINO IR model and make inference\n",
    "Convert to OpenVINO IR model format with `openvino.convert_model` function directly. Use `openvino.save_model` function to serialize the result of conversion."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bda2f58170bfa2f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 30480\n",
    "\n",
    "input_values = torch.zeros([1, MAX_SEQ_LENGTH], dtype=torch.float)\n",
    "attention_mask = torch.ones([1, MAX_SEQ_LENGTH], dtype=torch.int32)\n",
    "asr_model_xml_path = Path('models/ov_asr_model.xml')\n",
    "\n",
    "if not asr_model_xml_path.exists():\n",
    "    asr_model_xml_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    converted_model = openvino.convert_model(asr_model, example_input={'input_values': input_values})\n",
    "    openvino.save_model(converted_model, asr_model_xml_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f47ccb726cdb505d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compile model. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "813a9bce607b590c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "core = openvino.Core()\n",
    "\n",
    "compiled_asr_model = core.compile_model(asr_model_xml_path, device_name='CPU')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71f6e2b3495088b7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make inference."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4fb2cd466365800"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs = asr_processor(example['audio']['array'], sampling_rate=16_000, return_tensors=\"pt\")\n",
    "outputs = compiled_asr_model(inputs['input_values'])[0]\n",
    "\n",
    "ids = torch.argmax(torch.from_numpy(outputs), dim=-1)[0]\n",
    "transcription = asr_processor.decode(ids)\n",
    "print(transcription)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49f4b7871f18ab46"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name='7'></a>\n",
    "## Language Identification (LID) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86963727a1d32e5a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name='8'></a>\n",
    "### Download pretrained model and processor\n",
    "Different LID models are available based on the number of languages they can recognize - 126, 256, 512, 1024, 2048, 4017. We will use 126."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb607febc51e3782"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForSequenceClassification, AutoFeatureExtractor\n",
    "\n",
    "model_id = \"facebook/mms-lid-126\"\n",
    "\n",
    "lid_processor = AutoFeatureExtractor.from_pretrained(model_id)\n",
    "lid_model = Wav2Vec2ForSequenceClassification.from_pretrained(model_id)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1995f9336132be61"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name='9'></a>\n",
    "### Make inference with the original model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "100d4f9dfff9a7d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs = lid_processor(example['audio']['array'], sampling_rate=16_000, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = lid_model(**inputs).logits\n",
    "\n",
    "lang_id = torch.argmax(outputs, dim=-1)[0].item()\n",
    "detected_lang = lid_model.config.id2label[lang_id]\n",
    "print(detected_lang)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef184f78ef5f39c0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name='10'></a>\n",
    "### Convert to OpenVINO IR model and make inference"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9bc6f53041bf77e4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 30480\n",
    "\n",
    "input_values = torch.zeros([1, MAX_SEQ_LENGTH], dtype=torch.float)\n",
    "attention_mask = torch.zeros([1, MAX_SEQ_LENGTH], dtype=torch.int32)\n",
    "lid_model_xml_path = Path('models/ov_lid_model.xml')\n",
    "\n",
    "if not lid_model_xml_path.exists():\n",
    "    lid_model_xml_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    converted_model = openvino.convert_model(lid_model, example_input={'input_values': input_values})\n",
    "    openvino.save_model(converted_model, lid_model_xml_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8edf31c50d2c1c9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "And compile."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a36953b5a21d5d6c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "core = openvino.Core()\n",
    "\n",
    "compiled_lid_model = core.compile_model(lid_model_xml_path, device_name='CPU')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf0b2d1c56693a83"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now it is possible to make inference. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40193d2a396bb746"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def detect_lang(audio_data):\n",
    "    inputs = lid_processor(audio_data, sampling_rate=16_000, return_tensors=\"pt\")\n",
    "    \n",
    "    outputs = compiled_lid_model(inputs['input_values'])[0]\n",
    "    \n",
    "    lang_id = torch.argmax(torch.from_numpy(outputs), dim=-1)[0].item()\n",
    "    detected_lang = lid_model.config.id2label[lang_id]\n",
    "    \n",
    "    return detected_lang"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5d96a19f0504f3d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "detect_lang(example['audio']['array'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcaae46ecd2077b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's check another language."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "346a0954d96d40df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mls = load_dataset(\"facebook/multilingual_librispeech\", \"french\", split=\"test\", streaming=True)\n",
    "mls = iter(mls)\n",
    "\n",
    "example = next(mls)\n",
    "print(example['text'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e4b10f76be235ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "detect_lang(example['audio']['array'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67f764403640f618"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
