{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14186cec-1749-4be0-96b9-56ab5518ddb6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Kosmos-2: Multimodal Large Language Model and OpenVINO\n",
    "\n",
    "[KOSMOS-2](https://github.com/microsoft/unilm/tree/master/kosmos-2) is a multimodal large language model (MLLM) that has new capabilities of multimodal grounding and referring. KOSMOS-2 can understand multimodal input, follow instructions, \n",
    "perceive object descriptions (e.g., bounding boxes), and ground language to the visual world.\n",
    "\n",
    "Multimodal Large Language Models (MLLMs) have successfully played a role as a general-purpose interface across a wide range of tasks, such as language, vision, and vision-language tasks. MLLMs can perceive general modalities, including texts, images, and audio, and generate responses using free-form texts under zero-shot and few-shot settings. \n",
    "\n",
    "[In this work](https://arxiv.org/abs/2306.14824), authors unlock the grounding capability for multimodal large language models. Grounding capability can provide a more convenient and efficient human-AI interaction for vision-language tasks. It enables the user to point to the object or region in the image directly rather than input detailed text descriptions to refer to it, the model can understand that image region with its spatial locations. Grounding capability also enables the model to respond with visual answers (i.e., bounding boxes), which can support more vision-language tasks such as referring expression comprehension. Visual answers are more accurate and resolve the coreference ambiguity compared with text-only responses. In addition, grounding capability can link noun phrases and referring expressions in the generated free-form text response to the image regions, providing more accurate, informational, and comprehensive answers.\n",
    "\n",
    "\n",
    "![image](https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/annotated_snowman.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663df1e2-6679-42e3-9b84-c55f5b4dc5a2",
   "metadata": {},
   "source": [
    "#### Table of contents:\n",
    "- [Install requirements](#Install-requirements)\n",
    "- [Original model inference](#Original-model-inference)\n",
    "- [Convert models to OpenVINO Intermediate representation (IR) format](#Convert-models-to-OpenVINO-Intermediate-representation-(IR)-format)\n",
    "  - [Convert the vision model](#Convert-the-vision-model)\n",
    "  - [Convert Image To Text Projection model](#Convert-Image-To-Text-Projection-model)\n",
    "  - [Convert Text model](#Convert-Text-model)\n",
    "- [Compiling models and prepare pipeline](#Compiling-models-and-prepare-pipeline)\n",
    "- [Inference](#Inference)\n",
    "- [Interactive inference](#Interactive-inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a75c7ce-6f94-43ba-8087-11d7ad717a6f",
   "metadata": {},
   "source": [
    "## Install requirements\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1176598f-a76b-4c3a-93c5-78f1bb4ea344",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip uninstall -q -y openvino-dev openvino openvino-nightly\n",
    "%pip install -q \"openvino-nightly\"\n",
    "%pip install -q \"transformers>=4.35\" Pillow gradio opencv-python\n",
    "%pip install -q --extra-index-url https://download.pytorch.org/whl/cpu torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28152b98-b619-4f1b-9164-dd9c726ffb82",
   "metadata": {},
   "source": [
    "## Original model inference\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Let's take the [original example](https://huggingface.co/microsoft/kosmos-2-patch14-224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebad0822-fbb3-45ac-a91e-861ad725fcd4",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>. the, to and of as in I that' for is was- on’ it with The as at bet he have from by are \" you his “ this said not has an ( but had we her they will my or were their): up about out who one all been she can more would It</image><grounding> An image of<phrase> a snowman</phrase><object><patch_index_0044><patch_index_0863></object> warming himself by<phrase> a fire</phrase><object><patch_index_0005><patch_index_0911></object>.\n",
      "<grounding> An image of<phrase> a snowman</phrase><object><patch_index_0044><patch_index_0863></object> warming himself by<phrase> a fire</phrase><object><patch_index_0005><patch_index_0911></object>.\n",
      "An image of a snowman warming himself by a fire.\n",
      "[('a snowman', (12, 21), [(0.390625, 0.046875, 0.984375, 0.828125)]), ('a fire', (41, 47), [(0.171875, 0.015625, 0.484375, 0.890625)])]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "\n",
    "model = AutoModelForVision2Seq.from_pretrained(\"microsoft/kosmos-2-patch14-224\")\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/kosmos-2-patch14-224\")\n",
    "\n",
    "prompt = \"<grounding>An image of\"\n",
    "\n",
    "url = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.png\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# The original Kosmos-2 demo saves the image first then reload it. For some images, this will give slightly different image input and change the generation outputs.\n",
    "image.save(\"new_image.jpg\")\n",
    "image = Image.open(\"new_image.jpg\")\n",
    "\n",
    "inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    pixel_values=inputs[\"pixel_values\"],\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    image_embeds=None,\n",
    "    image_embeds_position_mask=inputs[\"image_embeds_position_mask\"],\n",
    "    use_cache=True,\n",
    "    max_new_tokens=128,\n",
    ")\n",
    "\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(f'{generated_text=}')\n",
    "# Specify `cleanup_and_extract=False` in order to see the raw model generation.\n",
    "processed_text = processor.post_process_generation(generated_text, cleanup_and_extract=False)\n",
    "\n",
    "print(f'{processed_text=}')\n",
    "# `<grounding> An image of<phrase> a snowman</phrase><object><patch_index_0044><patch_index_0863></object> warming himself by<phrase> a fire</phrase><object><patch_index_0005><patch_index_0911></object>.`\n",
    "\n",
    "# By default, the generated  text is cleanup and the entities are extracted.\n",
    "processed_text, entities = processor.post_process_generation(generated_text)\n",
    "\n",
    "print(f'Cleaned up {processed_text=}')\n",
    "# `An image of a snowman warming himself by a fire.`\n",
    "\n",
    "print(f'{entities=}')\n",
    "# `[('a snowman', (12, 21), [(0.390625, 0.046875, 0.984375, 0.828125)]), ('a fire', (41, 47), [(0.171875, 0.015625, 0.484375, 0.890625)])]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a5d679d53082a5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Once you have the entities, you can use the following helper function to draw their bounding bboxes on the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def is_overlapping(rect1, rect2):\n",
    "    x1, y1, x2, y2 = rect1\n",
    "    x3, y3, x4, y4 = rect2\n",
    "    return not (x2 < x3 or x1 > x4 or y2 < y3 or y1 > y4)\n",
    "\n",
    "\n",
    "def draw_entity_boxes_on_image(image, entities):\n",
    "    \"\"\"_summary_\n",
    "    Args:\n",
    "        image (_type_): image or image path\n",
    "        collect_entity_location (_type_): _description_\n",
    "    \"\"\"\n",
    "    if isinstance(image, Image.Image):\n",
    "        image_h = image.height\n",
    "        image_w = image.width\n",
    "        image = np.array(image)[:, :, [2, 1, 0]]\n",
    "    else:\n",
    "        raise ValueError(f\"invaild image format, {type(image)} for {image}\")\n",
    "\n",
    "    if len(entities) == 0:\n",
    "        return image\n",
    "\n",
    "    new_image = image.copy()\n",
    "    previous_bboxes = []\n",
    "    # size of text\n",
    "    text_size = 1\n",
    "    # thickness of text\n",
    "    text_line = 1  # int(max(1 * min(image_h, image_w) / 512, 1))\n",
    "    box_line = 3\n",
    "    (c_width, text_height), _ = cv2.getTextSize(\"F\", cv2.FONT_HERSHEY_COMPLEX, text_size, text_line)\n",
    "    base_height = int(text_height * 0.675)\n",
    "    text_offset_original = text_height - base_height\n",
    "    text_spaces = 3\n",
    "\n",
    "    for entity_name, (start, end), bboxes in entities:\n",
    "        for (x1_norm, y1_norm, x2_norm, y2_norm) in bboxes:\n",
    "            orig_x1, orig_y1, orig_x2, orig_y2 = int(x1_norm * image_w), int(y1_norm * image_h), int(x2_norm * image_w), int(y2_norm * image_h)\n",
    "            # draw bbox\n",
    "            # random color\n",
    "            color = tuple(np.random.randint(0, 255, size=3).tolist())\n",
    "            new_image = cv2.rectangle(new_image, (orig_x1, orig_y1), (orig_x2, orig_y2), color, box_line)\n",
    "\n",
    "            l_o, r_o = box_line // 2 + box_line % 2, box_line // 2 + box_line % 2 + 1\n",
    "\n",
    "            x1 = orig_x1 - l_o\n",
    "            y1 = orig_y1 - l_o\n",
    "\n",
    "            if y1 < text_height + text_offset_original + 2 * text_spaces:\n",
    "                y1 = orig_y1 + r_o + text_height + text_offset_original + 2 * text_spaces\n",
    "                x1 = orig_x1 + r_o\n",
    "\n",
    "            # add text background\n",
    "            (text_width, text_height), _ = cv2.getTextSize(f\"  {entity_name}\", cv2.FONT_HERSHEY_COMPLEX, text_size, text_line)\n",
    "            text_bg_x1, text_bg_y1, text_bg_x2, text_bg_y2 = x1, y1 - (text_height + text_offset_original + 2 * text_spaces), x1 + text_width, y1\n",
    "\n",
    "            for prev_bbox in previous_bboxes:\n",
    "                while is_overlapping((text_bg_x1, text_bg_y1, text_bg_x2, text_bg_y2), prev_bbox):\n",
    "                    text_bg_y1 += (text_height + text_offset_original + 2 * text_spaces)\n",
    "                    text_bg_y2 += (text_height + text_offset_original + 2 * text_spaces)\n",
    "                    y1 += (text_height + text_offset_original + 2 * text_spaces)\n",
    "\n",
    "                    if text_bg_y2 >= image_h:\n",
    "                        text_bg_y1 = max(0, image_h - (text_height + text_offset_original + 2 * text_spaces))\n",
    "                        text_bg_y2 = image_h\n",
    "                        y1 = image_h\n",
    "                        break\n",
    "\n",
    "            alpha = 0.5\n",
    "            for i in range(text_bg_y1, text_bg_y2):\n",
    "                for j in range(text_bg_x1, text_bg_x2):\n",
    "                    if i < image_h and j < image_w:\n",
    "                        if j < text_bg_x1 + 1.35 * c_width:\n",
    "                            # original color\n",
    "                            bg_color = color\n",
    "                        else:\n",
    "                            # white\n",
    "                            bg_color = [255, 255, 255]\n",
    "                        new_image[i, j] = (alpha * new_image[i, j] + (1 - alpha) * np.array(bg_color)).astype(np.uint8)\n",
    "\n",
    "            cv2.putText(\n",
    "                new_image, f\"  {entity_name}\", (x1, y1 - text_offset_original - 1 * text_spaces), cv2.FONT_HERSHEY_COMPLEX, text_size, (0, 0, 0), text_line, cv2.LINE_AA\n",
    "            )\n",
    "            # previous_locations.append((x1, y1))\n",
    "            previous_bboxes.append((text_bg_x1, text_bg_y1, text_bg_x2, text_bg_y2))\n",
    "\n",
    "    pil_image = Image.fromarray(new_image[:, :, [2, 1, 0]])\n",
    "\n",
    "    return pil_image"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e58971ead7c372d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Draw the bounding bboxes\n",
    "new_image = draw_entity_boxes_on_image(image, entities)\n",
    "display(new_image)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5d8e7a052135c7b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert models to OpenVINO Intermediate representation (IR) format\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee206f85-101d-4276-8e31-50611c9a48ad"
  },
  {
   "cell_type": "markdown",
   "id": "9d6f6512-b155-4d92-8394-0662af9048bc",
   "metadata": {},
   "source": [
    "Define paths for converted models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7854da9-5e92-427a-b595-f7f27fba6648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "models_base_folder = Path(\"models\")\n",
    "VISION_MODEL_IR_PATH = models_base_folder / \"vision_model.xml\"\n",
    "IMAGE_TO_TEXT_PROJECTION_MODEL_IR_PATH = models_base_folder / \"image_to_text_projection_model.xml\"\n",
    "FIRST_STAGE_MODEL_PATH = models_base_folder / \"kosmos_input_embed.xml\"\n",
    "SECOND_STAGE_MODEL_PATH = models_base_folder / \"kosmos_with_past.xml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa115992-fca3-420f-89cc-bc684df11532",
   "metadata": {},
   "source": [
    "Define the conversion function for PyTorch modules. We use `ov.convert_model` function to obtain OpenVINO Intermediate Representation object and `ov.save_model` function to save it as XML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc816322-6da6-4d77-a3f9-d012d14850d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import openvino as ov\n",
    "\n",
    "\n",
    "def cleanup_torchscript_cache():\n",
    "    # cleanup memory\n",
    "    torch._C._jit_clear_class_registry()\n",
    "    torch.jit._recursive.concrete_type_store = torch.jit._recursive.ConcreteTypeStore()\n",
    "    torch.jit._state._clear_class_state()\n",
    "\n",
    "\n",
    "def convert(model: torch.nn.Module, xml_path: str, example_input):\n",
    "    xml_path = Path(xml_path)\n",
    "    if not xml_path.exists():\n",
    "        xml_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with torch.no_grad():\n",
    "            converted_model = ov.convert_model(model, example_input=example_input)\n",
    "        ov.save_model(converted_model, xml_path, compress_to_fp16=False)\n",
    "        \n",
    "        cleanup_torchscript_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf0e035-8cc4-49c8-b8c4-cb3d9f7aea2f",
   "metadata": {},
   "source": [
    "### Convert the vision model\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82351be8-02df-409c-97fa-969f7915203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert(model.vision_model, VISION_MODEL_IR_PATH, inputs[\"pixel_values\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11093a02-cb43-48de-a500-7cfb150c2180",
   "metadata": {},
   "source": [
    "### Convert Image To Text Projection model\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c36de38b-5e6d-4a91-8c5e-133ae5945217",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "def get_image_embeds(pixel_values):\n",
    "    vision_model_output = model.vision_model(pixel_values)\n",
    "    image_embeds = model.vision_model.model.post_layernorm(vision_model_output[0])\n",
    "    image_embeds = nn.functional.normalize(image_embeds, dim=-1)\n",
    "\n",
    "    return image_embeds\n",
    "\n",
    "\n",
    "image_embeds = get_image_embeds(inputs[\"pixel_values\"])\n",
    "convert(model.image_to_text_projection, IMAGE_TO_TEXT_PROJECTION_MODEL_IR_PATH, image_embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84783d6-5a58-4cac-a11e-a33d925f38fc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Convert Text model \n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e124152-1e9c-4622-b461-c49685ced0ec",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "\n",
    "from transformers.models.kosmos2.modeling_kosmos2 import create_position_ids_from_input_ids\n",
    "\n",
    "\n",
    "def get_projecton_image_embeds(pixel_values):\n",
    "    vision_model_output = model.vision_model(pixel_values)\n",
    "    image_embeds = model.vision_model.model.post_layernorm(vision_model_output[0])\n",
    "    image_embeds = nn.functional.normalize(image_embeds, dim=-1)\n",
    "    image_embeds, _ = model.image_to_text_projection(image_embeds)\n",
    "\n",
    "    return image_embeds\n",
    "\n",
    "\n",
    "def flattenize_inputs(inputs):\n",
    "    \"\"\"\n",
    "    Helper function for making nested inputs flattens\n",
    "    \"\"\"\n",
    "    flatten_inputs = []\n",
    "    for input_data in inputs:\n",
    "        if input_data is None:\n",
    "            continue\n",
    "        if isinstance(input_data, (list, tuple)):\n",
    "            flatten_inputs.extend(flattenize_inputs(input_data))\n",
    "        else:\n",
    "            flatten_inputs.append(input_data)\n",
    "    return flatten_inputs\n",
    "\n",
    "\n",
    "def postprocess_converted_model(ov_model, example_input=None, input_names=None, output_names=None, dynamic_shapes=None):\n",
    "    \"\"\"\n",
    "    Helper function for appling postprocessing on converted model with updating input names, shapes and output names\n",
    "    acording to requested specification\n",
    "    \"\"\"\n",
    "\n",
    "    flatten_example_inputs = flattenize_inputs(example_input) if example_input else []\n",
    "    if input_names:\n",
    "        for inp_name, m_input, input_data in zip(input_names, ov_model.inputs, flatten_example_inputs):\n",
    "            m_input.get_tensor().set_names({inp_name})\n",
    "    \n",
    "    if output_names:\n",
    "        for out, out_name in zip(ov_model.outputs, output_names):\n",
    "            out.get_tensor().set_names({out_name})\n",
    "\n",
    "    return ov_model\n",
    "\n",
    "\n",
    "def convert_text_model():\n",
    "    model.text_model.model.config.torchscript = True\n",
    "    model.text_model.config.torchscript = True\n",
    "    image_embeds = get_projecton_image_embeds(inputs[\"pixel_values\"])\n",
    "    conv_inputs = {\n",
    "        'input_ids': inputs[\"input_ids\"],\n",
    "        'attention_mask': inputs[\"attention_mask\"],\n",
    "        'image_embeds': image_embeds,\n",
    "        'image_embeds_position_mask': inputs[\"image_embeds_position_mask\"],\n",
    "    }\n",
    "    outs = model.text_model.model(**conv_inputs)\n",
    "    inputs_ = [\"input_ids\", 'attention_mask']\n",
    "    outputs = [\"logits\"]\n",
    "    dynamic_shapes = {\"input_ids\": {1: \"seq_len\"}, \"attention_mask\": {1: \"seq_len\"}, \"position_ids\": {0: \"seq_len\"}}\n",
    "    for idx in range(len(outs[1])):\n",
    "        inputs_.extend([f\"past_key_values.{idx}.key\", f\"past_key_values.{idx}.value\"])\n",
    "        dynamic_shapes[inputs_[-1]] = {2: \"past_sequence + sequence\"}\n",
    "        dynamic_shapes[inputs_[-2]] = {2: \"past_sequence + sequence\"}\n",
    "        outputs.extend([f\"present.{idx}.key\", f\"present.{idx}.value\"])\n",
    "\n",
    "    if not FIRST_STAGE_MODEL_PATH.exists():\n",
    "        ov_model = ov.convert_model(model.text_model.model, example_input=conv_inputs)\n",
    "        ov_model = postprocess_converted_model(ov_model, output_names=outputs)\n",
    "        ov.save_model(ov_model, FIRST_STAGE_MODEL_PATH)\n",
    "        del ov_model\n",
    "        cleanup_torchscript_cache()\n",
    "\n",
    "    if not SECOND_STAGE_MODEL_PATH.exists():\n",
    "        position_ids = create_position_ids_from_input_ids(\n",
    "            inputs[\"input_ids\"],\n",
    "            padding_idx=model.text_model.config.pad_token_id,\n",
    "            past_key_values_length=0,\n",
    "        )[:, -1:]\n",
    "\n",
    "        example_input_second_stage = {\n",
    "            \"input_ids\": inputs[\"input_ids\"][:, -1:],\n",
    "            \"attention_mask\": inputs[\"input_ids\"].new_ones(1, inputs[\"input_ids\"].shape[1] + 1),\n",
    "            'position_ids': position_ids,\n",
    "            \"past_key_values\": outs[1],\n",
    "        }\n",
    "        \n",
    "        ov_model = ov.convert_model(model.text_model.model, example_input=example_input_second_stage)\n",
    "        ov_model = postprocess_converted_model(\n",
    "            ov_model, \n",
    "            example_input=example_input_second_stage.values(), \n",
    "            input_names=inputs_, \n",
    "            output_names=outputs, \n",
    "            dynamic_shapes=dynamic_shapes\n",
    "        )\n",
    "        ov.save_model(ov_model, SECOND_STAGE_MODEL_PATH)\n",
    "        del ov_model\n",
    "        cleanup_torchscript_cache()\n",
    "\n",
    "\n",
    "convert_text_model()     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217e0425-e00e-4e6b-af5f-6e5dcc5a90dd",
   "metadata": {},
   "source": [
    "## Compiling models and prepare pipeline\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b58a25b-b114-4e52-b2e3-b97ad56e2744",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Select device that will be used to do models inference using OpenVINO from the dropdown list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31013a5f-4d35-40ae-8e87-52af60f4c76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce920dd24a9498088fd12868c57e976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=4, options=('CPU', 'GPU.0', 'GPU.1', 'GPU.2', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "core = ov.Core()\n",
    "DEVICE = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value='AUTO',\n",
    "    description='Device:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007b9693-dfd3-4413-81cf-4f69da0cffd8",
   "metadata": {},
   "source": [
    "Let's create callable wrapper classes for compiled models to allow interaction with original pipeline. Note that all of wrapper classes return `torch.Tensor`s instead of `np.array`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc4dc42a-8e85-4f31-a96f-acd3208923bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WraperInternalVisionModel:\n",
    "    post_layernorm = model.vision_model.model.post_layernorm\n",
    "    \n",
    "\n",
    "class VisionModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model_ir_path):\n",
    "        super().__init__()\n",
    "        self.model = WraperInternalVisionModel()\n",
    "        self.vision_model = core.compile_model(model_ir_path, DEVICE.value)\n",
    "\n",
    "    def forward(self, pixel_values, **kwargs):\n",
    "        vision_model_output = self.vision_model(pixel_values)[0]\n",
    "\n",
    "        return [torch.from_numpy(vision_model_output)]\n",
    "    \n",
    "\n",
    "class ImageToTextProjectionModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model_ir_path):\n",
    "        super().__init__()\n",
    "        self.image_to_text_projection = core.compile_model(model_ir_path, DEVICE.value)\n",
    "\n",
    "    def forward(self, image_embeds):\n",
    "        output = self.image_to_text_projection(image_embeds)\n",
    "        image_embeds = output[0]\n",
    "        projection_attentions = output[1]\n",
    "        return image_embeds, projection_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22759703-9491-409d-bf5d-fd66aafc4467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation import GenerationConfig, GenerationMixin\n",
    "from transformers.models.kosmos2.modeling_kosmos2 import Kosmos2ForConditionalGenerationModelOutput\n",
    "\n",
    "\n",
    "class KosmosForCausalLMWrapper(GenerationMixin):\n",
    "    def __init__(self, first_stage_model_path, second_stage_model_path, device):\n",
    "        \n",
    "        self.model_stage_1 = core.compile_model(first_stage_model_path, DEVICE.value)\n",
    "        self.model_stage_2 = core.read_model(second_stage_model_path)\n",
    "        self.input_names = {\n",
    "            key.get_any_name(): idx for idx, key in enumerate(self.model_stage_2.inputs)\n",
    "        }\n",
    "        self.output_names = {\n",
    "            key.get_any_name(): idx for idx, key in enumerate(self.model_stage_2.outputs)\n",
    "        }\n",
    "        self.key_value_input_names = [\n",
    "            key for key in self.input_names if \"key_values\" in key\n",
    "        ]\n",
    "        self.key_value_output_names = [\n",
    "            key for key in self.output_names if \"present\" in key\n",
    "        ]\n",
    "        self.model_stage_2 = core.compile_model(self.model_stage_2, DEVICE.value)\n",
    "\n",
    "        self.request = self.model_stage_2.create_infer_request()\n",
    "        self.config = model.config\n",
    "        self.generation_config = GenerationConfig.from_model_config(model.config)\n",
    "        self.main_input_name = \"input_ids\"\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.num_pkv = 2\n",
    "        self.lm_head = nn.Linear(in_features=model.text_model.config.embed_dim, out_features=model.text_model.config.vocab_size, bias=False)\n",
    "\n",
    "    def get_input_embeddings(self) -> nn.Module:\n",
    "        return self.model.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.model.embed_tokens = value\n",
    "\n",
    "    def get_output_embeddings(self) -> nn.Module:\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def can_generate(self):\n",
    "        \"\"\"Returns True to validate the check that the model using `GenerationMixin.generate()` can indeed generate.\"\"\"\n",
    "        return True\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        image_embeds: Optional[torch.Tensor] = None,\n",
    "        image_embeds_position_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids=None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        return self.forward(\n",
    "            input_ids, attention_mask, image_embeds, image_embeds_position_mask, position_ids, past_key_values\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        image_embeds: Optional[torch.Tensor] = None,\n",
    "        image_embeds_position_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids=None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        \n",
    "        **kwargs\n",
    "    ):\n",
    "        if past_key_values is None:\n",
    "            \n",
    "            outs = self.model_stage_1(\n",
    "                {\n",
    "                    'input_ids': input_ids,\n",
    "                    'attention_mask': attention_mask,\n",
    "                    'image_embeds': image_embeds,\n",
    "                    'image_embeds_position_mask': image_embeds_position_mask,\n",
    "                }\n",
    "            )            \n",
    "            lm_logits = model.text_model.lm_head(torch.from_numpy(outs[0]))\n",
    "\n",
    "            pkv = list(outs.values())[1:]\n",
    "            pkv = tuple(pkv[i : i + 2] for i in range(0, len(pkv), 2))\n",
    "\n",
    "            return Kosmos2ForConditionalGenerationModelOutput(logits=lm_logits, past_key_values=pkv)\n",
    "        \n",
    "        if past_key_values is not None:\n",
    "            past_key_values = tuple(\n",
    "                past_key_value\n",
    "                for pkv_per_layer in past_key_values\n",
    "                for past_key_value in pkv_per_layer\n",
    "            )\n",
    "            inputs_ = {\n",
    "                \"input_ids\": input_ids[:, -1].unsqueeze(-1),\n",
    "                \"attention_mask\": attention_mask,\n",
    "                'position_ids': position_ids\n",
    "            }\n",
    "            inputs_.update(dict(zip(self.key_value_input_names, past_key_values)))\n",
    "\n",
    "        # Run inference\n",
    "        self.request.start_async(inputs_, share_inputs=True)\n",
    "        self.request.wait()\n",
    "\n",
    "        logits = torch.from_numpy(self.request.get_tensor(\"logits\").data)\n",
    "        logits = model.text_model.lm_head(logits)\n",
    "\n",
    "        # Tuple of length equal to : number of layer * number of past_key_value per decoder layer (2 corresponds to the self-attention layer)\n",
    "        past_key_values = tuple(\n",
    "            self.request.get_tensor(key).data for key in self.key_value_output_names\n",
    "        )\n",
    "        # Tuple of tuple of length `n_layers`, with each tuple of length equal to 2 (k/v of self-attention)\n",
    "\n",
    "        past_key_values = tuple(\n",
    "            past_key_values[i : i + self.num_pkv]\n",
    "            for i in range(0, len(past_key_values), self.num_pkv)\n",
    "        )\n",
    "        \n",
    "        return Kosmos2ForConditionalGenerationModelOutput(logits=logits, past_key_values=past_key_values)\n",
    "\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self,\n",
    "        input_ids,\n",
    "        image_embeds=None,\n",
    "        image_embeds_position_mask=None,\n",
    "        past_key_values=None,\n",
    "        attention_mask=None,\n",
    "        use_cache=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        input_shape = input_ids.shape\n",
    "        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n",
    "        if attention_mask is None:\n",
    "            attention_mask = input_ids.new_ones(input_shape)\n",
    "\n",
    "        position_ids = None\n",
    "\n",
    "        # cut input_ids if past_key_values is used\n",
    "        if past_key_values is not None:\n",
    "            position_ids = create_position_ids_from_input_ids(\n",
    "                input_ids,\n",
    "                padding_idx=model.text_model.config.pad_token_id,\n",
    "                past_key_values_length=0,\n",
    "            )[:, -1:]\n",
    "\n",
    "            input_ids = input_ids[:, -1:]\n",
    "            image_embeds = None\n",
    "            image_embeds_position_mask = None\n",
    "        elif image_embeds_position_mask is not None:\n",
    "            batch_size, seq_len = input_ids.size()\n",
    "            mask_len = image_embeds_position_mask.size()[-1]\n",
    "            image_embeds_position_mask = torch.cat(\n",
    "                (\n",
    "                    image_embeds_position_mask,\n",
    "                    torch.zeros(size=(batch_size, seq_len - mask_len), dtype=torch.bool, device=input_ids.device),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"image_embeds\": image_embeds,\n",
    "            \"image_embeds_position_mask\": image_embeds_position_mask,\n",
    "            'position_ids': position_ids,\n",
    "            \"past_key_values\": past_key_values,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    # Copied from transformers.models.umt5.modeling_umt5.UMT5ForConditionalGeneration._reorder_cache\n",
    "    def _reorder_cache(past_key_values, beam_idx):\n",
    "        reordered_past = ()\n",
    "        for layer_past in past_key_values:\n",
    "            reordered_past += (\n",
    "                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n",
    "            )\n",
    "        return reordered_past\n",
    "\n",
    "\n",
    "class Kosmos2ForConditionalGenerationWrapper:\n",
    "    \n",
    "    def __init__(self, vision_model_path, image_to_text_projection_model_path, first_stage_model_path, second_stage_model_path, device):\n",
    "        self.vision_model = VisionModelWrapper(vision_model_path)\n",
    "        self.image_to_text_projection = ImageToTextProjectionModelWrapper(image_to_text_projection_model_path)\n",
    "        self.text_model = KosmosForCausalLMWrapper(first_stage_model_path, second_stage_model_path, device)\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        pixel_values=None,\n",
    "        image_embeds_position_mask=None,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        image_embeds=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        vision_model_output = self.vision_model(pixel_values)\n",
    "        image_embeds = model.vision_model.model.post_layernorm(vision_model_output[0])\n",
    "        # normalized features\n",
    "        image_embeds = nn.functional.normalize(image_embeds, dim=-1)\n",
    "        image_embeds, projection_attentions = self.image_to_text_projection(image_embeds.detach().numpy())\n",
    "\n",
    "        output = self.text_model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            image_embeds=image_embeds,\n",
    "            image_embeds_position_mask=image_embeds_position_mask,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0aef4494-50f7-4838-b3a4-9035c297d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_model = Kosmos2ForConditionalGenerationWrapper(VISION_MODEL_IR_PATH, IMAGE_TO_TEXT_PROJECTION_MODEL_IR_PATH, FIRST_STAGE_MODEL_PATH, SECOND_STAGE_MODEL_PATH, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bae26d3-2d18-44d7-9148-1a404fdfa08d",
   "metadata": {},
   "source": [
    "## Inference\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "743869c680a14ee",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<grounding> An image of<phrase> a snowman</phrase><object><patch_index_0044><patch_index_0863></object> warming himself by<phrase> a fire</phrase><object><patch_index_0005><patch_index_0911></object>.\n",
      "An image of a snowman warming himself by a fire.\n",
      "[('a snowman', (12, 21), [(0.390625, 0.046875, 0.984375, 0.828125)]), ('a fire', (41, 47), [(0.171875, 0.015625, 0.484375, 0.890625)])]\n"
     ]
    }
   ],
   "source": [
    "generated_ids_ = ov_model.generate(\n",
    "    pixel_values=inputs[\"pixel_values\"],\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    image_embeds=image_embeds,\n",
    "    image_embeds_position_mask=inputs[\"image_embeds_position_mask\"],\n",
    "    max_new_tokens=128,\n",
    ")\n",
    "\n",
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(f'{generated_text=}')\n",
    "# Specify `cleanup_and_extract=False` in order to see the raw model generation.\n",
    "processed_text = processor.post_process_generation(generated_text, cleanup_and_extract=False)\n",
    "\n",
    "print(f'{processed_text=}')\n",
    "# `<grounding> An image of<phrase> a snowman</phrase><object><patch_index_0044><patch_index_0863></object> warming himself by<phrase> a fire</phrase><object><patch_index_0005><patch_index_0911></object>.`\n",
    "\n",
    "# By default, the generated  text is cleanup and the entities are extracted.\n",
    "processed_text, entities = processor.post_process_generation(generated_text)\n",
    "\n",
    "print(f'Cleaned up {processed_text=}')\n",
    "# `An image of a snowman warming himself by a fire.`\n",
    "\n",
    "print(f'{entities=}')\n",
    "# `[('a snowman', (12, 21), [(0.390625, 0.046875, 0.984375, 0.828125)]), ('a fire', (41, 47), [(0.171875, 0.015625, 0.484375, 0.890625)])]`"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Interactive inference\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcd0b8fd2ac36d1a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def generate(image, prompt):\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "    generated_ids_ = ov_model.generate(\n",
    "        pixel_values=inputs[\"pixel_values\"],\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        image_embeds=None,\n",
    "        image_embeds_position_mask=inputs[\"image_embeds_position_mask\"],\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "    generated_text = processor.batch_decode(generated_ids_, skip_special_tokens=True)[0]\n",
    "    processed_text, entities = processor.post_process_generation(generated_text)\n",
    "\n",
    "    new_image = draw_entity_boxes_on_image(Image.fromarray(image), entities, show=False)\n",
    "    \n",
    "    return new_image, processed_text\n",
    "\n",
    "\n",
    "demo = gr.Interface(\n",
    "    generate,\n",
    "    [\n",
    "        gr.Image(label=\"Input image\"),\n",
    "        gr.Textbox(label=\"Prompt\"),\n",
    "    ],\n",
    "    [\"image\", \"text\"],\n",
    "    examples=[\n",
    "        [\"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.png\", \"An image of\"],\n",
    "        [\"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/two_dogs.jpg\", \"Why this picture is adorable?\"],\n",
    "        [\"https://ydshieh-kosmos-2.hf.space/file=/home/user/app/images/six_planes.png\", \"What is going on?\"]\n",
    "    ],\n",
    "    allow_flagging=\"never\",\n",
    ")\n",
    "try:\n",
    "    demo.queue().launch(debug=True)\n",
    "except Exception:\n",
    "    demo.queue().launch(debug=True, share=True)\n",
    "# if you are launching remotely, specify server_name and server_port\n",
    "# demo.launch(server_name='your server name', server_port='server port in int')\n",
    "# Read more in the docs: https://gradio.app/docs/"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e844208b-7c38-46ea-943b-efe382149e54"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
