{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e794fdbe-e032-470d-8eda-584f0e4f3604",
   "metadata": {},
   "source": [
    "# Virtual Try-On with CatVTON and OpenVINO\n",
    "\n",
    "### Abstract\n",
    "Virtual try-on methods based on diffusion models achieve realistic try-on effects but replicate the backbone network as a ReferenceNet or leverage additional image encoders to process condition inputs, resulting in high training and inference costs. [In this work](http://arxiv.org/abs/2407.15886), authors rethink the necessity of ReferenceNet and image encoders and innovate the interaction between garment and person, proposing CatVTON, a simple and efficient virtual try-on diffusion model.\n",
    "It facilitates the seamless transfer of in-shop or worn garments of arbitrary categories to target persons by simply\n",
    "concatenating them in spatial dimensions as inputs. The efficiency of the model is demonstrated in three aspects: \n",
    " 1. Lightweight network. Only the original diffusion modules are used, without additional network modules. The text encoder and cross attentions for text injection in the backbone are removed, further reducing the parameters by 167.02M.\n",
    " 2. Parameter-efficient training. We identified the try-on relevant modules through experiments and achieved high-quality try-on effects by training only 49.57M parameters (∼5.51% of the backbone network’s parameters). \n",
    " 3. Simplified inference. CatVTON eliminates all unnecessary conditions and preprocessing steps, including pose estimation, human parsing, and text input, requiring only garment reference, target person image, and mask for the virtual try-on process. Extensive experiments demonstrate that CatVTON achieves superior qualitative and quantitative results with fewer prerequisites and trainable parameters than baseline methods. Furthermore, CatVTON shows good generalization in in-the-wild scenarios despite using open-source datasets with only 73K samples.\n",
    "\n",
    "\n",
    "Teaser image from [CatVTON GitHub](https://github.com/Zheng-Chong/CatVTON)\n",
    "![teaser](https://github.com/Zheng-Chong/CatVTON/blob/edited/resource/img/teaser.jpg?raw=true)\n",
    "\n",
    "In this tutorial we consider how to convert, optimize and run this model using OpenVINO.\n",
    "\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Convert the model to OpenVINO IR](#Convert-the-model-to-OpenVINO-IR)\n",
    "- [Compiling models](#Compiling-models)\n",
    "- [Interactive demo](#Interactive-demo)\n",
    "\n",
    "\n",
    "### Installation Instructions\n",
    "\n",
    "This is a self-contained example that relies solely on its own code.\n",
    "\n",
    "We recommend  running the notebook in a virtual environment. You only need a Jupyter server to start.\n",
    "For details, please refer to [Installation Guide](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/README.md#-installation-guide).\n",
    "\n",
    "<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=5b5a4db0-7875-4bfb-bdbd-01698b5b1a77&file=notebooks/catvton/catvton.ipynb\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b91d041-f139-47c7-bca8-805249648b6f",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f0cac2-0661-4974-a5c6-34d0bed7c92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"openvino>=2024.4\"\n",
    "%pip install -q \"torch>=2.1\" \"diffusers>=0.29.1\" torchvision opencv_python \"numpy<2\" --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -q fvcore \"pillow\" \"tqdm\" \"gradio>=4.36\" \"omegaconf==2.4.0.dev3\" av pycocotools cloudpickle scipy accelerate \"transformers>=4.27.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14f62ef8-89d1-4ddc-9058-aa09fcc763fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    ")\n",
    "open(\"notebook_utils.py\", \"w\").write(r.text)\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/aleksandr-mokrov/openvino_notebooks/refs/heads/catvton/utils/cmd_helper.py\",\n",
    ")\n",
    "open(\"cmd_helper.py\", \"w\").write(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0d059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmd_helper import clone_repo\n",
    "\n",
    "\n",
    "clone_repo(\"https://github.com/Zheng-Chong/CatVTON.git\", \"3b795364a4d2f3b5adb365f39cdea376d20bc53c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18a61d2-910a-478d-9901-703de65377f3",
   "metadata": {},
   "source": [
    "### Convert the model to OpenVINO IR\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "You don't need to download checkpoints and load models, just call the helper function `download_models`. It takes care about it. Functions `convert_pipeline_models` and `convert_automasker_models` will convert models from pipeline and `automasker` in OpenVINO format.\n",
    "\n",
    "The original pipeline contains VAE encoder and decoder and UNET.\n",
    "![CatVTON-overview](https://github.com/user-attachments/assets/e35c8dab-1c54-47b1-a73b-2a62e6cdca7c)\n",
    "\n",
    "The `automasker` contains `DensePose` with `detectron2.GeneralizedRCNN` model and `SCHP` (`LIP` and `ATR` version).\n",
    "\n",
    "We use `ov.convert_model` function to obtain OpenVINO Intermediate Representation object and `ov.save_model` function to save it as XML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "228e4320-3b0b-45e5-a1ad-bb09878fe239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from ov_catvton_helper import download_models, convert_pipeline_models, convert_automasker_models\n",
    "\n",
    "\n",
    "MODEL_DIR = Path(\"models\")\n",
    "VAE_ENCODER_PATH = MODEL_DIR / \"vae_encoder.xml\"\n",
    "VAE_DECODER_PATH = MODEL_DIR / \"vae_decoder.xml\"\n",
    "UNET_PATH = MODEL_DIR / \"unet.xml\"\n",
    "DENSEPOSE_PROCESSOR_PATH = MODEL_DIR / \"densepose_processor.xml\"\n",
    "SCHP_PROCESSOR_ATR = MODEL_DIR / \"schp_processor_atr.xml\"\n",
    "SCHP_PROCESSOR_LIP = MODEL_DIR / \"schp_processor_lip.xml\"\n",
    "\n",
    "\n",
    "pipeline, mask_processor, automasker = download_models()\n",
    "convert_pipeline_models(pipeline, VAE_ENCODER_PATH, VAE_DECODER_PATH, UNET_PATH)\n",
    "convert_automasker_models(automasker, DENSEPOSE_PROCESSOR_PATH, SCHP_PROCESSOR_ATR, SCHP_PROCESSOR_LIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef1940a-9830-49e3-a625-58adc3f13fdf",
   "metadata": {},
   "source": [
    "## Compiling models\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Select device from dropdown list for running inference using OpenVINO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0629837a-25ec-47f9-8fd9-f87fc87fc1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "\n",
    "from notebook_utils import device_widget\n",
    "\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "device = device_widget()\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd405d5-5c57-4da5-b258-d458805a3afc",
   "metadata": {},
   "source": [
    "`get_compiled_pipeline` and `get_compiled_automasker`  functions defined in `ov_catvton_helper.py` provides convenient way for getting the pipeline and the `automasker` with compiled ov-models that are compatible with the original interface. It accepts the original pipeline and `automasker`, inference device and directories with converted models as arguments. Under the hood we create callable wrapper classes for compiled models to allow interaction with original pipelines. Note that all of wrapper classes return `torch.Tensor`s instead of `np.array`s. And then insert wrappers instances in the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8612d4be-e0cf-4249-881e-5270cc33ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ov_catvton_helper import get_compiled_pipeline, get_compiled_automasker\n",
    "\n",
    "\n",
    "pipeline = get_compiled_pipeline(pipeline, core, device, VAE_ENCODER_PATH, VAE_DECODER_PATH, UNET_PATH)\n",
    "automasker = get_compiled_automasker(automasker, core, device, DENSEPOSE_PROCESSOR_PATH, SCHP_PROCESSOR_ATR, SCHP_PROCESSOR_LIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f696671-3958-48d7-be83-e7b1b225cec7",
   "metadata": {},
   "source": [
    "## Interactive inference\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Please select below whether you would like to use the quantized models to launch the interactive demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317995b2-f331-413c-99cf-ae7af6a87f94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gradio_helper import make_demo\n",
    "\n",
    "\n",
    "output_dir = \"output\"\n",
    "demo = make_demo(pipeline, mask_processor, automasker, output_dir)\n",
    "try:\n",
    "    demo.launch(debug=True)\n",
    "except Exception:\n",
    "    demo.launch(debug=True, share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "openvino_notebooks": {
   "imageUrl": "https://github.com/user-attachments/assets/55319c63-f01c-4591-ac1e-3bb4e57dda35",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Image-to-Image"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
