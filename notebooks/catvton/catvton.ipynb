{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e794fdbe-e032-470d-8eda-584f0e4f3604",
   "metadata": {},
   "source": [
    "# Virtual Try-On with CatVTON and OpenVINO\n",
    "\n",
    "### Abstract\n",
    "Virtual try-on methods based on diffusion models achieve realistic try-on effects but replicate the backbone network as a ReferenceNet or leverage additional image encoders to process condition inputs, resulting in high training and inference costs. [In this work](http://arxiv.org/abs/2407.15886), authors rethink the necessity of ReferenceNet and image encoders and innovate the interaction between garment and person, proposing CatVTON, a simple and efficient virtual try-on diffusion model.\n",
    "It facilitates the seamless transfer of in-shop or worn garments of arbitrary categories to target persons by simply\n",
    "concatenating them in spatial dimensions as inputs. The efficiency of the model is demonstrated in three aspects: \n",
    " 1. Lightweight network. Only the original diffusion modules are used, without additional network modules. The text encoder and cross attentions for text injection in the backbone are removed, further reducing the parameters by 167.02M.\n",
    " 2. Parameter-efficient training. We identified the try-on relevant modules through experiments and achieved high-quality try-on effects by training only 49.57M parameters (∼5.51% of the backbone network’s parameters). \n",
    " 3. Simplified inference. CatVTON eliminates all unnecessary conditions and preprocessing steps, including pose estimation, human parsing, and text input, requiring only garment reference, target person image, and mask for the virtual try-on process. Extensive experiments demonstrate that CatVTON achieves superior qualitative and quantitative results with fewer prerequisites and trainable parameters than baseline methods. Furthermore, CatVTON shows good generalization in in-the-wild scenarios despite using open-source datasets with only 73K samples.\n",
    "\n",
    "\n",
    "Teaser image from [CatVTON GitHub](https://github.com/Zheng-Chong/CatVTON)\n",
    "![teaser](https://github.com/Zheng-Chong/CatVTON/blob/edited/resource/img/teaser.jpg?raw=true)\n",
    "\n",
    "In this tutorial we consider how to convert, optimize and run this model using OpenVINO.\n",
    "\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Convert and Optimize model](#Convert-and-Optimize-model)\n",
    "- [Run model inference](#Run-model-inference)\n",
    "    - [Select inference device](#Select-inference-device)\n",
    "    - [Initialize inference pipeline](#Initialize-inference-pipeline)\n",
    "- [Interactive demo](#Interactive-demo)\n",
    "\n",
    "\n",
    "### Installation Instructions\n",
    "\n",
    "This is a self-contained example that relies solely on its own code.\n",
    "\n",
    "We recommend  running the notebook in a virtual environment. You only need a Jupyter server to start.\n",
    "For details, please refer to [Installation Guide](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/README.md#-installation-guide).\n",
    "\n",
    "<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=5b5a4db0-7875-4bfb-bdbd-01698b5b1a77&file=notebooks/catvton/catvton.ipynb\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b91d041-f139-47c7-bca8-805249648b6f",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1f0cac2-0661-4974-a5c6-34d0bed7c92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"openvino>=2024.4\"\n",
    "%pip install -q \"torch>=2.1\" \"diffusers>=0.29.1\" torchvision opencv_python --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -q fvcore \"pillow\" \"tqdm\" \"gradio>=4.36\" \"omegaconf==2.4.0.dev3\" av pycocotools cloudpickle scipy accelerate \"transformers>=4.27.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "14f62ef8-89d1-4ddc-9058-aa09fcc763fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "catvton_path = Path(\"CatVTON\")\n",
    "\n",
    "if not catvton_path.exists():\n",
    "    exit_code = os.system(\"git clone https://github.com/Zheng-Chong/CatVTON.git\")\n",
    "    if exit_code != 0:\n",
    "        raise Exception(\"Failed to clone the repository!\")\n",
    "\n",
    "sys.path.insert(0, str(catvton_path))\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    ")\n",
    "open(\"notebook_utils.py\", \"w\").write(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18a61d2-910a-478d-9901-703de65377f3",
   "metadata": {},
   "source": [
    "### Convert the model to OpenVINO IR\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "228e4320-3b0b-45e5-a1ad-bb09878fe239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "MODEL_DIR = Path(\"models\")\n",
    "VAE_ENCODER_PATH = MODEL_DIR / \"vae_encoder.xml\"\n",
    "VAE_DECODER_PATH = MODEL_DIR / \"vae_decoder.xml\"\n",
    "UNET_PATH = MODEL_DIR / \"unet.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c11ec4e-2ace-4c8b-8493-663895b8579b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e5af680a344b7dba0ef7fe4fcdefdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "An error occurred while trying to fetch booksforcharlie/stable-diffusion-inpainting: booksforcharlie/stable-diffusion-inpainting does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "/home/maleksandr/test_notebooks/pixtral/openvino_notebooks/notebooks/pixtral/CatVTON/model/SCHP/__init__.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(ckpt_path, map_location='cpu')['state_dict']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from huggingface_hub import snapshot_download\n",
    "import yaml\n",
    "\n",
    "from model.cloth_masker import AutoMasker\n",
    "from model.pipeline import CatVTONPipeline\n",
    "\n",
    "\n",
    "resume_path = \"zhengchong/CatVTON\"\n",
    "base_model_path = \"booksforcharlie/stable-diffusion-inpainting\"\n",
    "repo_path = snapshot_download(repo_id=resume_path)\n",
    "output_dir = \"output\"\n",
    "\n",
    "\n",
    "pipeline = CatVTONPipeline(base_ckpt=base_model_path, attn_ckpt=repo_path, attn_ckpt_version=\"mix\", use_tf32=True, device=\"cpu\")\n",
    "\n",
    "# fix default config to use cpu\n",
    "with open(f\"{repo_path}/DensePose/densepose_rcnn_R_50_FPN_s1x.yaml\", \"r\") as fp:\n",
    "    data = yaml.safe_load(fp)\n",
    "\n",
    "data[\"MODEL\"].update({\"DEVICE\": \"cpu\"})\n",
    "\n",
    "with open(f\"{repo_path}/DensePose/densepose_rcnn_R_50_FPN_s1x.yaml\", \"w\") as fp:\n",
    "    yaml.safe_dump(data, fp)\n",
    "\n",
    "\n",
    "mask_processor = VaeImageProcessor(vae_scale_factor=8, do_normalize=False, do_binarize=True, do_convert_grayscale=True)\n",
    "automasker = AutoMasker(\n",
    "    densepose_ckpt=os.path.join(repo_path, \"DensePose\"),\n",
    "    schp_ckpt=os.path.join(repo_path, \"SCHP\"),\n",
    "    device=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c266f2-00eb-4db6-9024-7821b472e166",
   "metadata": {},
   "source": [
    "Let's define the conversion function for PyTorch modules. We use `ov.convert_model` function to obtain OpenVINO Intermediate Representation object and `ov.save_model` function to save it as XML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "948fdf35-65d7-4875-b873-0b43258d3ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import openvino as ov\n",
    "\n",
    "\n",
    "def convert(model: torch.nn.Module, xml_path: str, example_input):\n",
    "    xml_path = Path(xml_path)\n",
    "    if not xml_path.exists():\n",
    "        xml_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            converted_model = ov.convert_model(model, example_input=example_input)\n",
    "        ov.save_model(converted_model, xml_path)\n",
    "\n",
    "        # cleanup memory\n",
    "        torch._C._jit_clear_class_registry()\n",
    "        torch.jit._recursive.concrete_type_store = torch.jit._recursive.ConcreteTypeStore()\n",
    "        torch.jit._state._clear_class_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a30f57e6-363e-44b5-afdd-0b67f4eeeaeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maleksandr/test_notebooks/pixtral/openvino_notebooks/notebooks/pixtral/venv/lib/python3.10/site-packages/diffusers/models/downsampling.py:136: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert hidden_states.shape[1] == self.channels\n",
      "/home/maleksandr/test_notebooks/pixtral/openvino_notebooks/notebooks/pixtral/venv/lib/python3.10/site-packages/diffusers/models/downsampling.py:145: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert hidden_states.shape[1] == self.channels\n",
      "/home/maleksandr/test_notebooks/pixtral/openvino_notebooks/notebooks/pixtral/venv/lib/python3.10/site-packages/torch/jit/_trace.py:1306: TracerWarning: Trace had nondeterministic nodes. Did you forget call .eval() on your model? Nodes:\n",
      "\t%2495 : Float(1, 4, 128, 96, strides=[49152, 12288, 96, 1], requires_grad=0, device=cpu) = aten::randn(%2489, %2490, %2491, %2492, %2493, %2494) # /home/maleksandr/test_notebooks/pixtral/openvino_notebooks/notebooks/pixtral/venv/lib/python3.10/site-packages/diffusers/utils/torch_utils.py:81:0\n",
      "This may cause errors in trace checking. To disable trace checking, pass check_trace=False to torch.jit.trace()\n",
      "  _check_trace(\n",
      "/home/maleksandr/test_notebooks/pixtral/openvino_notebooks/notebooks/pixtral/venv/lib/python3.10/site-packages/torch/jit/_trace.py:1306: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Tensor-likes are not close!\n",
      "\n",
      "Mismatched elements: 27151 / 49152 (55.2%)\n",
      "Greatest absolute difference: 0.0009250640869140625 at index (0, 2, 0, 82) (up to 1e-05 allowed)\n",
      "Greatest relative difference: 0.0008534972047966543 at index (0, 2, 0, 82) (up to 1e-05 allowed)\n",
      "  _check_trace(\n"
     ]
    }
   ],
   "source": [
    "class VaeEncoder(torch.nn.Module):\n",
    "    def __init__(self, vae):\n",
    "        super().__init__()\n",
    "        self.vae = vae\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vae.encode(x).latent_dist.sample()\n",
    "\n",
    "\n",
    "convert(VaeEncoder(pipeline.vae), VAE_ENCODER_PATH, torch.zeros(1, 3, 1024, 768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b705486-754b-4748-be47-336947658ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maleksandr/test_notebooks/pixtral/openvino_notebooks/notebooks/pixtral/venv/lib/python3.10/site-packages/diffusers/models/upsampling.py:146: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert hidden_states.shape[1] == self.channels\n",
      "/home/maleksandr/test_notebooks/pixtral/openvino_notebooks/notebooks/pixtral/venv/lib/python3.10/site-packages/diffusers/models/upsampling.py:162: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if hidden_states.shape[0] >= 64:\n"
     ]
    }
   ],
   "source": [
    "class VaeDecoder(torch.nn.Module):\n",
    "    def __init__(self, vae):\n",
    "        super().__init__()\n",
    "        self.vae = vae\n",
    "\n",
    "    def forward(self, latents):\n",
    "        return self.vae.decode(latents)\n",
    "\n",
    "\n",
    "convert(VaeDecoder(pipeline.vae), VAE_DECODER_PATH, torch.zeros(1, 4, 128, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aba05580-5e06-4dec-bedd-eb2ae68ebf48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maleksandr/test_notebooks/pixtral/openvino_notebooks/notebooks/pixtral/venv/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_condition.py:1110: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if dim % default_overall_up_factor != 0:\n"
     ]
    }
   ],
   "source": [
    "class UNetWrapper(torch.nn.Module):\n",
    "    def __init__(self, unet):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "\n",
    "    def forward(self, sample=None, timestep=None, encoder_hidden_states=None, return_dict=None):\n",
    "        result = self.unet(sample=sample, timestep=timestep, encoder_hidden_states=encoder_hidden_states, return_dict=False)\n",
    "        return result\n",
    "\n",
    "\n",
    "inpainting_latent_model_input = torch.zeros(2, 9, 256, 96)\n",
    "timestep = torch.tensor(0)\n",
    "encoder_hidden_states = torch.zeros(2, 1, 768)\n",
    "example_input = (inpainting_latent_model_input, timestep, encoder_hidden_states)\n",
    "\n",
    "convert(UNetWrapper(pipeline.unet), UNET_PATH, example_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef1940a-9830-49e3-a625-58adc3f13fdf",
   "metadata": {},
   "source": [
    "## Compiling models\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Select device from dropdown list for running inference using OpenVINO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0629837a-25ec-47f9-8fd9-f87fc87fc1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a051c2ba8094b0fa269f5ea1f658bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=1, options=('CPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core = ov.Core()\n",
    "\n",
    "from notebook_utils import device_widget\n",
    "\n",
    "device = device_widget()\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "452d5ee6-0eea-4f2b-bb9a-b69e1e0dbd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_unet = core.compile_model(UNET_PATH, device.value)\n",
    "compiled_vae_encoder = core.compile_model(VAE_ENCODER_PATH, device.value)\n",
    "compiled_vae_decoder = core.compile_model(VAE_DECODER_PATH, device.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd405d5-5c57-4da5-b258-d458805a3afc",
   "metadata": {},
   "source": [
    "Let's create callable wrapper classes for compiled models to allow interaction with original pipelines. Note that all of wrapper classes return `torch.Tensor`s instead of `np.array`s. And then insert wrappers instances in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8612d4be-e0cf-4249-881e-5270cc33ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "class VAEWrapper(torch.nn.Module):\n",
    "    def __init__(self, vae_encoder, vae_decoder, config):\n",
    "        super().__init__()\n",
    "        self.vae_enocder = vae_encoder\n",
    "        self.vae_decoder = vae_decoder\n",
    "        self.device = \"cpu\"\n",
    "        self.dtype = torch.float32\n",
    "        self.config = config\n",
    "\n",
    "    def encode(self, pixel_values):\n",
    "        outs = self.vae_enocder(pixel_values)\n",
    "        outs = torch.from_numpy(outs[0])\n",
    "        result = namedtuple(\"VAE\", \"latent_dist\")(namedtuple(\"Sample\", \"sample\")(lambda: outs))\n",
    "        return result\n",
    "\n",
    "    def decode(self, latents):\n",
    "        outs = self.vae_decoder(latents)\n",
    "        outs = namedtuple(\"VAE\", \"sample\")(torch.from_numpy(outs[0]))\n",
    "        return outs\n",
    "\n",
    "\n",
    "class ConvUnetWrapper(torch.nn.Module):\n",
    "    def __init__(self, unet):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "\n",
    "    def forward(self, sample, timestep, encoder_hidden_states=None, **kwargs):\n",
    "        outputs = self.unet(\n",
    "            {\n",
    "                \"sample\": sample,\n",
    "                \"timestep\": timestep,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        return [torch.from_numpy(outputs[0])]\n",
    "\n",
    "\n",
    "pipeline.vae = VAEWrapper(compiled_vae_encoder, compiled_vae_decoder, pipeline.vae.config)\n",
    "pipeline.unet = ConvUnetWrapper(compiled_unet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f696671-3958-48d7-be83-e7b1b225cec7",
   "metadata": {},
   "source": [
    "## Interactive inference\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Please select below whether you would like to use the quantized models to launch the interactive demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317995b2-f331-413c-99cf-ae7af6a87f94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maleksandr/test_notebooks/pixtral/openvino_notebooks/notebooks/pixtral/venv/lib/python3.10/site-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 50/50 [28:13<00:00, 33.88s/it]\n"
     ]
    }
   ],
   "source": [
    "from gradio_helper import make_demo\n",
    "\n",
    "demo = make_demo(pipeline, mask_processor, automasker, output_dir)\n",
    "try:\n",
    "    demo.launch(debug=True)\n",
    "except Exception:\n",
    "    demo.launch(debug=True, share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "openvino_notebooks": {
   "imageUrl": "https://github.com/user-attachments/assets/55319c63-f01c-4591-ac1e-3bb4e57dda35",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Image-to-Image"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
